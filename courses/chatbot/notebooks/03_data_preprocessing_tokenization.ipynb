{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing and Tokenization for Chatbots\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand text preprocessing fundamentals for NLP\n",
    "- Implement a custom tokenizer from scratch\n",
    "- Build vocabulary with frequency analysis\n",
    "- Create dataset classes for conversational data\n",
    "- Compare custom vs pre-built tokenization solutions\n",
    "\n",
    "**Prerequisites:**\n",
    "- PyTorch fundamentals (Notebook 01)\n",
    "- Tensor operations for NLP (Notebook 02)\n",
    "- Basic understanding of text processing\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Text preprocessing and tokenization are crucial steps in any NLP pipeline. Before we can train a chatbot, we need to convert raw text into numerical representations that neural networks can understand. This notebook will guide you through building these components from scratch, helping you understand the underlying concepts before using pre-built solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "# Data science and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing Pipeline\n",
    "\n",
    "Before tokenization, we need to clean and normalize our text data. This involves several steps:\n",
    "\n",
    "1. **Lowercasing**: Convert all text to lowercase for consistency\n",
    "2. **Punctuation handling**: Decide how to handle punctuation marks\n",
    "3. **Special character removal**: Remove or replace special characters\n",
    "4. **Whitespace normalization**: Handle multiple spaces, tabs, newlines\n",
    "5. **Contraction expansion**: Convert contractions (don't → do not)\n",
    "\n",
    "Let's implement each step with educational explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Educational text preprocessor that demonstrates common NLP preprocessing steps.\n",
    "    \n",
    "    This class implements various text cleaning and normalization techniques\n",
    "    commonly used in chatbot development. Each method includes detailed\n",
    "    explanations of why the preprocessing step is important.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 lowercase: bool = True,\n",
    "                 remove_punctuation: bool = False,\n",
    "                 expand_contractions: bool = True,\n",
    "                 remove_extra_whitespace: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor with configuration options.\n",
    "        \n",
    "        Args:\n",
    "            lowercase: Whether to convert text to lowercase\n",
    "            remove_punctuation: Whether to remove punctuation marks\n",
    "            expand_contractions: Whether to expand contractions\n",
    "            remove_extra_whitespace: Whether to normalize whitespace\n",
    "        \n",
    "        Educational Note:\n",
    "        - Lowercasing reduces vocabulary size but loses capitalization information\n",
    "        - Punctuation can be important for chatbots (questions vs statements)\n",
    "        - Contractions should be expanded for consistency\n",
    "        \"\"\"\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.expand_contractions = expand_contractions\n",
    "        self.remove_extra_whitespace = remove_extra_whitespace\n",
    "        \n",
    "        # Common contractions mapping for educational purposes\n",
    "        self.contractions = {\n",
    "            \"don't\": \"do not\",\n",
    "            \"won't\": \"will not\",\n",
    "            \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\",\n",
    "            \"'re\": \" are\",\n",
    "            \"'ve\": \" have\",\n",
    "            \"'ll\": \" will\",\n",
    "            \"'d\": \" would\",\n",
    "            \"'m\": \" am\"\n",
    "        }\n",
    "    \n",
    "    def expand_contractions_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Expand contractions in the text.\n",
    "        \n",
    "        Educational Note:\n",
    "        Contractions like \"don't\" should be expanded to \"do not\" because:\n",
    "        1. It creates consistency in the vocabulary\n",
    "        2. It helps the model understand negation better\n",
    "        3. It reduces the number of unique tokens\n",
    "        \"\"\"\n",
    "        if not self.expand_contractions:\n",
    "            return text\n",
    "            \n",
    "        # Convert to lowercase for matching, but preserve original case\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            # Use word boundaries to avoid partial matches\n",
    "            pattern = r'\\b' + re.escape(contraction) + r'\\b'\n",
    "            text = re.sub(pattern, expansion, text, flags=re.IGNORECASE)\n",
    "            \n",
    "        return text\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize whitespace in the text.\n",
    "        \n",
    "        Educational Note:\n",
    "        Multiple spaces, tabs, and newlines can create inconsistencies.\n",
    "        We normalize them to single spaces for cleaner tokenization.\n",
    "        \"\"\"\n",
    "        if not self.remove_extra_whitespace:\n",
    "            return text\n",
    "            \n",
    "        # Replace multiple whitespace characters with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove leading and trailing whitespace\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def remove_punctuation_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove punctuation from text.\n",
    "        \n",
    "        Educational Note:\n",
    "        Removing punctuation reduces vocabulary size but loses important\n",
    "        information like questions (?) vs statements (.). For chatbots,\n",
    "        we might want to keep some punctuation.\n",
    "        \"\"\"\n",
    "        if not self.remove_punctuation:\n",
    "            return text\n",
    "            \n",
    "        # Remove all punctuation except spaces\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        return text.translate(translator)\n",
    "    \n",
    "    def preprocess(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Apply all preprocessing steps to the input text.\n",
    "        \n",
    "        Educational Note:\n",
    "        The order of preprocessing steps matters:\n",
    "        1. Expand contractions first (before lowercasing)\n",
    "        2. Handle punctuation\n",
    "        3. Normalize case\n",
    "        4. Clean whitespace last\n",
    "        \"\"\"\n",
    "        # Step 1: Expand contractions (before lowercasing)\n",
    "        text = self.expand_contractions_text(text)\n",
    "        \n",
    "        # Step 2: Handle punctuation\n",
    "        text = self.remove_punctuation_text(text)\n",
    "        \n",
    "        # Step 3: Convert to lowercase\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Step 4: Normalize whitespace\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Test the preprocessor with examples\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "test_texts = [\n",
    "    \"Hello! How are you today?\",\n",
    "    \"I don't think that's right...\",\n",
    "    \"What's   the    weather like?\",\n",
    "    \"Can't we do better than this?\"\n",
    "]\n",
    "\n",
    "print(\"Text Preprocessing Examples:\")\n",
    "print(\"=\" * 50)\n",
    "for text in test_texts:\n",
    "    processed = preprocessor.preprocess(text)\n",
    "    print(f\"Original:  '{text}'\")\n",
    "    print(f\"Processed: '{processed}'\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Tokenizer Implementation\n",
    "\n",
    "Now let's build a custom tokenizer from scratch. A tokenizer converts text into tokens (words, subwords, or characters) and maps them to numerical IDs.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Vocabulary**: The set of all unique tokens\n",
    "- **Token-to-ID mapping**: Each token gets a unique integer ID\n",
    "- **Special tokens**: `<PAD>`, `<UNK>`, `<SOS>`, `<EOS>` for special purposes\n",
    "- **Encoding**: Convert text to token IDs\n",
    "- **Decoding**: Convert token IDs back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EducationalTokenizer:\n",
    "    \"\"\"\n",
    "    A custom tokenizer implementation for educational purposes.\n",
    "    \n",
    "    This tokenizer demonstrates the core concepts of tokenization:\n",
    "    - Vocabulary building from training data\n",
    "    - Token-to-ID and ID-to-token mappings\n",
    "    - Handling unknown tokens\n",
    "    - Special tokens for sequence modeling\n",
    "    \n",
    "    Educational Note:\n",
    "    While production systems use sophisticated tokenizers (BPE, WordPiece),\n",
    "    this simple word-level tokenizer helps understand the fundamentals.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 max_vocab_size: int = 10000,\n",
    "                 min_frequency: int = 1,\n",
    "                 special_tokens: List[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer.\n",
    "        \n",
    "        Args:\n",
    "            max_vocab_size: Maximum number of tokens in vocabulary\n",
    "            min_frequency: Minimum frequency for a token to be included\n",
    "            special_tokens: List of special tokens to include\n",
    "        \n",
    "        Educational Note:\n",
    "        - max_vocab_size controls memory usage and model size\n",
    "        - min_frequency helps filter out rare/noisy tokens\n",
    "        - Special tokens handle sequence boundaries and unknown words\n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_frequency = min_frequency\n",
    "        \n",
    "        # Define special tokens\n",
    "        if special_tokens is None:\n",
    "            self.special_tokens = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "        else:\n",
    "            self.special_tokens = special_tokens\n",
    "        \n",
    "        # Initialize mappings\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.token_frequencies = Counter()\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Add special tokens first (they get IDs 0, 1, 2, ...)\n",
    "        for token in self.special_tokens:\n",
    "            self._add_token(token)\n",
    "    \n",
    "    def _add_token(self, token: str) -> int:\n",
    "        \"\"\"\n",
    "        Add a token to the vocabulary.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method maintains the bidirectional mapping between\n",
    "        tokens and their integer IDs. The ID assignment is sequential.\n",
    "        \"\"\"\n",
    "        if token not in self.token_to_id:\n",
    "            token_id = len(self.token_to_id)\n",
    "            self.token_to_id[token] = token_id\n",
    "            self.id_to_token[token_id] = token\n",
    "            self.vocab_size += 1\n",
    "        return self.token_to_id[token]\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split text into tokens.\n",
    "        \n",
    "        Educational Note:\n",
    "        This is a simple word-level tokenization. More sophisticated\n",
    "        approaches include:\n",
    "        - Subword tokenization (BPE, WordPiece)\n",
    "        - Character-level tokenization\n",
    "        - Sentence piece tokenization\n",
    "        \"\"\"\n",
    "        # Simple whitespace tokenization\n",
    "        # In practice, you might use more sophisticated methods\n",
    "        tokens = text.strip().split()\n",
    "        return tokens\n",
    "    \n",
    "    def build_vocabulary(self, texts: List[str], preprocessor: TextPreprocessor = None):\n",
    "        \"\"\"\n",
    "        Build vocabulary from a list of texts.\n",
    "        \n",
    "        Educational Note:\n",
    "        Vocabulary building involves:\n",
    "        1. Tokenizing all texts\n",
    "        2. Counting token frequencies\n",
    "        3. Filtering by frequency and vocabulary size\n",
    "        4. Creating token-to-ID mappings\n",
    "        \"\"\"\n",
    "        print(\"Building vocabulary...\")\n",
    "        \n",
    "        # Count token frequencies\n",
    "        all_tokens = []\n",
    "        for text in texts:\n",
    "            # Preprocess if preprocessor is provided\n",
    "            if preprocessor:\n",
    "                text = preprocessor.preprocess(text)\n",
    "            \n",
    "            # Tokenize and collect tokens\n",
    "            tokens = self.tokenize(text)\n",
    "            all_tokens.extend(tokens)\n",
    "            self.token_frequencies.update(tokens)\n",
    "        \n",
    "        print(f\"Total tokens found: {len(all_tokens)}\")\n",
    "        print(f\"Unique tokens found: {len(self.token_frequencies)}\")\n",
    "        \n",
    "        # Filter tokens by frequency and add to vocabulary\n",
    "        # Sort by frequency (descending) to keep most common tokens\n",
    "        sorted_tokens = self.token_frequencies.most_common()\n",
    "        \n",
    "        added_tokens = 0\n",
    "        for token, freq in sorted_tokens:\n",
    "            # Skip if we've reached max vocabulary size\n",
    "            if len(self.token_to_id) >= self.max_vocab_size:\n",
    "                break\n",
    "            \n",
    "            # Skip if frequency is too low\n",
    "            if freq < self.min_frequency:\n",
    "                break\n",
    "            \n",
    "            # Skip if token is already in vocabulary (special tokens)\n",
    "            if token not in self.token_to_id:\n",
    "                self._add_token(token)\n",
    "                added_tokens += 1\n",
    "        \n",
    "        print(f\"Vocabulary built with {self.vocab_size} tokens\")\n",
    "        print(f\"Added {added_tokens} new tokens (excluding special tokens)\")\n",
    "    \n",
    "    def encode(self, text: str, \n",
    "               add_special_tokens: bool = True,\n",
    "               preprocessor: TextPreprocessor = None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert text to token IDs.\n",
    "        \n",
    "        Educational Note:\n",
    "        Encoding involves:\n",
    "        1. Preprocessing the text\n",
    "        2. Tokenizing into words/subwords\n",
    "        3. Converting tokens to IDs\n",
    "        4. Handling unknown tokens\n",
    "        5. Adding special tokens if needed\n",
    "        \"\"\"\n",
    "        # Preprocess if preprocessor is provided\n",
    "        if preprocessor:\n",
    "            text = preprocessor.preprocess(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenize(text)\n",
    "        \n",
    "        # Convert tokens to IDs\n",
    "        token_ids = []\n",
    "        \n",
    "        # Add start-of-sequence token\n",
    "        if add_special_tokens and '<SOS>' in self.token_to_id:\n",
    "            token_ids.append(self.token_to_id['<SOS>'])\n",
    "        \n",
    "        # Convert each token to ID\n",
    "        unk_id = self.token_to_id.get('<UNK>', 1)  # Default to ID 1\n",
    "        for token in tokens:\n",
    "            token_id = self.token_to_id.get(token, unk_id)\n",
    "            token_ids.append(token_id)\n",
    "        \n",
    "        # Add end-of-sequence token\n",
    "        if add_special_tokens and '<EOS>' in self.token_to_id:\n",
    "            token_ids.append(self.token_to_id['<EOS>'])\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids: List[int], \n",
    "               skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Convert token IDs back to text.\n",
    "        \n",
    "        Educational Note:\n",
    "        Decoding is the reverse of encoding:\n",
    "        1. Convert IDs back to tokens\n",
    "        2. Handle special tokens\n",
    "        3. Join tokens into text\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        \n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.id_to_token:\n",
    "                token = self.id_to_token[token_id]\n",
    "                \n",
    "                # Skip special tokens if requested\n",
    "                if skip_special_tokens and token in self.special_tokens:\n",
    "                    continue\n",
    "                \n",
    "                tokens.append(token)\n",
    "            else:\n",
    "                # Handle invalid token IDs\n",
    "                tokens.append('<UNK>')\n",
    "        \n",
    "        # Join tokens with spaces\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def get_vocab_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get information about the vocabulary.\n",
    "        \n",
    "        Educational Note:\n",
    "        Vocabulary statistics help understand the tokenizer's behavior\n",
    "        and can guide hyperparameter tuning.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'vocab_size': self.vocab_size,\n",
    "            'special_tokens': self.special_tokens,\n",
    "            'most_common_tokens': self.token_frequencies.most_common(10),\n",
    "            'total_token_occurrences': sum(self.token_frequencies.values())\n",
    "        }\n",
    "\n",
    "# Test the tokenizer\n",
    "print(\"Testing Custom Tokenizer:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create sample texts for testing\n",
    "sample_texts = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am doing well, thank you!\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Machine learning is fascinating.\"\n",
    "]\n",
    "\n",
    "# Initialize tokenizer and preprocessor\n",
    "tokenizer = EducationalTokenizer(max_vocab_size=100)\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Build vocabulary\n",
    "tokenizer.build_vocabulary(sample_texts, preprocessor)\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"Hello, what is machine learning?\"\n",
    "print(f\"\\nTest text: '{test_text}'\")\n",
    "\n",
    "# Encode\n",
    "token_ids = tokenizer.encode(test_text, preprocessor=preprocessor)\n",
    "print(f\"Encoded IDs: {token_ids}\")\n",
    "\n",
    "# Decode\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"Decoded text: '{decoded_text}'\")\n",
    "\n",
    "# Show vocabulary info\n",
    "vocab_info = tokenizer.get_vocab_info()\n",
    "print(f\"\\nVocabulary Info:\")\n",
    "for key, value in vocab_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vocabulary Analysis and Visualization\n",
    "\n",
    "Understanding your vocabulary is crucial for building effective chatbots. Let's analyze token frequencies, distribution, and create visualizations to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_vocabulary(tokenizer: EducationalTokenizer, \n",
    "                      texts: List[str], \n",
    "                      preprocessor: TextPreprocessor = None):\n",
    "    \"\"\"\n",
    "    Perform comprehensive vocabulary analysis with visualizations.\n",
    "    \n",
    "    Educational Note:\n",
    "    Vocabulary analysis helps us understand:\n",
    "    - Token frequency distribution (Zipf's law)\n",
    "    - Vocabulary coverage\n",
    "    - Most/least common tokens\n",
    "    - Text length statistics\n",
    "    \"\"\"\n",
    "    print(\"Performing Vocabulary Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Collect all tokens and their frequencies\n",
    "    all_tokens = []\n",
    "    text_lengths = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if preprocessor:\n",
    "            processed_text = preprocessor.preprocess(text)\n",
    "        else:\n",
    "            processed_text = text\n",
    "        \n",
    "        tokens = tokenizer.tokenize(processed_text)\n",
    "        all_tokens.extend(tokens)\n",
    "        text_lengths.append(len(tokens))\n",
    "    \n",
    "    # Basic statistics\n",
    "    unique_tokens = len(set(all_tokens))\n",
    "    total_tokens = len(all_tokens)\n",
    "    avg_text_length = np.mean(text_lengths)\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens:,}\")\n",
    "    print(f\"Unique tokens: {unique_tokens:,}\")\n",
    "    print(f\"Vocabulary diversity: {unique_tokens/total_tokens:.3f}\")\n",
    "    print(f\"Average text length: {avg_text_length:.1f} tokens\")\n",
    "    print(f\"Text length range: {min(text_lengths)} - {max(text_lengths)} tokens\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Token frequency distribution (top 20)\n",
    "    most_common = tokenizer.token_frequencies.most_common(20)\n",
    "    tokens, frequencies = zip(*most_common)\n",
    "    \n",
    "    axes[0, 0].bar(range(len(tokens)), frequencies)\n",
    "    axes[0, 0].set_xticks(range(len(tokens)))\n",
    "    axes[0, 0].set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    axes[0, 0].set_title('Top 20 Most Frequent Tokens')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 2. Zipf's law visualization (log-log plot)\n",
    "    all_frequencies = [freq for token, freq in tokenizer.token_frequencies.most_common()]\n",
    "    ranks = range(1, len(all_frequencies) + 1)\n",
    "    \n",
    "    axes[0, 1].loglog(ranks, all_frequencies, 'b-', alpha=0.7)\n",
    "    axes[0, 1].set_title(\"Zipf's Law: Token Frequency vs Rank\")\n",
    "    axes[0, 1].set_xlabel('Rank (log scale)')\n",
    "    axes[0, 1].set_ylabel('Frequency (log scale)')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Text length distribution\n",
    "    axes[1, 0].hist(text_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].axvline(avg_text_length, color='red', linestyle='--', \n",
    "                       label=f'Mean: {avg_text_length:.1f}')\n",
    "    axes[1, 0].set_title('Distribution of Text Lengths')\n",
    "    axes[1, 0].set_xlabel('Number of Tokens')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Vocabulary growth curve\n",
    "    vocab_sizes = []\n",
    "    seen_tokens = set()\n",
    "    \n",
    "    for i, token in enumerate(all_tokens):\n",
    "        seen_tokens.add(token)\n",
    "        if i % 10 == 0:  # Sample every 10 tokens for efficiency\n",
    "            vocab_sizes.append(len(seen_tokens))\n",
    "    \n",
    "    sample_points = range(0, len(all_tokens), 10)\n",
    "    axes[1, 1].plot(sample_points[:len(vocab_sizes)], vocab_sizes)\n",
    "    axes[1, 1].set_title('Vocabulary Growth Curve')\n",
    "    axes[1, 1].set_xlabel('Number of Tokens Processed')\n",
    "    axes[1, 1].set_ylabel('Unique Tokens Seen')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational insights\n",
    "    print(\"\\nEducational Insights:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"1. Zipf's Law: Natural language follows a power law distribution\")\n",
    "    print(\"   - Few tokens are very frequent, many tokens are rare\")\n",
    "    print(\"   - This affects vocabulary size decisions\")\n",
    "    \n",
    "    print(\"\\n2. Vocabulary Diversity:\")\n",
    "    diversity = unique_tokens / total_tokens\n",
    "    if diversity > 0.7:\n",
    "        print(\"   - High diversity: Many unique tokens, might need larger vocabulary\")\n",
    "    elif diversity > 0.3:\n",
    "        print(\"   - Medium diversity: Balanced token distribution\")\n",
    "    else:\n",
    "        print(\"   - Low diversity: Many repeated tokens, smaller vocabulary sufficient\")\n",
    "    \n",
    "    print(\"\\n3. Text Length Distribution:\")\n",
    "    if max(text_lengths) > 3 * avg_text_length:\n",
    "        print(\"   - High variance in text lengths, consider sequence length limits\")\n",
    "    else:\n",
    "        print(\"   - Consistent text lengths, good for batch processing\")\n",
    "    \n",
    "    return {\n",
    "        'total_tokens': total_tokens,\n",
    "        'unique_tokens': unique_tokens,\n",
    "        'diversity': diversity,\n",
    "        'avg_length': avg_text_length,\n",
    "        'length_range': (min(text_lengths), max(text_lengths))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversational Dataset Class\n",
    "\n",
    "Now let's create a PyTorch Dataset class specifically designed for conversational data. This will handle loading, preprocessing, and batching of conversation pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ConversationPair:\n",
    "    \"\"\"\n",
    "    Data class representing a single conversation pair.\n",
    "    \n",
    "    Educational Note:\n",
    "    Using dataclasses makes the code more readable and provides\n",
    "    automatic __init__, __repr__, and other methods.\n",
    "    \"\"\"\n",
    "    input_text: str\n",
    "    target_text: str\n",
    "    context: Optional[str] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize metadata if not provided.\"\"\"\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "class ConversationalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for conversational data.\n",
    "    \n",
    "    This dataset handles:\n",
    "    - Loading conversation data from JSON files\n",
    "    - Converting conversations to input-output pairs\n",
    "    - Tokenizing and encoding text\n",
    "    - Padding sequences to consistent lengths\n",
    "    \n",
    "    Educational Note:\n",
    "    Inheriting from torch.utils.data.Dataset allows us to use\n",
    "    PyTorch's DataLoader for efficient batching and shuffling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_path: str,\n",
    "                 tokenizer: EducationalTokenizer,\n",
    "                 preprocessor: TextPreprocessor = None,\n",
    "                 max_length: int = 128,\n",
    "                 include_context: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize the conversational dataset.\n",
    "        \n",
    "        Args:\n",
    "            data_path: Path to JSON file containing conversations\n",
    "            tokenizer: Tokenizer for encoding text\n",
    "            preprocessor: Text preprocessor (optional)\n",
    "            max_length: Maximum sequence length for padding/truncation\n",
    "            include_context: Whether to include conversation context\n",
    "        \n",
    "        Educational Note:\n",
    "        - max_length controls memory usage and training efficiency\n",
    "        - Context can provide additional information for responses\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.preprocessor = preprocessor\n",
    "        self.max_length = max_length\n",
    "        self.include_context = include_context\n",
    "        \n",
    "        # Load and process conversation data\n",
    "        self.conversation_pairs = self._load_conversations(data_path)\n",
    "        \n",
    "        print(f\"Loaded {len(self.conversation_pairs)} conversation pairs\")\n",
    "    \n",
    "    def _load_conversations(self, data_path: str) -> List[ConversationPair]:\n",
    "        \"\"\"\n",
    "        Load conversations from JSON file and convert to pairs.\n",
    "        \n",
    "        Educational Note:\n",
    "        We convert multi-turn conversations into input-output pairs.\n",
    "        Each user message becomes an input, and the following bot\n",
    "        response becomes the target output.\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "        \n",
    "        try:\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                conversations = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Could not find data file at {data_path}\")\n",
    "            return []\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error: Invalid JSON format in {data_path}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        for conv in conversations:\n",
    "            messages = conv.get('messages', [])\n",
    "            context = conv.get('context', '')\n",
    "            metadata = conv.get('metadata', {})\n",
    "            \n",
    "            # Extract input-output pairs from conversation\n",
    "            for i in range(len(messages) - 1):\n",
    "                current_msg = messages[i]\n",
    "                next_msg = messages[i + 1]\n",
    "                \n",
    "                # Create pair if current is user and next is bot\n",
    "                if (current_msg.get('speaker') == 'user' and \n",
    "                    next_msg.get('speaker') == 'bot'):\n",
    "                    \n",
    "                    input_text = current_msg.get('text', '')\n",
    "                    target_text = next_msg.get('text', '')\n",
    "                    \n",
    "                    # Include context if requested\n",
    "                    if self.include_context and context:\n",
    "                        input_text = f\"Context: {context}. User: {input_text}\"\n",
    "                    \n",
    "                    pair = ConversationPair(\n",
    "                        input_text=input_text,\n",
    "                        target_text=target_text,\n",
    "                        context=context,\n",
    "                        metadata=metadata\n",
    "                    )\n",
    "                    pairs.append(pair)\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def _pad_sequence(self, token_ids: List[int], max_length: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Pad or truncate sequence to specified length.\n",
    "        \n",
    "        Educational Note:\n",
    "        Padding ensures all sequences in a batch have the same length,\n",
    "        which is required for efficient tensor operations.\n",
    "        \"\"\"\n",
    "        pad_id = self.tokenizer.token_to_id.get('<PAD>', 0)\n",
    "        \n",
    "        if len(token_ids) > max_length:\n",
    "            # Truncate if too long\n",
    "            return token_ids[:max_length]\n",
    "        else:\n",
    "            # Pad if too short\n",
    "            padding_length = max_length - len(token_ids)\n",
    "            return token_ids + [pad_id] * padding_length\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the number of conversation pairs.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method is required by PyTorch's Dataset interface.\n",
    "        \"\"\"\n",
    "        return len(self.conversation_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a single conversation pair as tensors.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method is called by PyTorch's DataLoader to get individual\n",
    "        samples. We return tensors ready for model training.\n",
    "        \"\"\"\n",
    "        pair = self.conversation_pairs[idx]\n",
    "        \n",
    "        # Encode input and target texts\n",
    "        input_ids = self.tokenizer.encode(\n",
    "            pair.input_text, \n",
    "            add_special_tokens=True,\n",
    "            preprocessor=self.preprocessor\n",
    "        )\n",
    "        \n",
    "        target_ids = self.tokenizer.encode(\n",
    "            pair.target_text,\n",
    "            add_special_tokens=True, \n",
    "            preprocessor=self.preprocessor\n",
    "        )\n",
    "        \n",
    "        # Pad sequences\n",
    "        input_ids = self._pad_sequence(input_ids, self.max_length)\n",
    "        target_ids = self._pad_sequence(target_ids, self.max_length)\n",
    "        \n",
    "        # Create attention masks (1 for real tokens, 0 for padding)\n",
    "        pad_id = self.tokenizer.token_to_id.get('<PAD>', 0)\n",
    "        input_mask = [1 if token_id != pad_id else 0 for token_id in input_ids]\n",
    "        target_mask = [1 if token_id != pad_id else 0 for token_id in target_ids]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'input_mask': torch.tensor(input_mask, dtype=torch.long),\n",
    "            'target_ids': torch.tensor(target_ids, dtype=torch.long),\n",
    "            'target_mask': torch.tensor(target_mask, dtype=torch.long),\n",
    "            'input_text': pair.input_text,\n",
    "            'target_text': pair.target_text\n",
    "        }\n",
    "    \n",
    "    def get_sample_batch(self, batch_size: int = 4) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample batch for testing and demonstration.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method helps visualize what the model will receive\n",
    "        during training.\n",
    "        \"\"\"\n",
    "        dataloader = DataLoader(self, batch_size=batch_size, shuffle=True)\n",
    "        return next(iter(dataloader))\n",
    "    \n",
    "    def analyze_dataset(self):\n",
    "        \"\"\"\n",
    "        Analyze the dataset and provide statistics.\n",
    "        \n",
    "        Educational Note:\n",
    "        Dataset analysis helps understand data characteristics\n",
    "        and guide preprocessing decisions.\n",
    "        \"\"\"\n",
    "        input_lengths = []\n",
    "        target_lengths = []\n",
    "        \n",
    "        for pair in self.conversation_pairs:\n",
    "            input_tokens = self.tokenizer.tokenize(pair.input_text)\n",
    "            target_tokens = self.tokenizer.tokenize(pair.target_text)\n",
    "            \n",
    "            input_lengths.append(len(input_tokens))\n",
    "            target_lengths.append(len(target_tokens))\n",
    "        \n",
    "        print(\"Dataset Analysis:\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Total conversation pairs: {len(self.conversation_pairs)}\")\n",
    "        print(f\"Average input length: {np.mean(input_lengths):.1f} tokens\")\n",
    "        print(f\"Average target length: {np.mean(target_lengths):.1f} tokens\")\n",
    "        print(f\"Max input length: {max(input_lengths)} tokens\")\n",
    "        print(f\"Max target length: {max(target_lengths)} tokens\")\n",
    "        print(f\"Sequences longer than max_length ({self.max_length}):\")\n",
    "        print(f\"  Input: {sum(1 for l in input_lengths if l > self.max_length)}\")\n",
    "        print(f\"  Target: {sum(1 for l in target_lengths if l > self.max_length)}\")\n",
    "        \n",
    "        # Visualize length distributions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "        \n",
    "        ax1.hist(input_lengths, bins=20, alpha=0.7, label='Input')\n",
    "        ax1.axvline(self.max_length, color='red', linestyle='--', \n",
    "                   label=f'Max Length ({self.max_length})')\n",
    "        ax1.set_title('Input Length Distribution')\n",
    "        ax1.set_xlabel('Number of Tokens')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.legend()\n",
    "        \n",
    "        ax2.hist(target_lengths, bins=20, alpha=0.7, label='Target', color='orange')\n",
    "        ax2.axvline(self.max_length, color='red', linestyle='--',\n",
    "                   label=f'Max Length ({self.max_length})')\n",
    "        ax2.set_title('Target Length Distribution')\n",
    "        ax2.set_xlabel('Number of Tokens')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparison with Pre-built Tokenizers\n",
    "\n",
    "Now let's compare our custom tokenizer with professional tokenizers like those from Hugging Face. This will help you understand the trade-offs and when to use each approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This section demonstrates the concepts even if transformers library isn't installed\n",
    "# In a real environment, you would install: pip install transformers\n",
    "\n",
    "def compare_tokenizers(text_samples: List[str]):\n",
    "    \"\"\"\n",
    "    Compare our custom tokenizer with pre-built solutions.\n",
    "    \n",
    "    Educational Note:\n",
    "    This comparison helps understand:\n",
    "    - Vocabulary efficiency (tokens per text)\n",
    "    - Handling of unknown words\n",
    "    - Subword tokenization benefits\n",
    "    - Processing speed differences\n",
    "    \"\"\"\n",
    "    print(\"Tokenizer Comparison\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Initialize our custom tokenizer\n",
    "    custom_tokenizer = EducationalTokenizer(max_vocab_size=1000)\n",
    "    preprocessor = TextPreprocessor()\n",
    "    \n",
    "    # Build vocabulary from samples\n",
    "    custom_tokenizer.build_vocabulary(text_samples, preprocessor)\n",
    "    \n",
    "    # Try to import and use Hugging Face tokenizer\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        # Use a simple pre-trained tokenizer\n",
    "        hf_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        hf_available = True\n",
    "        print(\"✓ Hugging Face tokenizer loaded successfully\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"⚠ Hugging Face transformers not available\")\n",
    "        print(\"  Install with: pip install transformers\")\n",
    "        print(\"  Showing custom tokenizer analysis only\")\n",
    "        hf_available = False\n",
    "        hf_tokenizer = None\n",
    "    \n",
    "    # Compare tokenization results\n",
    "    comparison_results = []\n",
    "    \n",
    "    for i, text in enumerate(text_samples[:5]):  # Limit to first 5 samples\n",
    "        print(f\"\\nExample {i+1}: '{text}'\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Custom tokenizer\n",
    "        custom_tokens = custom_tokenizer.tokenize(preprocessor.preprocess(text))\n",
    "        custom_ids = custom_tokenizer.encode(text, preprocessor=preprocessor)\n",
    "        \n",
    "        print(f\"Custom Tokenizer:\")\n",
    "        print(f\"  Tokens: {custom_tokens}\")\n",
    "        print(f\"  Token count: {len(custom_tokens)}\")\n",
    "        print(f\"  Token IDs: {custom_ids[:10]}{'...' if len(custom_ids) > 10 else ''}\")\n",
    "        \n",
    "        result = {\n",
    "            'text': text,\n",
    "            'custom_token_count': len(custom_tokens),\n",
    "            'custom_tokens': custom_tokens\n",
    "        }\n",
    "        \n",
    "        # Hugging Face tokenizer (if available)\n",
    "        if hf_available:\n",
    "            hf_encoding = hf_tokenizer(text, return_tensors='pt')\n",
    "            hf_tokens = hf_tokenizer.tokenize(text)\n",
    "            hf_ids = hf_encoding['input_ids'][0].tolist()\n",
    "            \n",
    "            print(f\"\\nHugging Face Tokenizer (BERT):\")\n",
    "            print(f\"  Tokens: {hf_tokens}\")\n",
    "            print(f\"  Token count: {len(hf_tokens)}\")\n",
    "            print(f\"  Token IDs: {hf_ids[:10]}{'...' if len(hf_ids) > 10 else ''}\")\n",
    "            \n",
    "            result.update({\n",
    "                'hf_token_count': len(hf_tokens),\n",
    "                'hf_tokens': hf_tokens\n",
    "            })\n",
    "        \n",
    "        comparison_results.append(result)\n",
    "    \n",
    "    # Analysis and insights\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"ANALYSIS AND INSIGHTS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Calculate average token counts\n",
    "    custom_avg = np.mean([r['custom_token_count'] for r in comparison_results])\n",
    "    \n",
    "    print(f\"\\n1. Token Efficiency:\")\n",
    "    print(f\"   Custom tokenizer average: {custom_avg:.1f} tokens per text\")\n",
    "    \n",
    "    if hf_available:\n",
    "        hf_avg = np.mean([r['hf_token_count'] for r in comparison_results])\n",
    "        print(f\"   Hugging Face average: {hf_avg:.1f} tokens per text\")\n",
    "        \n",
    "        efficiency_ratio = custom_avg / hf_avg\n",
    "        if efficiency_ratio > 1.2:\n",
    "            print(f\"   → Custom tokenizer uses {efficiency_ratio:.1f}x more tokens\")\n",
    "            print(f\"     This is expected for word-level vs subword tokenization\")\n",
    "        elif efficiency_ratio < 0.8:\n",
    "            print(f\"   → Custom tokenizer is more efficient ({1/efficiency_ratio:.1f}x fewer tokens)\")\n",
    "        else:\n",
    "            print(f\"   → Similar efficiency between tokenizers\")\n",
    "    \n",
    "    print(f\"\\n2. Vocabulary Characteristics:\")\n",
    "    print(f\"   Custom vocabulary size: {custom_tokenizer.vocab_size}\")\n",
    "    \n",
    "    if hf_available:\n",
    "        print(f\"   BERT vocabulary size: {hf_tokenizer.vocab_size:,}\")\n",
    "        print(f\"   → Pre-trained tokenizers have much larger vocabularies\")\n",
    "        print(f\"     This allows better handling of rare/unknown words\")\n",
    "    \n",
    "    print(f\"\\n3. Key Differences:\")\n",
    "    print(f\"   Custom Tokenizer (Word-level):\")\n",
    "    print(f\"   ✓ Simple and interpretable\")\n",
    "    print(f\"   ✓ Fast training and inference\")\n",
    "    print(f\"   ✓ Good for small, domain-specific datasets\")\n",
    "    print(f\"   ✗ Large vocabulary for diverse text\")\n",
    "    print(f\"   ✗ Poor handling of unknown words\")\n",
    "    print(f\"   ✗ No subword information\")\n",
    "    \n",
    "    if hf_available:\n",
    "        print(f\"\\n   Pre-trained Tokenizer (Subword):\")\n",
    "        print(f\"   ✓ Handles unknown words well\")\n",
    "        print(f\"   ✓ Efficient vocabulary usage\")\n",
    "        print(f\"   ✓ Captures morphological patterns\")\n",
    "        print(f\"   ✓ Works across different domains\")\n",
    "        print(f\"   ✗ More complex to understand\")\n",
    "        print(f\"   ✗ Requires pre-training or large datasets\")\n",
    "        print(f\"   ✗ Less interpretable tokens\")\n",
    "    \n",
    "    print(f\"\\n4. When to Use Each:\")\n",
    "    print(f\"   Use Custom Tokenizer when:\")\n",
    "    print(f\"   - Learning tokenization concepts\")\n",
    "    print(f\"   - Working with small, controlled datasets\")\n",
    "    print(f\"   - Need full control over vocabulary\")\n",
    "    print(f\"   - Interpretability is crucial\")\n",
    "    \n",
    "    print(f\"\\n   Use Pre-trained Tokenizer when:\")\n",
    "    print(f\"   - Building production systems\")\n",
    "    print(f\"   - Working with diverse text data\")\n",
    "    print(f\"   - Need robust unknown word handling\")\n",
    "    print(f\"   - Want to leverage pre-trained models\")\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Test the comparison\n",
    "sample_texts = [\n",
    "    \"Hello, how are you doing today?\",\n",
    "    \"What's the weather like?\",\n",
    "    \"I don't understand this complicated explanation.\",\n",
    "    \"Can you help me with machine learning?\",\n",
    "    \"That's absolutely fantastic!\"\n",
    "]\n",
    "\n",
    "comparison_results = compare_tokenizers(sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Practical Demonstration with Real Data\n",
    "\n",
    "Let's put everything together and work with our actual conversation data to see the complete preprocessing pipeline in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the actual conversation data\n",
    "print(\"Loading Conversation Data...\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Path to our conversation data\n",
    "data_path = '../data/conversations/simple_qa_pairs.json'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Warning: Data file not found at {data_path}\")\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    \n",
    "    # Create sample data if file doesn't exist\n",
    "    sample_conversations = [\n",
    "        {\n",
    "            \"id\": \"demo_001\",\n",
    "            \"messages\": [\n",
    "                {\"speaker\": \"user\", \"text\": \"Hello there!\"},\n",
    "                {\"speaker\": \"bot\", \"text\": \"Hi! How can I help you today?\"}\n",
    "            ],\n",
    "            \"context\": \"greeting\",\n",
    "            \"metadata\": {\"category\": \"greeting\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"demo_002\",\n",
    "            \"messages\": [\n",
    "                {\"speaker\": \"user\", \"text\": \"What is machine learning?\"},\n",
    "                {\"speaker\": \"bot\", \"text\": \"Machine learning is a method of data analysis that automates analytical model building.\"}\n",
    "            ],\n",
    "            \"context\": \"educational\",\n",
    "            \"metadata\": {\"category\": \"education\"}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Save sample data\n",
    "    os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
    "    with open(data_path, 'w') as f:\n",
    "        json.dump(sample_conversations, f, indent=2)\n",
    "    \n",
    "    print(f\"Sample data created at {data_path}\")\n",
    "\n",
    "# Initialize components\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=False,  # Keep punctuation for chatbots\n",
    "    expand_contractions=True,\n",
    "    remove_extra_whitespace=True\n",
    ")\n",
    "\n",
    "tokenizer = EducationalTokenizer(\n",
    "    max_vocab_size=1000,\n",
    "    min_frequency=1\n",
    ")\n",
    "\n",
    "# Load conversations and extract text for vocabulary building\n",
    "with open(data_path, 'r') as f:\n",
    "    conversations = json.load(f)\n",
    "\n",
    "all_texts = []\n",
    "for conv in conversations:\n",
    "    for message in conv.get('messages', []):\n",
    "        text = message.get('text', '')\n",
    "        if text:\n",
    "            all_texts.append(text)\n",
    "\n",
    "print(f\"Extracted {len(all_texts)} messages from conversations\")\n",
    "\n",
    "# Build vocabulary\n",
    "print(\"\\nBuilding vocabulary from conversation data...\")\n",
    "tokenizer.build_vocabulary(all_texts, preprocessor)\n",
    "\n",
    "# Analyze vocabulary\n",
    "print(\"\\nVocabulary Analysis:\")\n",
    "vocab_stats = analyze_vocabulary(tokenizer, all_texts, preprocessor)\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nCreating conversational dataset...\")\n",
    "dataset = ConversationalDataset(\n",
    "    data_path=data_path,\n",
    "    tokenizer=tokenizer,\n",
    "    preprocessor=preprocessor,\n",
    "    max_length=64,  # Shorter for demo data\n",
    "    include_context=False\n",
    ")\n",
    "\n",
    "# Analyze dataset\n",
    "dataset.analyze_dataset()\n",
    "\n",
    "# Show sample batch\n",
    "print(\"\\nSample Batch from Dataset:\")\n",
    "print(\"=\" * 30)\n",
    "sample_batch = dataset.get_sample_batch(batch_size=2)\n",
    "\n",
    "for i in range(len(sample_batch['input_text'])):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Input text: '{sample_batch['input_text'][i]}'\")\n",
    "    print(f\"  Target text: '{sample_batch['target_text'][i]}'\")\n",
    "    print(f\"  Input IDs: {sample_batch['input_ids'][i][:10].tolist()}...\")\n",
    "    print(f\"  Target IDs: {sample_batch['target_ids'][i][:10].tolist()}...\")\n",
    "    \n",
    "    # Decode to verify\n",
    "    decoded_input = tokenizer.decode(sample_batch['input_ids'][i].tolist())\n",
    "    decoded_target = tokenizer.decode(sample_batch['target_ids'][i].tolist())\n",
    "    print(f\"  Decoded input: '{decoded_input}'\")\n",
    "    print(f\"  Decoded target: '{decoded_target}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Takeaways\n",
    "\n",
    "In this notebook, we've covered the essential concepts of text preprocessing and tokenization for chatbot development:\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Text Preprocessing Pipeline**\n",
    "   - Contraction expansion\n",
    "   - Case normalization\n",
    "   - Punctuation handling\n",
    "   - Whitespace normalization\n",
    "\n",
    "2. **Custom Tokenizer Implementation**\n",
    "   - Vocabulary building from training data\n",
    "   - Token-to-ID mappings\n",
    "   - Special tokens for sequence modeling\n",
    "   - Encoding and decoding processes\n",
    "\n",
    "3. **Vocabulary Analysis**\n",
    "   - Frequency distributions and Zipf's law\n",
    "   - Vocabulary diversity metrics\n",
    "   - Text length statistics\n",
    "   - Visualization techniques\n",
    "\n",
    "4. **PyTorch Dataset Integration**\n",
    "   - Conversational data handling\n",
    "   - Sequence padding and truncation\n",
    "   - Attention masks\n",
    "   - Batch processing\n",
    "\n",
    "5. **Comparison with Production Tokenizers**\n",
    "   - Word-level vs subword tokenization\n",
    "   - Trade-offs in vocabulary size and efficiency\n",
    "   - When to use custom vs pre-built solutions\n",
    "\n",
    "### Key Insights:\n",
    "\n",
    "- **Preprocessing matters**: Clean, consistent text leads to better model performance\n",
    "- **Vocabulary size is a trade-off**: Larger vocabularies capture more nuance but require more memory\n",
    "- **Special tokens are crucial**: They help models understand sequence boundaries and handle unknown words\n",
    "- **Subword tokenization is powerful**: It handles unknown words better than word-level approaches\n",
    "- **Dataset design affects training**: Proper padding, masking, and batching are essential for efficient training\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In the next notebook, we'll use these preprocessing tools to build neural network architectures for our chatbot, starting with simple feedforward networks and progressing to more sophisticated architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercises and Challenges\n",
    "\n",
    "Try these exercises to deepen your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Experiment with different preprocessing options\n",
    "print(\"Exercise 1: Preprocessing Experiments\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# TODO: Try different preprocessing configurations and compare results\n",
    "# 1. Create preprocessors with different settings\n",
    "# 2. Compare vocabulary sizes and token distributions\n",
    "# 3. Analyze the impact on a sample text\n",
    "\n",
    "sample_text = \"I don't think that's right! What's your opinion?\"\n",
    "\n",
    "configs = [\n",
    "    {'name': 'Minimal', 'remove_punctuation': False, 'lowercase': False},\n",
    "    {'name': 'Standard', 'remove_punctuation': False, 'lowercase': True},\n",
    "    {'name': 'Aggressive', 'remove_punctuation': True, 'lowercase': True}\n",
    "]\n",
    "\n",
    "print(f\"Original text: '{sample_text}'\")\n",
    "print()\n",
    "\n",
    "for config in configs:\n",
    "    name = config.pop('name')\n",
    "    preprocessor = TextPreprocessor(**config)\n",
    "    processed = preprocessor.preprocess(sample_text)\n",
    "    print(f\"{name:12}: '{processed}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Exercise 2: Custom Tokenization Strategy\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Implement a character-level tokenizer\n",
    "# 1. Create a tokenizer that works at character level\n",
    "# 2. Compare vocabulary size with word-level tokenizer\n",
    "# 3. Analyze pros and cons for chatbot applications\n",
    "\n",
    "class CharacterTokenizer:\n",
    "    \"\"\"Simple character-level tokenizer for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.char_to_id = {}\n",
    "        self.id_to_char = {}\n",
    "        self.vocab_size = 0\n",
    "        \n",
    "        # Add special tokens\n",
    "        special_chars = ['<PAD>', '<UNK>', '<SOS>', '<EOS>']\n",
    "        for char in special_chars:\n",
    "            self._add_char(char)\n",
    "    \n",
    "    def _add_char(self, char):\n",
    "        if char not in self.char_to_id:\n",
    "            char_id = len(self.char_to_id)\n",
    "            self.char_to_id[char] = char_id\n",
    "            self.id_to_char[char_id] = char\n",
    "            self.vocab_size += 1\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        for text in texts:\n",
    "            for char in text:\n",
    "                self._add_char(char)\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_id.get(char, self.char_to_id['<UNK>']) for char in text]\n",
    "    \n",
    "    def decode(self, char_ids):\n",
    "        return ''.join([self.id_to_char.get(cid, '<UNK>') for cid in char_ids])\n",
    "\n",
    "# Compare character vs word tokenization\n",
    "char_tokenizer = CharacterTokenizer()\n",
    "char_tokenizer.build_vocabulary(all_texts)\n",
    "\n",
    "test_text = \"Hello, world!\"\n",
    "word_tokens = tokenizer.encode(test_text, preprocessor=preprocessor)\n",
    "char_tokens = char_tokenizer.encode(test_text)\n",
    "\n",
    "print(f\"Test text: '{test_text}'\")\n",
    "print(f\"Word tokenizer: {len(word_tokens)} tokens, vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Char tokenizer: {len(char_tokens)} tokens, vocab size: {char_tokenizer.vocab_size}\")\n",
    "print(f\"Word tokens: {word_tokens}\")\n",
    "print(f\"Char tokens: {char_tokens}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Exercise 3: Dataset Optimization\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# TODO: Experiment with different max_length values\n",
    "# 1. Create datasets with different max_length settings\n",
    "# 2. Analyze the trade-offs in truncation vs padding\n",
    "# 3. Find the optimal max_length for your data\n",
    "\n",
    "max_lengths = [16, 32, 64, 128]\n",
    "truncation_stats = []\n",
    "\n",
    "for max_len in max_lengths:\n",
    "    temp_dataset = ConversationalDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "        max_length=max_len\n",
    "    )\n",
    "    \n",
    "    # Calculate truncation statistics\n",
    "    truncated_inputs = 0\n",
    "    truncated_targets = 0\n",
    "    total_padding = 0\n",
    "    \n",
    "    for i in range(len(temp_dataset)):\n",
    "        sample = temp_dataset[i]\n",
    "        \n",
    "        # Count non-padding tokens\n",
    "        input_length = sample['input_mask'].sum().item()\n",
    "        target_length = sample['target_mask'].sum().item()\n",
    "        \n",
    "        if input_length == max_len:\n",
    "            truncated_inputs += 1\n",
    "        if target_length == max_len:\n",
    "            truncated_targets += 1\n",
    "        \n",
    "        total_padding += (max_len - input_length) + (max_len - target_length)\n",
    "    \n",
    "    truncation_stats.append({\n",
    "        'max_length': max_len,\n",
    "        'truncated_inputs': truncated_inputs,\n",
    "        'truncated_targets': truncated_targets,\n",
    "        'avg_padding': total_padding / (len(temp_dataset) * 2)\n",
    "    })\n",
    "\n",
    "print(\"Max Length Analysis:\")\n",
    "print(f\"{'Max Len':<8} {'Trunc Input':<12} {'Trunc Target':<13} {'Avg Padding':<12}\")\n",
    "print(\"-\" * 50)\n",
    "for stats in truncation_stats:\n",
    "    print(f\"{stats['max_length']:<8} {stats['truncated_inputs']:<12} \"\n",
    "          f\"{stats['truncated_targets']:<13} {stats['avg_padding']:<12.1f}\")\n",
    "\n",
    "print(\"\\nRecommendation: Choose max_length that minimizes truncation while keeping padding reasonable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}