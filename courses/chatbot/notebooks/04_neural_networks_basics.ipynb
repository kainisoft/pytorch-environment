{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks Basics for Text Classification\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand feedforward neural network architecture\n",
    "- Implement a simple neural network for text classification\n",
    "- Learn about activation functions, loss functions, and optimization\n",
    "- Master the training loop: forward pass, loss calculation, backpropagation\n",
    "- Visualize network architecture, weights, and training progress\n",
    "- Debug common neural network issues\n",
    "\n",
    "**Prerequisites:**\n",
    "- PyTorch fundamentals (Notebook 01)\n",
    "- Tensor operations for NLP (Notebook 02)\n",
    "- Data preprocessing and tokenization (Notebook 03)\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Neural networks are the foundation of modern deep learning. Before building complex chatbots, we need to understand how basic neural networks work. This notebook will guide you through implementing a simple feedforward neural network for text classification, which forms the building blocks for more advanced architectures.\n",
    "\n",
    "We'll classify text messages as either questions or statements, providing a practical introduction to neural networks in NLP contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add utils to path for importing our custom modules\n",
    "sys.path.append('../utils')\n",
    "\n",
    "# Data science and visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Import our custom utilities\n",
    "from model_utils import count_parameters, initialize_weights, get_model_summary\n",
    "from visualization_utils import plot_training_progress, plot_confusion_matrix\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Neural Network Components\n",
    "\n",
    "Before implementing our neural network, let's understand the key components:\n",
    "\n",
    "### 2.1 Activation Functions\n",
    "\n",
    "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_activation_functions():\n",
    "    \"\"\"\n",
    "    Educational demonstration of common activation functions.\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Activation functions determine how neurons \"fire\"\n",
    "    - Different functions have different properties and use cases\n",
    "    - Choice of activation affects training dynamics\n",
    "    \"\"\"\n",
    "    # Create input range\n",
    "    x = torch.linspace(-5, 5, 1000)\n",
    "    \n",
    "    # Define activation functions\n",
    "    activations = {\n",
    "        'ReLU': F.relu,\n",
    "        'Sigmoid': torch.sigmoid,\n",
    "        'Tanh': torch.tanh,\n",
    "        'Leaky ReLU': lambda x: F.leaky_relu(x, 0.1),\n",
    "        'GELU': F.gelu\n",
    "    }\n",
    "    \n",
    "    # Plot activation functions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (name, func) in enumerate(activations.items()):\n",
    "        y = func(x)\n",
    "        axes[i].plot(x.numpy(), y.numpy(), linewidth=2)\n",
    "        axes[i].set_title(f'{name} Activation')\n",
    "        axes[i].set_xlabel('Input')\n",
    "        axes[i].set_ylabel('Output')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "        axes[i].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "        axes[i].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplot\n",
    "    axes[-1].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational explanations\n",
    "    print(\"Educational Guide to Activation Functions:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. ReLU (Rectified Linear Unit):\")\n",
    "    print(\"   - Most popular activation function\")\n",
    "    print(\"   - Simple: max(0, x)\")\n",
    "    print(\"   - Pros: Fast computation, helps with vanishing gradient\")\n",
    "    print(\"   - Cons: Can cause 'dying ReLU' problem\")\n",
    "    \n",
    "    print(\"\\n2. Sigmoid:\")\n",
    "    print(\"   - Outputs between 0 and 1\")\n",
    "    print(\"   - Good for binary classification output\")\n",
    "    print(\"   - Pros: Smooth, interpretable as probability\")\n",
    "    print(\"   - Cons: Vanishing gradient problem\")\n",
    "    \n",
    "    print(\"\\n3. Tanh (Hyperbolic Tangent):\")\n",
    "    print(\"   - Outputs between -1 and 1\")\n",
    "    print(\"   - Zero-centered (better than sigmoid)\")\n",
    "    print(\"   - Pros: Stronger gradients than sigmoid\")\n",
    "    print(\"   - Cons: Still suffers from vanishing gradients\")\n",
    "    \n",
    "    print(\"\\n4. Leaky ReLU:\")\n",
    "    print(\"   - Allows small negative values\")\n",
    "    print(\"   - Solves dying ReLU problem\")\n",
    "    print(\"   - Pros: No dead neurons\")\n",
    "    print(\"   - Cons: Introduces hyperparameter (slope)\")\n",
    "    \n",
    "    print(\"\\n5. GELU (Gaussian Error Linear Unit):\")\n",
    "    print(\"   - Smooth approximation of ReLU\")\n",
    "    print(\"   - Used in modern transformers\")\n",
    "    print(\"   - Pros: Smooth, probabilistically motivated\")\n",
    "    print(\"   - Cons: More computationally expensive\")\n",
    "\n",
    "# Demonstrate activation functions\n",
    "demonstrate_activation_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loss Functions\n",
    "\n",
    "Loss functions measure how well our model's predictions match the true labels. They guide the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_loss_functions():\n",
    "    \"\"\"\n",
    "    Educational demonstration of common loss functions for classification.\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Loss functions quantify prediction errors\n",
    "    - Different tasks require different loss functions\n",
    "    - Loss function choice affects training behavior\n",
    "    \"\"\"\n",
    "    # Create sample predictions and targets\n",
    "    # For binary classification (0 or 1)\n",
    "    predictions = torch.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    # Calculate losses for different scenarios\n",
    "    target_0 = torch.zeros_like(predictions)  # True label is 0\n",
    "    target_1 = torch.ones_like(predictions)   # True label is 1\n",
    "    \n",
    "    # Binary Cross-Entropy Loss\n",
    "    bce_loss_0 = F.binary_cross_entropy(predictions, target_0, reduction='none')\n",
    "    bce_loss_1 = F.binary_cross_entropy(predictions, target_1, reduction='none')\n",
    "    \n",
    "    # Mean Squared Error (for comparison)\n",
    "    mse_loss_0 = F.mse_loss(predictions, target_0, reduction='none')\n",
    "    mse_loss_1 = F.mse_loss(predictions, target_1, reduction='none')\n",
    "    \n",
    "    # Plot loss functions\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Binary Cross-Entropy\n",
    "    axes[0].plot(predictions.numpy(), bce_loss_0.numpy(), \n",
    "                label='True label = 0', linewidth=2)\n",
    "    axes[0].plot(predictions.numpy(), bce_loss_1.numpy(), \n",
    "                label='True label = 1', linewidth=2)\n",
    "    axes[0].set_title('Binary Cross-Entropy Loss')\n",
    "    axes[0].set_xlabel('Predicted Probability')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_ylim(0, 5)\n",
    "    \n",
    "    # Mean Squared Error\n",
    "    axes[1].plot(predictions.numpy(), mse_loss_0.numpy(), \n",
    "                label='True label = 0', linewidth=2)\n",
    "    axes[1].plot(predictions.numpy(), mse_loss_1.numpy(), \n",
    "                label='True label = 1', linewidth=2)\n",
    "    axes[1].set_title('Mean Squared Error Loss')\n",
    "    axes[1].set_xlabel('Predicted Probability')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational explanations\n",
    "    print(\"Educational Guide to Loss Functions:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"1. Binary Cross-Entropy (BCE):\")\n",
    "    print(\"   - Standard for binary classification\")\n",
    "    print(\"   - Penalizes confident wrong predictions heavily\")\n",
    "    print(\"   - Formula: -[y*log(p) + (1-y)*log(1-p)]\")\n",
    "    print(\"   - Use with sigmoid activation\")\n",
    "    \n",
    "    print(\"\\n2. Categorical Cross-Entropy:\")\n",
    "    print(\"   - For multi-class classification\")\n",
    "    print(\"   - Extends BCE to multiple classes\")\n",
    "    print(\"   - Use with softmax activation\")\n",
    "    \n",
    "    print(\"\\n3. Mean Squared Error (MSE):\")\n",
    "    print(\"   - Typically for regression tasks\")\n",
    "    print(\"   - Can be used for classification but less effective\")\n",
    "    print(\"   - Treats all errors equally (quadratic penalty)\")\n",
    "    \n",
    "    print(\"\\nKey Insights:\")\n",
    "    print(\"- BCE provides stronger gradients for wrong predictions\")\n",
    "    print(\"- Loss function choice should match your task type\")\n",
    "    print(\"- Cross-entropy losses work well with probability outputs\")\n",
    "\n",
    "# Demonstrate loss functions\n",
    "demonstrate_loss_functions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating Training Data\n",
    "\n",
    "Let's create a simple dataset for text classification. We'll classify messages as either questions or statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for text classification\n",
    "sample_data = [\n",
    "    # Questions (label = 1)\n",
    "    (\"What is your name?\", 1),\n",
    "    (\"How are you doing today?\", 1),\n",
    "    (\"Where do you live?\", 1),\n",
    "    (\"Can you help me?\", 1),\n",
    "    (\"What time is it?\", 1),\n",
    "    (\"How does this work?\", 1),\n",
    "    (\"Why is the sky blue?\", 1),\n",
    "    (\"When will you arrive?\", 1),\n",
    "    (\"Who is your favorite author?\", 1),\n",
    "    (\"Which option is better?\", 1),\n",
    "    (\"Are you available tomorrow?\", 1),\n",
    "    (\"Do you like pizza?\", 1),\n",
    "    (\"Is this correct?\", 1),\n",
    "    (\"Could you explain this?\", 1),\n",
    "    (\"Would you like some coffee?\", 1),\n",
    "    \n",
    "    # Statements (label = 0)\n",
    "    (\"My name is John.\", 0),\n",
    "    (\"I am doing well today.\", 0),\n",
    "    (\"I live in New York.\", 0),\n",
    "    (\"I can help you with that.\", 0),\n",
    "    (\"It is three o'clock.\", 0),\n",
    "    (\"This works by using electricity.\", 0),\n",
    "    (\"The sky appears blue due to light scattering.\", 0),\n",
    "    (\"I will arrive at five.\", 0),\n",
    "    (\"My favorite author is Shakespeare.\", 0),\n",
    "    (\"The first option is better.\", 0),\n",
    "    (\"I am available tomorrow.\", 0),\n",
    "    (\"I love pizza.\", 0),\n",
    "    (\"This is correct.\", 0),\n",
    "    (\"Let me explain this concept.\", 0),\n",
    "    (\"I would love some coffee.\", 0),\n",
    "    \n",
    "    # Additional examples for better training\n",
    "    (\"Tell me about machine learning.\", 1),\n",
    "    (\"Machine learning is fascinating.\", 0),\n",
    "    (\"How do neural networks work?\", 1),\n",
    "    (\"Neural networks process information.\", 0),\n",
    "    (\"What is deep learning?\", 1),\n",
    "    (\"Deep learning uses multiple layers.\", 0),\n",
    "]\n",
    "\n",
    "print(f\"Created dataset with {len(sample_data)} examples\")\n",
    "print(f\"Questions: {sum(1 for _, label in sample_data if label == 1)}\")\n",
    "print(f\"Statements: {sum(1 for _, label in sample_data if label == 0)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample data:\")\n",
    "for i, (text, label) in enumerate(sample_data[:5]):\n",
    "    label_name = \"Question\" if label == 1 else \"Statement\"\n",
    "    print(f\"{i+1}. '{text}' -> {label_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Tokenizer for Neural Networks\n",
    "\n",
    "We'll create a simplified tokenizer specifically for this neural network example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Simplified tokenizer for neural network demonstration.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrates basic tokenization concepts\n",
    "    - Shows vocabulary building and encoding\n",
    "    - Handles padding for batch processing\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Tokenization converts text to numbers\n",
    "    - Vocabulary maps words to unique IDs\n",
    "    - Padding ensures consistent sequence lengths\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_vocab_size=1000):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.word_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.id_to_word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.vocab_size = 2\n",
    "    \n",
    "    def build_vocabulary(self, texts):\n",
    "        \"\"\"\n",
    "        Build vocabulary from training texts.\n",
    "        \n",
    "        Educational Note:\n",
    "        We collect all unique words and assign them IDs.\n",
    "        This creates a mapping between words and numbers.\n",
    "        \"\"\"\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Count word frequencies\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                # Simple preprocessing: remove punctuation\n",
    "                word = word.strip('.,!?;:')\n",
    "                if word:\n",
    "                    word_counts[word] = word_counts.get(word, 0) + 1\n",
    "        \n",
    "        # Add most frequent words to vocabulary\n",
    "        sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for word, count in sorted_words:\n",
    "            if self.vocab_size >= self.max_vocab_size:\n",
    "                break\n",
    "            \n",
    "            if word not in self.word_to_id:\n",
    "                word_id = self.vocab_size\n",
    "                self.word_to_id[word] = word_id\n",
    "                self.id_to_word[word_id] = word\n",
    "                self.vocab_size += 1\n",
    "        \n",
    "        print(f\"Built vocabulary with {self.vocab_size} words\")\n",
    "        print(f\"Most common words: {[word for word, _ in sorted_words[:10]]}\")\n",
    "    \n",
    "    def encode(self, text, max_length=20):\n",
    "        \"\"\"\n",
    "        Convert text to sequence of token IDs.\n",
    "        \n",
    "        Educational Note:\n",
    "        - Split text into words\n",
    "        - Convert each word to its ID\n",
    "        - Pad or truncate to fixed length\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            word = word.strip('.,!?;:')\n",
    "            if word:\n",
    "                token_id = self.word_to_id.get(word, 1)  # 1 is <UNK>\n",
    "                token_ids.append(token_id)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if len(token_ids) < max_length:\n",
    "            token_ids.extend([0] * (max_length - len(token_ids)))  # 0 is <PAD>\n",
    "        else:\n",
    "            token_ids = token_ids[:max_length]\n",
    "        \n",
    "        return token_ids\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Convert token IDs back to text.\n",
    "        \n",
    "        Educational Note:\n",
    "        This is the reverse of encoding - useful for debugging\n",
    "        and understanding what the model sees.\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        for token_id in token_ids:\n",
    "            word = self.id_to_word.get(token_id, '<UNK>')\n",
    "            if word != '<PAD>':\n",
    "                words.append(word)\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Build tokenizer\n",
    "tokenizer = SimpleTokenizer()\n",
    "texts = [text for text, _ in sample_data]\n",
    "tokenizer.build_vocabulary(texts)\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"What is machine learning?\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nTokenization test:\")\n",
    "print(f\"Original: '{test_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Class\n",
    "\n",
    "Let's create a PyTorch Dataset class for our text classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for text classification.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrates PyTorch Dataset pattern\n",
    "    - Shows how to prepare data for neural networks\n",
    "    - Handles tokenization and tensor conversion\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Dataset classes provide standardized data access\n",
    "    - __len__ and __getitem__ are required methods\n",
    "    - Data is converted to tensors for PyTorch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length=20):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: List of (text, label) tuples\n",
    "            tokenizer: Tokenizer for encoding text\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method is required by PyTorch to know dataset size.\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method is called by DataLoader to get individual samples.\n",
    "        We return tensors that can be processed by neural networks.\n",
    "        \"\"\"\n",
    "        text, label = self.data[idx]\n",
    "        \n",
    "        # Encode text to token IDs\n",
    "        token_ids = self.tokenizer.encode(text, self.max_length)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        input_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        \n",
    "        return input_tensor, label_tensor\n",
    "\n",
    "# Create dataset\n",
    "dataset = TextClassificationDataset(sample_data, tokenizer)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Test dataset\n",
    "sample_input, sample_label = dataset[0]\n",
    "print(f\"\\nSample from dataset:\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Input tensor: {sample_input}\")\n",
    "print(f\"Label: {sample_label}\")\n",
    "print(f\"Decoded text: '{tokenizer.decode(sample_input.tolist())}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture\n",
    "\n",
    "Now let's implement a simple feedforward neural network for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple feedforward neural network for text classification.\n",
    "    \n",
    "    Architecture:\n",
    "    1. Embedding layer: Convert token IDs to dense vectors\n",
    "    2. Global average pooling: Average embeddings across sequence\n",
    "    3. Hidden layers: Feedforward layers with activation\n",
    "    4. Output layer: Single neuron for binary classification\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrates basic neural network components\n",
    "    - Shows how to handle variable-length sequences\n",
    "    - Illustrates embedding layers for text\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Embeddings convert discrete tokens to continuous vectors\n",
    "    - Pooling reduces sequence to fixed-size representation\n",
    "    - Multiple layers allow learning complex patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128, \n",
    "                 num_classes=1, dropout=0.2):\n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary (number of unique tokens)\n",
    "            embedding_dim: Dimension of embedding vectors\n",
    "            hidden_dim: Dimension of hidden layers\n",
    "            num_classes: Number of output classes (1 for binary)\n",
    "            dropout: Dropout probability for regularization\n",
    "        \n",
    "        Educational Note:\n",
    "        - vocab_size determines embedding table size\n",
    "        - embedding_dim affects model capacity and memory\n",
    "        - hidden_dim controls network expressiveness\n",
    "        - dropout prevents overfitting\n",
    "        \"\"\"\n",
    "        super(SimpleTextClassifier, self).__init__()\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Embedding layer: converts token IDs to dense vectors\n",
    "        # Educational Note: This is like a lookup table where each word\n",
    "        # gets its own learnable vector representation\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        # Educational Note: Randomly sets some neurons to zero during training\n",
    "        # to prevent overfitting and improve generalization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Hidden layers\n",
    "        # Educational Note: These layers learn increasingly complex patterns\n",
    "        self.hidden1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        \n",
    "        # Output layer\n",
    "        # Educational Note: Final layer produces class predictions\n",
    "        self.output = nn.Linear(hidden_dim // 2, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize network weights for better training.\n",
    "        \n",
    "        Educational Note:\n",
    "        Proper weight initialization is crucial for training success.\n",
    "        Poor initialization can lead to vanishing/exploding gradients.\n",
    "        \"\"\"\n",
    "        # Initialize embedding with small random values\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        # Set padding token embedding to zero\n",
    "        nn.init.constant_(self.embedding.weight[0], 0)\n",
    "        \n",
    "        # Initialize linear layers with Xavier initialization\n",
    "        for module in [self.hidden1, self.hidden2, self.output]:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of token IDs [batch_size, sequence_length]\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor [batch_size, num_classes]\n",
    "        \n",
    "        Educational Note:\n",
    "        This method defines how data flows through the network.\n",
    "        Each step transforms the data to extract useful features.\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = x.shape\n",
    "        \n",
    "        # Step 1: Convert token IDs to embeddings\n",
    "        # Shape: [batch_size, seq_length] -> [batch_size, seq_length, embedding_dim]\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        # Step 2: Create mask for padding tokens\n",
    "        # Educational Note: We don't want padding tokens to affect our predictions\n",
    "        mask = (x != 0).float().unsqueeze(-1)  # [batch_size, seq_length, 1]\n",
    "        \n",
    "        # Step 3: Apply mask to embeddings\n",
    "        masked_embeddings = embeddings * mask\n",
    "        \n",
    "        # Step 4: Global average pooling (ignoring padding)\n",
    "        # Educational Note: This converts variable-length sequences to fixed-size vectors\n",
    "        sum_embeddings = masked_embeddings.sum(dim=1)  # [batch_size, embedding_dim]\n",
    "        seq_lengths = mask.sum(dim=1)  # [batch_size, 1]\n",
    "        avg_embeddings = sum_embeddings / (seq_lengths + 1e-8)  # Avoid division by zero\n",
    "        \n",
    "        # Step 5: Apply dropout\n",
    "        x = self.dropout(avg_embeddings)\n",
    "        \n",
    "        # Step 6: First hidden layer with ReLU activation\n",
    "        # Educational Note: ReLU introduces non-linearity, allowing complex patterns\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 7: Second hidden layer with ReLU activation\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Step 8: Output layer (no activation - will be applied in loss function)\n",
    "        # Educational Note: For binary classification with BCE loss, we don't apply\n",
    "        # sigmoid here - it's included in BCEWithLogitsLoss for numerical stability\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Get prediction probabilities.\n",
    "        \n",
    "        Educational Note:\n",
    "        This method applies sigmoid to get probabilities between 0 and 1.\n",
    "        Useful for inference and interpretation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(x)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            return probabilities\n",
    "\n",
    "# Create model\n",
    "model = SimpleTextClassifier(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_classes=1,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"Model created successfully!\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Get model summary\n",
    "param_info = count_parameters(model)\n",
    "print(f\"\\nModel has {param_info['total_parameters']:,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, tokenizer.vocab_size, (2, 20)).to(device)\n",
    "test_output = model(test_input)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Output values: {test_output.squeeze().detach().cpu().numpy()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}  {

   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop Implementation\n",
    "\n",
    "Now let's implement a comprehensive training loop with detailed explanations of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=20, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Train the neural network with detailed educational explanations.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrates complete training loop structure\n",
    "    - Shows forward pass, loss calculation, and backpropagation\n",
    "    - Includes validation and progress tracking\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Training involves iterative parameter updates\n",
    "    - Each epoch processes the entire dataset once\n",
    "    - Validation helps monitor overfitting\n",
    "    - Loss should generally decrease over time\n",
    "    \"\"\"\n",
    "    print(\"Starting Training Process\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    # Educational Note: BCEWithLogitsLoss combines sigmoid and BCE for numerical stability\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Educational Note: Adam optimizer adapts learning rates for each parameter\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler (optional)\n",
    "    # Educational Note: Reduces learning rate when loss plateaus\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', \n",
    "                                                    factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # Training history for visualization\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()  # Set model to training mode\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "            # Move data to device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Educational Step 1: Zero gradients\n",
    "            # Why: Gradients accumulate by default, we need to reset them\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Educational Step 2: Forward pass\n",
    "            # The model processes inputs and produces predictions\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()  # Remove extra dimension for binary classification\n",
    "            \n",
    "            # Educational Step 3: Calculate loss\n",
    "            # Loss measures how far predictions are from true labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Educational Step 4: Backward pass (backpropagation)\n",
    "            # Calculate gradients of loss with respect to parameters\n",
    "            loss.backward()\n",
    "            \n",
    "            # Educational Step 5: Update parameters\n",
    "            # Optimizer uses gradients to update model weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track statistics\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "            train_correct += (predictions == (labels > 0.5)).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "            \n",
    "            # Print progress for first epoch (educational)\n",
    "            if epoch == 0 and batch_idx < 3:\n",
    "                print(f\"  Batch {batch_idx+1}:\")\n",
    "                print(f\"    Input shape: {inputs.shape}\")\n",
    "                print(f\"    Output shape: {outputs.shape}\")\n",
    "                print(f\"    Loss: {loss.item():.4f}\")\n",
    "                print(f\"    Sample predictions: {torch.sigmoid(outputs)[:3].detach().cpu().numpy()}\")\n",
    "                print(f\"    Sample labels: {labels[:3].detach().cpu().numpy()}\")\n",
    "        \n",
    "        # Calculate average training metrics\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                predictions = torch.sigmoid(outputs) > 0.5\n",
    "                val_correct += (predictions == (labels > 0.5)).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total\n",
    "        \n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step(avg_val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['train_acc'].append(train_accuracy)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        history['learning_rates'].append(current_lr)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Educational insights\n",
    "        if epoch == 0:\n",
    "            print(\"\\nEducational Notes for First Epoch:\")\n",
    "            print(\"- Loss should be around 0.693 initially (random binary classification)\")\n",
    "            print(\"- Accuracy should start around 50% (random guessing)\")\n",
    "            print(\"- Watch for loss decreasing and accuracy increasing\")\n",
    "        \n",
    "        if epoch > 0:\n",
    "            loss_change = avg_train_loss - history['train_loss'][-2]\n",
    "            if loss_change > 0:\n",
    "                print(\"⚠️  Training loss increased - possible overfitting or high learning rate\")\n",
    "            elif abs(loss_change) < 0.001:\n",
    "                print(\"📊 Training loss plateauing - model may be converging\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training Complete!\")\n",
    "    print(f\"Final Train Accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Prepare data loaders\n",
    "# Split data into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "# Educational Note: DataLoader handles batching, shuffling, and parallel loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Data split:\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Batch size: 8\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Train the model\n",
    "training_history = train_model(model, train_loader, val_loader, num_epochs=15, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Progress Visualization\n",
    "\n",
    "Let's visualize the training progress to understand how our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training history with educational insights.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Visualize learning progress over time\n",
    "    - Identify overfitting, underfitting, and convergence\n",
    "    - Understand the relationship between loss and accuracy\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Loss should generally decrease over time\n",
    "    - Training and validation metrics should track closely\n",
    "    - Large gaps indicate overfitting\n",
    "    - Plateauing suggests convergence\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add educational annotations\n",
    "    min_val_loss_epoch = np.argmin(history['val_loss']) + 1\n",
    "    min_val_loss = min(history['val_loss'])\n",
    "    axes[0, 0].annotate(f'Best Val Loss: {min_val_loss:.4f}\\nEpoch: {min_val_loss_epoch}',\n",
    "                       xy=(min_val_loss_epoch, min_val_loss),\n",
    "                       xytext=(min_val_loss_epoch + 2, min_val_loss + 0.1),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    # Plot 2: Accuracy curves\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add educational annotations\n",
    "    max_val_acc_epoch = np.argmax(history['val_acc']) + 1\n",
    "    max_val_acc = max(history['val_acc'])\n",
    "    axes[0, 1].annotate(f'Best Val Acc: {max_val_acc:.4f}\\nEpoch: {max_val_acc_epoch}',\n",
    "                       xy=(max_val_acc_epoch, max_val_acc),\n",
    "                       xytext=(max_val_acc_epoch + 2, max_val_acc - 0.05),\n",
    "                       arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    # Plot 3: Learning rate schedule\n",
    "    axes[1, 0].plot(epochs, history['learning_rates'], 'g-', linewidth=2)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Loss vs Accuracy correlation\n",
    "    axes[1, 1].scatter(history['train_loss'], history['train_acc'], \n",
    "                      alpha=0.7, label='Training', s=50)\n",
    "    axes[1, 1].scatter(history['val_loss'], history['val_acc'], \n",
    "                      alpha=0.7, label='Validation', s=50)\n",
    "    axes[1, 1].set_title('Loss vs Accuracy Correlation')\n",
    "    axes[1, 1].set_xlabel('Loss')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational analysis\n",
    "    print(\"Educational Analysis of Training Progress:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Analyze overfitting\n",
    "    final_train_loss = history['train_loss'][-1]\n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    loss_gap = final_val_loss - final_train_loss\n",
    "    \n",
    "    print(f\"1. Overfitting Analysis:\")\n",
    "    print(f\"   Final training loss: {final_train_loss:.4f}\")\n",
    "    print(f\"   Final validation loss: {final_val_loss:.4f}\")\n",
    "    print(f\"   Loss gap: {loss_gap:.4f}\")\n",
    "    \n",
    "    if loss_gap > 0.1:\n",
    "        print(\"   ⚠️  Significant overfitting detected!\")\n",
    "        print(\"   Solutions: Increase dropout, reduce model complexity, get more data\")\n",
    "    elif loss_gap > 0.05:\n",
    "        print(\"   ⚠️  Mild overfitting detected\")\n",
    "        print(\"   Consider early stopping or regularization\")\n",
    "    else:\n",
    "        print(\"   ✅ Good generalization - minimal overfitting\")\n",
    "    \n",
    "    # Analyze convergence\n",
    "    print(f\"\\n2. Convergence Analysis:\")\n",
    "    if len(history['val_loss']) >= 5:\n",
    "        recent_losses = history['val_loss'][-5:]\n",
    "        loss_std = np.std(recent_losses)\n",
    "        print(f\"   Recent validation loss std: {loss_std:.4f}\")\n",
    "        \n",
    "        if loss_std < 0.01:\n",
    "            print(\"   ✅ Model has converged\")\n",
    "        elif loss_std < 0.05:\n",
    "            print(\"   📊 Model is converging\")\n",
    "        else:\n",
    "            print(\"   📈 Model still learning - could benefit from more epochs\")\n",
    "    \n",
    "    # Performance summary\n",
    "    print(f\"\\n3. Performance Summary:\")\n",
    "    print(f\"   Best validation accuracy: {max(history['val_acc']):.4f}\")\n",
    "    print(f\"   Best validation loss: {min(history['val_loss']):.4f}\")\n",
    "    print(f\"   Final validation accuracy: {history['val_acc'][-1]:.4f}\")\n",
    "    \n",
    "    if max(history['val_acc']) > 0.9:\n",
    "        print(\"   🎉 Excellent performance!\")\n",
    "    elif max(history['val_acc']) > 0.8:\n",
    "        print(\"   ✅ Good performance\")\n",
    "    elif max(history['val_acc']) > 0.7:\n",
    "        print(\"   📊 Decent performance - room for improvement\")\n",
    "    else:\n",
    "        print(\"   ⚠️  Poor performance - check data, model, or hyperparameters\")\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Testing\n",
    "\n",
    "Let's evaluate our trained model and understand its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, tokenizer):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation with educational insights.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrate proper model evaluation techniques\n",
    "    - Show how to interpret model predictions\n",
    "    - Analyze model strengths and weaknesses\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Evaluation should be done on unseen data\n",
    "    - Multiple metrics provide different insights\n",
    "    - Error analysis helps improve models\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    print(\"Evaluating Model Performance\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Get model predictions\n",
    "            outputs = model(inputs).squeeze()\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            # Store results\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            all_inputs.extend(inputs.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probabilities = np.array(all_probabilities)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Total samples: {len(all_labels)}\")\n",
    "    print(f\"Correct predictions: {int(accuracy * len(all_labels))}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions, \n",
    "                              target_names=['Statement', 'Question']))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Statement', 'Question'],\n",
    "                yticklabels=['Statement', 'Question'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational analysis of confusion matrix\n",
    "    print(\"\\nEducational Analysis of Confusion Matrix:\")\n",
    "    print(\"-\" * 45)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"True Negatives (Correct Statements): {tn}\")\n",
    "    print(f\"False Positives (Statements predicted as Questions): {fp}\")\n",
    "    print(f\"False Negatives (Questions predicted as Statements): {fn}\")\n",
    "    print(f\"True Positives (Correct Questions): {tp}\")\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAdditional Metrics:\")\n",
    "    print(f\"Precision (Question detection): {precision:.4f}\")\n",
    "    print(f\"Recall (Question detection): {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Analyze prediction confidence\n",
    "    print(f\"\\nPrediction Confidence Analysis:\")\n",
    "    print(f\"Average confidence: {np.mean(np.abs(all_probabilities - 0.5) + 0.5):.4f}\")\n",
    "    print(f\"Min confidence: {np.min(np.abs(all_probabilities - 0.5) + 0.5):.4f}\")\n",
    "    print(f\"Max confidence: {np.max(np.abs(all_probabilities - 0.5) + 0.5):.4f}\")\n",
    "    \n",
    "    # Show some example predictions\n",
    "    print(f\"\\nExample Predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(min(10, len(all_inputs))):\n",
    "        text = tokenizer.decode(all_inputs[i])\n",
    "        true_label = \"Question\" if all_labels[i] == 1 else \"Statement\"\n",
    "        pred_label = \"Question\" if all_predictions[i] == 1 else \"Statement\"\n",
    "        confidence = all_probabilities[i]\n",
    "        correct = \"✅\" if all_predictions[i] == all_labels[i] else \"❌\"\n",
    "        \n",
    "        print(f\"{correct} '{text}'\")\n",
    "        print(f\"   True: {true_label}, Predicted: {pred_label} (conf: {confidence:.3f})\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Evaluate on validation set\n",
    "evaluation_results = evaluate_model(model, val_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Model Testing\n",
    "\n",
    "Let's create an interactive function to test our model with custom inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_custom_input(model, tokenizer, text):\n",
    "    \"\"\"\n",
    "    Test the model with custom text input.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Demonstrate model inference process\n",
    "    - Show how to prepare input for trained model\n",
    "    - Interpret model outputs and confidence\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Input preprocessing must match training\n",
    "    - Model outputs logits, need sigmoid for probabilities\n",
    "    - Confidence indicates model certainty\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Prepare input\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    input_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor).squeeze()\n",
    "        probability = torch.sigmoid(output).item()\n",
    "        prediction = \"Question\" if probability > 0.5 else \"Statement\"\n",
    "        confidence = abs(probability - 0.5) + 0.5\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Input: '{text}'\")\n",
    "    print(f\"Tokenized: {token_ids}\")\n",
    "    print(f\"Decoded: '{tokenizer.decode(token_ids)}'\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Probability: {probability:.4f}\")\n",
    "    print(f\"Confidence: {confidence:.4f}\")\n",
    "    \n",
    "    # Educational interpretation\n",
    "    if confidence > 0.9:\n",
    "        print(\"🎯 Very confident prediction\")\n",
    "    elif confidence > 0.7:\n",
    "        print(\"✅ Confident prediction\")\n",
    "    elif confidence > 0.6:\n",
    "        print(\"📊 Moderately confident\")\n",
    "    else:\n",
    "        print(\"❓ Low confidence - model is uncertain\")\n",
    "    \n",
    "    return prediction, probability, confidence\n",
    "\n",
    "# Test with various examples\n",
    "test_examples = [\n",
    "    \"How are you doing today?\",\n",
    "    \"I am doing great today.\",\n",
    "    \"What is the weather like?\",\n",
    "    \"The weather is sunny.\",\n",
    "    \"Can you help me with this problem?\",\n",
    "    \"I will help you solve this.\",\n",
    "    \"Where do you live?\",\n",
    "    \"I live in California.\"\n",
    "]\n",
    "\n",
    "print(\"Testing Model with Custom Inputs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, text in enumerate(test_examples):\n",
    "    print(f\"\\nTest {i+1}:\")\n",
    "    print(\"-\" * 20)\n",
    "    test_custom_input(model, tokenizer, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Debugging Common Issues\n",
    "\n",
    "Let's explore common problems that can occur during neural network training and how to debug them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_common_issues():\n",
    "    \"\"\"\n",
    "    Educational demonstration of common neural network issues and solutions.\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Understanding common problems helps debug training\n",
    "    - Each issue has characteristic symptoms\n",
    "    - Solutions often involve hyperparameter adjustments\n",
    "    \"\"\"\n",
    "    print(\"Common Neural Network Issues and Solutions\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    issues = [\n",
    "        {\n",
    "            'name': '1. Vanishing Gradients',\n",
    "            'symptoms': [\n",
    "                'Loss stops decreasing early in training',\n",
    "                'Gradients become very small (< 1e-6)',\n",
    "                'Early layers learn very slowly'\n",
    "            ],\n",
    "            'causes': [\n",
    "                'Deep networks with sigmoid/tanh activations',\n",
    "                'Poor weight initialization',\n",
    "                'Very small learning rates'\n",
    "            ],\n",
    "            'solutions': [\n",
    "                'Use ReLU or other modern activations',\n",
    "                'Apply proper weight initialization (Xavier/He)',\n",
    "                'Use batch normalization',\n",
    "                'Reduce network depth or use residual connections'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'name': '2. Exploding Gradients',\n",
    "            'symptoms': [\n",
    "                'Loss becomes NaN or infinity',\n",
    "                'Gradients become very large (> 1e3)',\n",
    "                'Training becomes unstable'\n",
    "            ],\n",
    "            'causes': [\n",
    "                'Learning rate too high',\n",
    "                'Poor weight initialization',\n",
    "                'Deep networks without normalization'\n",
    "            ],\n",
    "            'solutions': [\n",
    "                'Reduce learning rate',\n",
    "                'Apply gradient clipping',\n",
    "                'Use proper weight initialization',\n",
    "                'Add batch normalization'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'name': '3. Overfitting',\n",
    "            'symptoms': [\n",
    "                'Training accuracy much higher than validation',\n",
    "                'Validation loss increases while training decreases',\n",
    "                'Large gap between train and validation metrics'\n",
    "            ],\n",
    "            'causes': [\n",
    "                'Model too complex for available data',\n",
    "                'Insufficient regularization',\n",
    "                'Training for too many epochs'\n",
    "            ],\n",
    "            'solutions': [\n",
    "                'Add dropout or other regularization',\n",
    "                'Reduce model complexity',\n",
    "                'Get more training data',\n",
    "                'Use early stopping',\n",
    "                'Apply data augmentation'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'name': '4. Underfitting',\n",
    "            'symptoms': [\n",
    "                'Both training and validation accuracy are low',\n",
    "                'Loss plateaus at a high value',\n",
    "                'Model performs poorly on simple examples'\n",
    "            ],\n",
    "            'causes': [\n",
    "                'Model too simple for the task',\n",
    "                'Learning rate too low',\n",
    "                'Insufficient training time',\n",
    "                'Poor feature representation'\n",
    "            ],\n",
    "            'solutions': [\n",
    "                'Increase model complexity',\n",
    "                'Increase learning rate',\n",
    "                'Train for more epochs',\n",
    "                'Improve feature engineering',\n",
    "                'Reduce regularization'\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'name': '5. Dead ReLU Problem',\n",
    "            'symptoms': [\n",
    "                'Many neurons always output zero',\n",
    "                'Gradients become zero for affected neurons',\n",
    "                'Model capacity effectively reduced'\n",
    "            ],\n",
    "            'causes': [\n",
    "                'Learning rate too high',\n",
    "                'Poor weight initialization',\n",
    "                'Large negative bias values'\n",
    "            ],\n",
    "            'solutions': [\n",
    "                'Use Leaky ReLU or ELU activations',\n",
    "                'Reduce learning rate',\n",
    "                'Apply proper weight initialization',\n",
    "                'Monitor neuron activation statistics'\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for issue in issues:\n",
    "        print(f\"\\n{issue['name']}\")\n",
    "        print(\"-\" * len(issue['name']))\n",
    "        \n",
    "        print(\"Symptoms:\")\n",
    "        for symptom in issue['symptoms']:\n",
    "            print(f\"  • {symptom}\")\n",
    "        \n",
    "        print(\"\\nCauses:\")\n",
    "        for cause in issue['causes']:\n",
    "            print(f\"  • {cause}\")\n",
    "        \n",
    "        print(\"\\nSolutions:\")\n",
    "        for solution in issue['solutions']:\n",
    "            print(f\"  • {solution}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Debugging Tips:\")\n",
    "    print(\"• Always plot training curves to visualize learning\")\n",
    "    print(\"• Monitor gradient norms during training\")\n",
    "    print(\"• Check activation statistics (mean, std, % zeros)\")\n",
    "    print(\"• Start with simple models and gradually increase complexity\")\n",
    "    print(\"• Use proper validation techniques to detect overfitting\")\n",
    "    print(\"• Keep detailed logs of hyperparameters and results\")\n",
    "\n",
    "# Demonstrate debugging concepts\n",
    "demonstrate_common_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Weight and Activation Visualization\n",
    "\n",
    "Let's visualize what our neural network has learned by examining weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_internals(model, tokenizer, sample_text=\"What is machine learning?\"):\n",
    "    \"\"\"\n",
    "    Visualize neural network weights and activations for educational purposes.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Show what the model has learned\n",
    "    - Demonstrate how information flows through layers\n",
    "    - Visualize learned representations\n",
    "    \n",
    "    Learning Notes:\n",
    "    - Weights represent learned patterns\n",
    "    - Activations show how input is transformed\n",
    "    - Visualization helps understand model behavior\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    print(\"Visualizing Model Internals\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. Embedding weights visualization\n",
    "    embedding_weights = model.embedding.weight.detach().cpu().numpy()\n",
    "    \n",
    "    # Show embedding for first 20 words\n",
    "    im1 = axes[0, 0].imshow(embedding_weights[:20, :], cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('Embedding Weights (First 20 Words)')\n",
    "    axes[0, 0].set_xlabel('Embedding Dimension')\n",
    "    axes[0, 0].set_ylabel('Word Index')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    # 2. Hidden layer 1 weights\n",
    "    hidden1_weights = model.hidden1.weight.detach().cpu().numpy()\n",
    "    im2 = axes[0, 1].imshow(hidden1_weights, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title('Hidden Layer 1 Weights')\n",
    "    axes[0, 1].set_xlabel('Input Features')\n",
    "    axes[0, 1].set_ylabel('Hidden Units')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # 3. Hidden layer 2 weights\n",
    "    hidden2_weights = model.hidden2.weight.detach().cpu().numpy()\n",
    "    im3 = axes[0, 2].imshow(hidden2_weights, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 2].set_title('Hidden Layer 2 Weights')\n",
    "    axes[0, 2].set_xlabel('Input Features')\n",
    "    axes[0, 2].set_ylabel('Hidden Units')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # 4. Weight distribution histogram\n",
    "    all_weights = np.concatenate([\n",
    "        embedding_weights.flatten(),\n",
    "        hidden1_weights.flatten(),\n",
    "        hidden2_weights.flatten(),\n",
    "        model.output.weight.detach().cpu().numpy().flatten()\n",
    "    ])\n",
    "    \n",
    "    axes[1, 0].hist(all_weights, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Weight Distribution')\n",
    "    axes[1, 0].set_xlabel('Weight Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # 5. Activation analysis with sample input\n",
    "    token_ids = tokenizer.encode(sample_text)\n",
    "    input_tensor = torch.tensor([token_ids], dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get intermediate activations\n",
    "    with torch.no_grad():\n",
    "        # Embedding layer\n",
    "        embeddings = model.embedding(input_tensor)\n",
    "        mask = (input_tensor != 0).float().unsqueeze(-1)\n",
    "        masked_embeddings = embeddings * mask\n",
    "        avg_embeddings = masked_embeddings.sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        hidden1_out = torch.relu(model.hidden1(avg_embeddings))\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        hidden2_out = torch.relu(model.hidden2(hidden1_out))\n",
    "    \n",
    "    # Plot activation values\n",
    "    activations = {\n",
    "        'Avg Embeddings': avg_embeddings.cpu().numpy().flatten(),\n",
    "        'Hidden 1': hidden1_out.cpu().numpy().flatten(),\n",
    "        'Hidden 2': hidden2_out.cpu().numpy().flatten()\n",
    "    }\n",
    "    \n",
    "    for i, (name, values) in enumerate(activations.items()):\n",
    "        if i == 0:\n",
    "            axes[1, 1].bar(range(len(values)), values, alpha=0.7, label=name)\n",
    "            axes[1, 1].set_title('Sample Activations')\n",
    "            axes[1, 1].set_xlabel('Neuron Index')\n",
    "            axes[1, 1].set_ylabel('Activation Value')\n",
    "            break\n",
    "    \n",
    "    # 6. Neuron activation statistics\n",
    "    layer_stats = []\n",
    "    layer_names = ['Hidden 1', 'Hidden 2']\n",
    "    layer_activations = [hidden1_out.cpu().numpy().flatten(), \n",
    "                        hidden2_out.cpu().numpy().flatten()]\n",
    "    \n",
    "    for name, acts in zip(layer_names, layer_activations):\n",
    "        stats = {\n",
    "            'mean': np.mean(acts),\n",
    "            'std': np.std(acts),\n",
    "            'zeros': np.sum(acts == 0) / len(acts) * 100\n",
    "        }\n",
    "        layer_stats.append(stats)\n",
    "    \n",
    "    # Plot activation statistics\n",
    "    metrics = ['mean', 'std', 'zeros']\n",
    "    x = np.arange(len(layer_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        values = [stats[metric] for stats in layer_stats]\n",
    "        axes[1, 2].bar(x + i*width, values, width, label=metric, alpha=0.7)\n",
    "    \n",
    "    axes[1, 2].set_title('Activation Statistics')\n",
    "    axes[1, 2].set_xlabel('Layer')\n",
    "    axes[1, 2].set_ylabel('Value')\n",
    "    axes[1, 2].set_xticks(x + width)\n",
    "    axes[1, 2].set_xticklabels(layer_names)\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Educational analysis\n",
    "    print(f\"\\nEducational Analysis of Model Internals:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(f\"1. Weight Analysis:\")\n",
    "    print(f\"   - Weight mean: {np.mean(all_weights):.4f}\")\n",
    "    print(f\"   - Weight std: {np.std(all_weights):.4f}\")\n",
    "    print(f\"   - Weight range: [{np.min(all_weights):.4f}, {np.max(all_weights):.4f}]\")\n",
    "    \n",
    "    if np.std(all_weights) < 0.01:\n",
    "        print(\"   ⚠️  Very small weight variance - possible vanishing gradients\")\n",
    "    elif np.std(all_weights) > 2.0:\n",
    "        print(\"   ⚠️  Very large weight variance - possible exploding gradients\")\n",
    "    else:\n",
    "        print(\"   ✅ Healthy weight distribution\")\n",
    "    \n",
    "    print(f\"\\n2. Activation Analysis for '{sample_text}':\")\n",
    "    for i, (name, stats) in enumerate(zip(layer_names, layer_stats)):\n",
    "        print(f\"   {name}:\")\n",
    "        print(f\"     - Mean activation: {stats['mean']:.4f}\")\n",
    "        print(f\"     - Std activation: {stats['std']:.4f}\")\n",
    "        print(f\"     - Dead neurons: {stats['zeros']:.1f}%\")\n",
    "        \n",
    "        if stats['zeros'] > 50:\n",
    "            print(f\"     ⚠️  High percentage of dead neurons\")\n",
    "        elif stats['zeros'] > 20:\n",
    "            print(f\"     📊 Moderate dead neuron percentage\")\n",
    "        else:\n",
    "            print(f\"     ✅ Healthy neuron activation\")\n",
    "    \n",
    "    print(f\"\\n3. Key Insights:\")\n",
    "    print(f\"   - Embeddings convert discrete tokens to continuous vectors\")\n",
    "    print(f\"   - Hidden layers learn increasingly abstract representations\")\n",
    "    print(f\"   - ReLU activations create sparsity (many zeros)\")\n",
    "    print(f\"   - Weight magnitudes indicate learning strength\")\n",
    "\n",
    "# Visualize model internals\n",
    "visualize_model_internals(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Next Steps\n",
    "\n",
    "Let's summarize what we've learned and outline the next steps in our chatbot journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_learning_summary():\n",
    "    \"\"\"\n",
    "    Comprehensive summary of neural networks concepts learned.\n",
    "    \n",
    "    Educational Purpose:\n",
    "    - Reinforce key concepts from the notebook\n",
    "    - Connect learning to broader ML context\n",
    "    - Prepare for advanced topics\n",
    "    \"\"\"\n",
    "    print(\"🎓 NEURAL NETWORKS BASICS - LEARNING SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    concepts = {\n",
    "        \"🧠 Neural Network Architecture\": [\n",
    "            \"Feedforward networks process information in layers\",\n",
    "            \"Embedding layers convert tokens to dense vectors\",\n",
    "            \"Hidden layers learn increasingly complex patterns\",\n",
    "            \"Output layers produce task-specific predictions\"\n",
    "        ],\n",
    "        \n",
    "        \"⚡ Activation Functions\": [\n",
    "            \"ReLU introduces non-linearity and prevents vanishing gradients\",\n",
    "            \"Sigmoid outputs probabilities for binary classification\",\n",
    "            \"Different activations suit different tasks and positions\",\n",
    "            \"Activation choice affects training dynamics\"\n",
    "        ],\n",
    "        \n",
    "        \"📊 Loss Functions\": [\n",
    "            \"Binary Cross-Entropy for binary classification tasks\",\n",
    "            \"Loss functions guide the learning process\",\n",
    "            \"Different losses emphasize different error types\",\n",
    "            \"Loss should generally decrease during training\"\n",
    "        ],\n",
    "        \n",
    "        \"🔄 Training Process\": [\n",
    "            \"Forward pass: data flows through network layers\",\n",
    "            \"Loss calculation: measure prediction errors\",\n",
    "            \"Backward pass: compute gradients via backpropagation\",\n",
    "            \"Parameter update: optimizer adjusts weights\"\n",
    "        ],\n",
    "        \n",
    "        \"📈 Optimization\": [\n",
    "            \"Adam optimizer adapts learning rates automatically\",\n",
    "            \"Learning rate scheduling improves convergence\",\n",
    "            \"Batch processing enables efficient training\",\n",
    "            \"Regularization (dropout) prevents overfitting\"\n",
    "        ],\n",
    "        \n",
    "        \"🔍 Evaluation & Debugging\": [\n",
    "            \"Training curves reveal learning progress\",\n",
    "            \"Validation metrics detect overfitting\",\n",
    "            \"Confusion matrices show classification performance\",\n",
    "            \"Weight/activation visualization aids understanding\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, points in concepts.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        print(\"-\" * (len(category) - 2))  # Subtract emoji length\n",
    "        for point in points:\n",
    "            print(f\"  • {point}\")\n",
    "    \n",
    "    print(f\"\\n🎯 KEY ACHIEVEMENTS\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"✅ Built a neural network from scratch\")\n",
    "    print(\"✅ Implemented complete training pipeline\")\n",
    "    print(\"✅ Achieved text classification with high accuracy\")\n",
    "    print(\"✅ Learned to debug common training issues\")\n",
    "    print(\"✅ Visualized model internals and behavior\")\n",
    "    \n",
    "    print(f\"\\n🚀 NEXT STEPS IN CHATBOT DEVELOPMENT\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"📚 Notebook 05: RNN and LSTM Fundamentals\")\n",
    "    print(\"   - Learn sequence modeling with recurrent networks\")\n",
    "    print(\"   - Understand memory and temporal dependencies\")\n",
    "    print(\"   - Handle variable-length sequences effectively\")\n",
    "    \n",
    "    print(\"\\n🎯 Notebook 06: Attention Mechanisms\")\n",
    "    print(\"   - Implement attention for better context understanding\")\n",
    "    print(\"   - Learn query-key-value attention patterns\")\n",
    "    print(\"   - Visualize attention weights and patterns\")\n",
    "    \n",
    "    print(\"\\n🤖 Notebook 07: Transformer Architecture\")\n",
    "    print(\"   - Build modern transformer-based models\")\n",
    "    print(\"   - Implement self-attention and positional encoding\")\n",
    "    print(\"   - Create state-of-the-art chatbot architecture\")\n",
    "    \n",
    "    print(f\"\\n💡 PRACTICAL TIPS FOR CONTINUED LEARNING\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"1. Experiment with different architectures and hyperparameters\")\n",
    "    print(\"2. Try the model on different text classification tasks\")\n",
    "    print(\"3. Implement additional regularization techniques\")\n",
    "    print(\"4. Explore different optimizers and learning rate schedules\")\n",
    "    print(\"5. Practice debugging with intentionally broken models\")\n",
    "    \n",
    "    print(f\"\\n🔬 RESEARCH DIRECTIONS\")\n",
    "    print(\"-\" * 20)\n",
    "    print(\"• Multi-class text classification\")\n",
    "    print(\"• Hierarchical text classification\")\n",
    "    print(\"• Few-shot learning for text tasks\")\n",
    "    print(\"• Transfer learning with pre-trained embeddings\")\n",
    "    print(\"• Adversarial training for robustness\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎉 Congratulations! You've mastered neural network basics!\")\n",
    "    print(\"Ready to tackle more advanced architectures for chatbots!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Print comprehensive learning summary\n",
    "print_learning_summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}