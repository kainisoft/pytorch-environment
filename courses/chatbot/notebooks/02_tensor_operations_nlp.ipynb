{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tensor Operations for NLP\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Understand how to represent text as tensors\n",
        "- Master text preprocessing operations using PyTorch\n",
        "- Learn about word embeddings and their tensor representations\n",
        "- Create and manipulate embedding spaces\n",
        "- Visualize text data transformations\n",
        "- Build foundational skills for chatbot development\n",
        "\n",
        "**Prerequisites:**\n",
        "- PyTorch fundamentals (Notebook 01)\n",
        "- Basic understanding of NLP concepts\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Welcome to the Tensor Operations for NLP notebook! This tutorial will guide you through the essential concepts of representing and manipulating text data using PyTorch tensors, focusing on practical NLP applications that will be crucial for building our chatbot.\n",
        "\n",
        "Text data is fundamentally different from numerical data - it's discrete, symbolic, and has complex relationships. To process text with neural networks, we need to convert it into numerical representations that tensors can handle effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup\n",
        "\n",
        "Let's start by importing the necessary libraries and setting up our environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter, defaultdict\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set up plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Representation Using Tensors\n",
        "\n",
        "Before we can process text with neural networks, we need to convert text into numerical representations that tensors can handle. Let's explore different approaches to text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Character-Level Representation\n",
        "\n",
        "The simplest approach is to represent text at the character level, where each character is mapped to a unique integer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Character-level representation\n",
        "print(\"=== Character-Level Text Representation ===\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello, world! How are you?\"\n",
        "print(f\"Original text: '{text}'\")\n",
        "print(f\"Text length: {len(text)} characters\")\n",
        "print()\n",
        "\n",
        "# Create character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(f\"Unique characters: {chars}\")\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print()\n",
        "\n",
        "# Create character-to-index and index-to-character mappings\n",
        "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
        "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "print(\"Character to index mapping:\")\n",
        "for char, idx in char_to_idx.items():\n",
        "    print(f\"  '{char}' -> {idx}\")\n",
        "print()\n",
        "\n",
        "# Convert text to tensor\n",
        "char_indices = [char_to_idx[ch] for ch in text]\n",
        "char_tensor = torch.tensor(char_indices, dtype=torch.long)\n",
        "\n",
        "print(f\"Character indices: {char_indices}\")\n",
        "print(f\"Character tensor: {char_tensor}\")\n",
        "print(f\"Tensor shape: {char_tensor.shape}\")\n",
        "print(f\"Tensor dtype: {char_tensor.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One-hot encoding for characters\n",
        "print(\"=== One-Hot Encoding for Characters ===\")\n",
        "\n",
        "# Create one-hot encoded representation\n",
        "def char_to_onehot(char_indices, vocab_size):\n",
        "    \"\"\"Convert character indices to one-hot encoding\"\"\"\n",
        "    one_hot = torch.zeros(len(char_indices), vocab_size)\n",
        "    for i, idx in enumerate(char_indices):\n",
        "        one_hot[i, idx] = 1\n",
        "    return one_hot\n",
        "\n",
        "# Alternative using PyTorch's built-in function\n",
        "one_hot_tensor = F.one_hot(char_tensor, num_classes=vocab_size).float()\n",
        "\n",
        "print(f\"One-hot tensor shape: {one_hot_tensor.shape}\")\n",
        "print(f\"First few characters as one-hot vectors:\")\n",
        "for i in range(min(5, len(text))):\n",
        "    char = text[i]\n",
        "    vector = one_hot_tensor[i]\n",
        "    print(f\"  '{char}' -> {vector.numpy()}\")\n",
        "\n",
        "# Visualize one-hot encoding\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.imshow(one_hot_tensor[:15].T, cmap='Blues', aspect='auto')\n",
        "plt.title('One-Hot Encoding Visualization (First 15 Characters)')\n",
        "plt.xlabel('Character Position')\n",
        "plt.ylabel('Character Index')\n",
        "plt.colorbar(label='Activation')\n",
        "\n",
        "# Add character labels\n",
        "char_labels = [text[i] for i in range(min(15, len(text)))]\n",
        "plt.xticks(range(len(char_labels)), char_labels)\n",
        "plt.yticks(range(vocab_size), chars)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Word-Level Representation\n",
        "\n",
        "For most NLP tasks, word-level representation is more practical and meaningful than character-level representation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word-level representation\n",
        "print(\"=== Word-Level Text Representation ===\")\n",
        "\n",
        "# Sample sentences for our chatbot\n",
        "sentences = [\n",
        "    \"Hello, how can I help you today?\",\n",
        "    \"What is the weather like?\",\n",
        "    \"Can you tell me a joke?\",\n",
        "    \"How are you doing?\",\n",
        "    \"What time is it?\",\n",
        "    \"Thank you for your help!\",\n",
        "    \"Goodbye, have a nice day!\"\n",
        "]\n",
        "\n",
        "print(\"Sample sentences:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"  {i+1}. {sentence}\")\n",
        "print()\n",
        "\n",
        "# Simple tokenization function\n",
        "def simple_tokenize(text):\n",
        "    \"\"\"Simple tokenization: lowercase and split by spaces, remove punctuation\"\"\"\n",
        "    # Convert to lowercase and remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
        "    return text.split()\n",
        "\n",
        "# Tokenize all sentences\n",
        "tokenized_sentences = [simple_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "print(\"Tokenized sentences:\")\n",
        "for i, tokens in enumerate(tokenized_sentences):\n",
        "    print(f\"  {i+1}. {tokens}\")\n",
        "print()\n",
        "\n",
        "# Build vocabulary\n",
        "all_words = []\n",
        "for tokens in tokenized_sentences:\n",
        "    all_words.extend(tokens)\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "vocab = ['<PAD>', '<UNK>'] + [word for word, count in word_counts.most_common()]\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {vocab}\")\n",
        "print(f\"Word frequencies: {dict(word_counts.most_common())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create word-to-index mappings\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "\n",
        "print(\"Word to index mapping:\")\n",
        "for word, idx in word_to_idx.items():\n",
        "    print(f\"  '{word}' -> {idx}\")\n",
        "print()\n",
        "\n",
        "# Convert sentences to tensor representation\n",
        "def sentence_to_indices(tokens, word_to_idx, max_length=None):\n",
        "    \"\"\"Convert tokenized sentence to indices with optional padding\"\"\"\n",
        "    indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "    \n",
        "    if max_length:\n",
        "        if len(indices) < max_length:\n",
        "            # Pad with <PAD> tokens\n",
        "            indices.extend([word_to_idx['<PAD>']] * (max_length - len(indices)))\n",
        "        else:\n",
        "            # Truncate if too long\n",
        "            indices = indices[:max_length]\n",
        "    \n",
        "    return indices\n",
        "\n",
        "# Convert all sentences to indices with padding\n",
        "max_length = max(len(tokens) for tokens in tokenized_sentences)\n",
        "print(f\"Maximum sentence length: {max_length} words\")\n",
        "\n",
        "sentence_indices = [sentence_to_indices(tokens, word_to_idx, max_length) \n",
        "                   for tokens in tokenized_sentences]\n",
        "\n",
        "# Create tensor\n",
        "sentences_tensor = torch.tensor(sentence_indices, dtype=torch.long)\n",
        "\n",
        "print(f\"\\nSentences tensor shape: {sentences_tensor.shape}\")\n",
        "print(f\"Sentences tensor:\")\n",
        "print(sentences_tensor)\n",
        "\n",
        "# Show the mapping back to words\n",
        "print(\"\\nTensor to words mapping:\")\n",
        "for i, indices in enumerate(sentence_indices[:3]):  # Show first 3 sentences\n",
        "    words = [idx_to_word[idx] for idx in indices]\n",
        "    print(f\"  Sentence {i+1}: {indices} -> {words}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Text Preprocessing Operations with Tensors\n",
        "\n",
        "Now let's explore common text preprocessing operations that we can perform using PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Text preprocessing operations\n",
        "print(\"=== Text Preprocessing with Tensors ===\")\n",
        "\n",
        "# Load sample conversation data\n",
        "def load_conversation_data():\n",
        "    \"\"\"Load conversation data from JSON file\"\"\"\n",
        "    try:\n",
        "        with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
        "            data = json.load(f)\n",
        "        return data['conversations']\n",
        "    except FileNotFoundError:\n",
        "        # Fallback data if file doesn't exist\n",
        "        return [\n",
        "            {\"input\": \"Hello\", \"response\": \"Hi there! How can I help you?\"},\n",
        "            {\"input\": \"How are you?\", \"response\": \"I'm doing well, thank you for asking!\"},\n",
        "            {\"input\": \"What's your name?\", \"response\": \"I'm a helpful AI assistant.\"},\n",
        "            {\"input\": \"Tell me a joke\", \"response\": \"Why don't scientists trust atoms? Because they make up everything!\"},\n",
        "            {\"input\": \"What time is it?\", \"response\": \"I don't have access to real-time information.\"},\n",
        "            {\"input\": \"Goodbye\", \"response\": \"Goodbye! Have a great day!\"}\n",
        "        ]\n",
        "\n",
        "conversations = load_conversation_data()\n",
        "print(f\"Loaded {len(conversations)} conversation pairs\")\n",
        "\n",
        "# Extract inputs and responses\n",
        "inputs = [conv['input'] for conv in conversations]\n",
        "responses = [conv['response'] for conv in conversations]\n",
        "\n",
        "print(\"\\nSample conversations:\")\n",
        "for i, (inp, resp) in enumerate(zip(inputs[:3], responses[:3])):\n",
        "    print(f\"  {i+1}. Input: '{inp}' -> Response: '{resp}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced tokenization and preprocessing\n",
        "class TextPreprocessor:\n",
        "    def __init__(self, vocab_size=1000, min_freq=1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.word_to_idx = {}\n",
        "        self.idx_to_word = {}\n",
        "        self.vocab = []\n",
        "        \n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Advanced tokenization with better handling\"\"\"\n",
        "        # Convert to lowercase\n",
        "        text = text.lower()\n",
        "        # Handle contractions\n",
        "        text = re.sub(r\"won't\", \"will not\", text)\n",
        "        text = re.sub(r\"can't\", \"cannot\", text)\n",
        "        text = re.sub(r\"n't\", \" not\", text)\n",
        "        text = re.sub(r\"'re\", \" are\", text)\n",
        "        text = re.sub(r\"'ve\", \" have\", text)\n",
        "        text = re.sub(r\"'ll\", \" will\", text)\n",
        "        text = re.sub(r\"'d\", \" would\", text)\n",
        "        text = re.sub(r\"'m\", \" am\", text)\n",
        "        # Remove punctuation and split\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        return text.split()\n",
        "    \n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from texts\"\"\"\n",
        "        # Tokenize all texts\n",
        "        all_tokens = []\n",
        "        for text in texts:\n",
        "            all_tokens.extend(self.tokenize(text))\n",
        "        \n",
        "        # Count word frequencies\n",
        "        word_counts = Counter(all_tokens)\n",
        "        \n",
        "        # Filter by minimum frequency and vocabulary size\n",
        "        filtered_words = [word for word, count in word_counts.items() \n",
        "                         if count >= self.min_freq]\n",
        "        \n",
        "        # Sort by frequency and take top vocab_size - 2 (for special tokens)\n",
        "        sorted_words = sorted(filtered_words, \n",
        "                            key=lambda x: word_counts[x], reverse=True)\n",
        "        \n",
        "        # Build vocabulary with special tokens\n",
        "        self.vocab = ['<PAD>', '<UNK>'] + sorted_words[:self.vocab_size-2]\n",
        "        \n",
        "        # Create mappings\n",
        "        self.word_to_idx = {word: i for i, word in enumerate(self.vocab)}\n",
        "        self.idx_to_word = {i: word for i, word in enumerate(self.vocab)}\n",
        "        \n",
        "        print(f\"Built vocabulary with {len(self.vocab)} words\")\n",
        "        print(f\"Most common words: {self.vocab[2:12]}\")\n",
        "        \n",
        "    def text_to_indices(self, text, max_length=None):\n",
        "        \"\"\"Convert text to indices\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        indices = [self.word_to_idx.get(token, self.word_to_idx['<UNK>']) \n",
        "                  for token in tokens]\n",
        "        \n",
        "        if max_length:\n",
        "            if len(indices) < max_length:\n",
        "                indices.extend([self.word_to_idx['<PAD>']] * (max_length - len(indices)))\n",
        "            else:\n",
        "                indices = indices[:max_length]\n",
        "        \n",
        "        return indices\n",
        "    \n",
        "    def indices_to_text(self, indices):\n",
        "        \"\"\"Convert indices back to text\"\"\"\n",
        "        words = [self.idx_to_word.get(idx, '<UNK>') for idx in indices]\n",
        "        # Remove padding tokens\n",
        "        words = [word for word in words if word != '<PAD>']\n",
        "        return ' '.join(words)\n",
        "\n",
        "# Initialize preprocessor\n",
        "preprocessor = TextPreprocessor(vocab_size=100, min_freq=1)\n",
        "\n",
        "# Build vocabulary from all texts\n",
        "all_texts = inputs + responses\n",
        "preprocessor.build_vocab(all_texts)\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(preprocessor.vocab)}\")\n",
        "print(f\"Sample vocabulary: {preprocessor.vocab[:20]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Word Embeddings: From Scratch and Pre-trained\n",
        "\n",
        "Word embeddings are dense vector representations of words that capture semantic relationships. Let's explore both creating embeddings from scratch and using pre-trained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Creating Word Embeddings from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating word embeddings from scratch\n",
        "print(\"=== Word Embeddings from Scratch ===\")\n",
        "\n",
        "# Embedding parameters\n",
        "vocab_size = len(preprocessor.vocab)\n",
        "embedding_dim = 50  # Dimension of embedding vectors\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Embedding dimension: {embedding_dim}\")\n",
        "\n",
        "# Create embedding layer\n",
        "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "print(f\"\\nEmbedding layer: {embedding_layer}\")\n",
        "print(f\"Embedding weight shape: {embedding_layer.weight.shape}\")\n",
        "print(f\"Number of parameters: {embedding_layer.weight.numel()}\")\n",
        "\n",
        "# Get embeddings for some words\n",
        "sample_words = ['hello', 'help', 'you', 'time', '<UNK>', '<PAD>']\n",
        "sample_indices = [preprocessor.word_to_idx.get(word, preprocessor.word_to_idx['<UNK>']) \n",
        "                 for word in sample_words]\n",
        "sample_tensor = torch.tensor(sample_indices, dtype=torch.long)\n",
        "\n",
        "# Get embeddings\n",
        "sample_embeddings = embedding_layer(sample_tensor)\n",
        "\n",
        "print(f\"\\nSample word embeddings:\")\n",
        "print(f\"Sample indices: {sample_indices}\")\n",
        "print(f\"Sample embeddings shape: {sample_embeddings.shape}\")\n",
        "\n",
        "for i, word in enumerate(sample_words):\n",
        "    embedding = sample_embeddings[i]\n",
        "    print(f\"  '{word}' (idx {sample_indices[i]}): {embedding[:5].detach().numpy()}... (first 5 dims)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate embedding properties\n",
        "print(\"=== Embedding Properties and Operations ===\")\n",
        "\n",
        "# Convert conversation data to tensors\n",
        "max_input_length = max(len(preprocessor.tokenize(text)) for text in inputs)\n",
        "max_response_length = max(len(preprocessor.tokenize(text)) for text in responses)\n",
        "max_length = max(max_input_length, max_response_length)\n",
        "\n",
        "print(f\"Max input length: {max_input_length}\")\n",
        "print(f\"Max response length: {max_response_length}\")\n",
        "print(f\"Using max length: {max_length}\")\n",
        "\n",
        "# Convert to indices\n",
        "input_indices = [preprocessor.text_to_indices(text, max_length) for text in inputs]\n",
        "response_indices = [preprocessor.text_to_indices(text, max_length) for text in responses]\n",
        "\n",
        "# Create tensors\n",
        "input_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
        "response_tensor = torch.tensor(response_indices, dtype=torch.long)\n",
        "\n",
        "# Get embeddings for our conversation data\n",
        "input_embeddings = embedding_layer(input_tensor)\n",
        "response_embeddings = embedding_layer(response_tensor)\n",
        "\n",
        "print(f\"\\nInput embeddings shape: {input_embeddings.shape}\")\n",
        "print(f\"Response embeddings shape: {response_embeddings.shape}\")\n",
        "\n",
        "# Calculate similarity between words using cosine similarity\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
        "    return F.cosine_similarity(a.unsqueeze(0), b.unsqueeze(0)).item()\n",
        "\n",
        "# Compare some word embeddings\n",
        "word_pairs = [('hello', 'hi'), ('help', 'you'), ('what', 'how'), ('time', 'day')]\n",
        "\n",
        "print(\"\\nWord similarity (random embeddings):\")\n",
        "for word1, word2 in word_pairs:\n",
        "    if word1 in preprocessor.word_to_idx and word2 in preprocessor.word_to_idx:\n",
        "        idx1 = preprocessor.word_to_idx[word1]\n",
        "        idx2 = preprocessor.word_to_idx[word2]\n",
        "        \n",
        "        emb1 = embedding_layer(torch.tensor([idx1]))[0]\n",
        "        emb2 = embedding_layer(torch.tensor([idx2]))[0]\n",
        "        \n",
        "        similarity = cosine_similarity(emb1, emb2)\n",
        "        print(f\"  '{word1}' vs '{word2}': {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"  '{word1}' vs '{word2}': One or both words not in vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Training Word Embeddings with Skip-gram\n",
        "\n",
        "Let's implement a simple Skip-gram model to train meaningful word embeddings on our text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Skip-gram implementation\n",
        "print(\"=== Training Word Embeddings with Skip-gram ===\")\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "        \n",
        "    def forward(self, center_word):\n",
        "        embeds = self.embeddings(center_word)\n",
        "        out = self.linear(embeds)\n",
        "        return out\n",
        "\n",
        "# Create training data for skip-gram\n",
        "def create_skipgram_data(texts, preprocessor, window_size=2):\n",
        "    \"\"\"Create (center_word, context_word) pairs for skip-gram training\"\"\"\n",
        "    data = []\n",
        "    \n",
        "    for text in texts:\n",
        "        tokens = preprocessor.tokenize(text)\n",
        "        indices = [preprocessor.word_to_idx.get(token, preprocessor.word_to_idx['<UNK>']) \n",
        "                  for token in tokens]\n",
        "        \n",
        "        for i, center_idx in enumerate(indices):\n",
        "            # Get context words within window\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(indices), i + window_size + 1)\n",
        "            \n",
        "            for j in range(start, end):\n",
        "                if i != j:  # Skip the center word itself\n",
        "                    context_idx = indices[j]\n",
        "                    data.append((center_idx, context_idx))\n",
        "    \n",
        "    return data\n",
        "\n",
        "# Create training data\n",
        "skipgram_data = create_skipgram_data(all_texts, preprocessor, window_size=2)\n",
        "print(f\"Created {len(skipgram_data)} training pairs\")\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nSample training pairs:\")\n",
        "for i in range(min(10, len(skipgram_data))):\n",
        "    center_idx, context_idx = skipgram_data[i]\n",
        "    center_word = preprocessor.idx_to_word[center_idx]\n",
        "    context_word = preprocessor.idx_to_word[context_idx]\n",
        "    print(f\"  ({center_word}, {context_word})\")\n",
        "\n",
        "# Initialize model\n",
        "embedding_dim = 32  # Smaller for faster training\n",
        "model = SkipGram(vocab_size, embedding_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the skip-gram model\n",
        "print(\"=== Training Skip-gram Model ===\")\n",
        "\n",
        "# Convert data to tensors\n",
        "center_words = torch.tensor([pair[0] for pair in skipgram_data], dtype=torch.long)\n",
        "context_words = torch.tensor([pair[1] for pair in skipgram_data], dtype=torch.long)\n",
        "\n",
        "# Training parameters\n",
        "n_epochs = 50\n",
        "batch_size = 16\n",
        "n_batches = len(skipgram_data) // batch_size\n",
        "\n",
        "print(f\"Training for {n_epochs} epochs with batch size {batch_size}\")\n",
        "print(f\"Number of batches per epoch: {n_batches}\")\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "for epoch in range(n_epochs):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    # Shuffle data\n",
        "    perm = torch.randperm(len(skipgram_data))\n",
        "    center_words_shuffled = center_words[perm]\n",
        "    context_words_shuffled = context_words[perm]\n",
        "    \n",
        "    for batch_idx in range(n_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        \n",
        "        batch_center = center_words_shuffled[start_idx:end_idx]\n",
        "        batch_context = context_words_shuffled[start_idx:end_idx]\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(batch_center)\n",
        "        loss = criterion(outputs, batch_context)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / n_batches\n",
        "    losses.append(avg_loss)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1:3d}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "print(\"\\nTraining completed!\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses)\n",
        "plt.title('Skip-gram Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Embedding Space Visualization\n",
        "\n",
        "Let's visualize our trained embeddings to understand the relationships between words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize embedding space\n",
        "print(\"=== Embedding Space Visualization ===\")\n",
        "\n",
        "# Get trained embeddings\n",
        "trained_embeddings = model.embeddings.weight.detach().numpy()\n",
        "\n",
        "print(f\"Trained embeddings shape: {trained_embeddings.shape}\")\n",
        "\n",
        "# Use PCA to reduce dimensionality for visualization\n",
        "pca = PCA(n_components=2)\n",
        "embeddings_2d = pca.fit_transform(trained_embeddings)\n",
        "\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.3f}\")\n",
        "\n",
        "# Plot embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
        "\n",
        "# Add labels for some words\n",
        "words_to_show = ['hello', 'help', 'you', 'what', 'how', 'time', 'day', 'thank', 'goodbye']\n",
        "for word in words_to_show:\n",
        "    if word in preprocessor.word_to_idx:\n",
        "        idx = preprocessor.word_to_idx[word]\n",
        "        x, y = embeddings_2d[idx]\n",
        "        plt.annotate(word, (x, y), xytext=(5, 5), textcoords='offset points')\n",
        "\n",
        "plt.title('Word Embeddings Visualization (PCA)')\n",
        "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f} variance)')\n",
        "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f} variance)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Compare word similarities with trained embeddings\n",
        "def get_word_similarity(word1, word2, embeddings, preprocessor):\n",
        "    \"\"\"Calculate similarity between two words using trained embeddings\"\"\"\n",
        "    idx1 = preprocessor.word_to_idx.get(word1, preprocessor.word_to_idx['<UNK>'])\n",
        "    idx2 = preprocessor.word_to_idx.get(word2, preprocessor.word_to_idx['<UNK>'])\n",
        "    \n",
        "    emb1 = torch.tensor(embeddings[idx1])\n",
        "    emb2 = torch.tensor(embeddings[idx2])\n",
        "    \n",
        "    return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "# Test word similarities\n",
        "test_pairs = [('hello', 'hi'), ('help', 'you'), ('what', 'how'), ('time', 'day'), ('thank', 'thanks')]\n",
        "\n",
        "print(\"\\nWord similarities (trained embeddings):\")\n",
        "for word1, word2 in test_pairs:\n",
        "    if word1 in preprocessor.word_to_idx and word2 in preprocessor.word_to_idx:\n",
        "        similarity = get_word_similarity(word1, word2, trained_embeddings, preprocessor)\n",
        "        print(f\"  '{word1}' vs '{word2}': {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"  '{word1}' vs '{word2}': One or both words not in vocabulary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Tensor Operations for Text Processing\n",
        "\n",
        "Let's explore advanced tensor operations that are commonly used in NLP tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced tensor operations for NLP\n",
        "print(\"=== Advanced Tensor Operations for NLP ===\")\n",
        "\n",
        "# Create sample batch of sequences\n",
        "batch_size = 4\n",
        "seq_length = 8\n",
        "vocab_size = len(preprocessor.vocab)\n",
        "\n",
        "# Random sequences for demonstration\n",
        "sequences = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
        "print(f\"Sample sequences shape: {sequences.shape}\")\n",
        "print(f\"Sample sequences:\")\n",
        "print(sequences)\n",
        "\n",
        "# 1. Masking operations\n",
        "print(\"\\n1. Masking Operations:\")\n",
        "\n",
        "# Create padding mask (assuming 0 is padding token)\n",
        "padding_mask = (sequences != 0).float()\n",
        "print(f\"Padding mask shape: {padding_mask.shape}\")\n",
        "print(f\"Padding mask:\")\n",
        "print(padding_mask)\n",
        "\n",
        "# Calculate sequence lengths\n",
        "seq_lengths = padding_mask.sum(dim=1)\n",
        "print(f\"Sequence lengths: {seq_lengths}\")\n",
        "\n",
        "# 2. Embedding lookup\n",
        "print(\"\\n2. Embedding Lookup:\")\n",
        "embedding_dim = 16\n",
        "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "embedded_sequences = embedding(sequences)\n",
        "print(f\"Embedded sequences shape: {embedded_sequences.shape}\")\n",
        "print(f\"First sequence embedding (first 3 tokens, first 5 dims):\")\n",
        "print(embedded_sequences[0, :3, :5])\n",
        "\n",
        "# 3. Masked operations\n",
        "print(\"\\n3. Masked Operations:\")\n",
        "\n",
        "# Apply mask to embeddings\n",
        "mask_expanded = padding_mask.unsqueeze(-1).expand_as(embedded_sequences)\n",
        "masked_embeddings = embedded_sequences * mask_expanded\n",
        "\n",
        "print(f\"Masked embeddings shape: {masked_embeddings.shape}\")\n",
        "\n",
        "# Calculate mean embeddings (ignoring padding)\n",
        "sum_embeddings = masked_embeddings.sum(dim=1)  # Sum over sequence length\n",
        "mean_embeddings = sum_embeddings / seq_lengths.unsqueeze(-1)\n",
        "\n",
        "print(f\"Mean embeddings shape: {mean_embeddings.shape}\")\n",
        "print(f\"Mean embeddings (first 5 dims):\")\n",
        "print(mean_embeddings[:, :5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Attention-like operations\n",
        "print(\"4. Attention-like Operations:\")\n",
        "\n",
        "# Simple attention mechanism\n",
        "def simple_attention(embeddings, mask):\n",
        "    \"\"\"Simple attention mechanism\"\"\"\n",
        "    # Calculate attention scores (simplified)\n",
        "    attention_scores = torch.sum(embeddings, dim=-1)  # [batch_size, seq_length]\n",
        "    \n",
        "    # Apply mask to attention scores\n",
        "    attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
        "    \n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "    \n",
        "    # Apply attention weights to embeddings\n",
        "    attended_embeddings = torch.sum(embeddings * attention_weights.unsqueeze(-1), dim=1)\n",
        "    \n",
        "    return attended_embeddings, attention_weights\n",
        "\n",
        "attended_emb, attention_weights = simple_attention(embedded_sequences, padding_mask)\n",
        "\n",
        "print(f\"Attended embeddings shape: {attended_emb.shape}\")\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"Attention weights (first sequence): {attention_weights[0]}\")\n",
        "\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(attention_weights.detach().numpy(), cmap='Blues', aspect='auto')\n",
        "plt.title('Attention Weights Visualization')\n",
        "plt.xlabel('Sequence Position')\n",
        "plt.ylabel('Batch Index')\n",
        "plt.colorbar(label='Attention Weight')\n",
        "plt.show()\n",
        "\n",
        "# 5. Sequence similarity operations\n",
        "print(\"\\n5. Sequence Similarity Operations:\")\n",
        "\n",
        "# Calculate pairwise similarities between sequences\n",
        "def sequence_similarity_matrix(embeddings):\n",
        "    \"\"\"Calculate pairwise cosine similarities between sequences\"\"\"\n",
        "    # Normalize embeddings\n",
        "    normalized_emb = F.normalize(embeddings, p=2, dim=-1)\n",
        "    \n",
        "    # Calculate similarity matrix\n",
        "    similarity_matrix = torch.mm(normalized_emb, normalized_emb.t())\n",
        "    \n",
        "    return similarity_matrix\n",
        "\n",
        "similarity_matrix = sequence_similarity_matrix(mean_embeddings)\n",
        "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
        "print(f\"Similarity matrix:\")\n",
        "print(similarity_matrix)\n",
        "\n",
        "# Visualize similarity matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(similarity_matrix.detach().numpy(), cmap='RdYlBu', vmin=-1, vmax=1)\n",
        "plt.title('Sequence Similarity Matrix')\n",
        "plt.xlabel('Sequence Index')\n",
        "plt.ylabel('Sequence Index')\n",
        "plt.colorbar(label='Cosine Similarity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Next Steps\n",
        "\n",
        "Let's summarize what we've learned about tensor operations for NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary of key concepts\n",
        "print(\"=== Summary: Tensor Operations for NLP ===\")\n",
        "print()\n",
        "\n",
        "concepts = {\n",
        "    \"Text Representation\": [\n",
        "        \"Character-level: Simple but limited semantic understanding\",\n",
        "        \"Word-level: More meaningful for most NLP tasks\",\n",
        "        \"Vocabulary building: Map words to unique indices\",\n",
        "        \"Padding: Handle variable-length sequences\"\n",
        "    ],\n",
        "    \n",
        "    \"Text Preprocessing\": [\n",
        "        \"Tokenization: Split text into meaningful units\",\n",
        "        \"Normalization: Lowercase, handle contractions\",\n",
        "        \"Vocabulary filtering: Remove rare words, limit size\",\n",
        "        \"Special tokens: <PAD>, <UNK> for handling edge cases\"\n",
        "    ],\n",
        "    \n",
        "    \"Word Embeddings\": [\n",
        "        \"Dense vector representations of words\",\n",
        "        \"Capture semantic relationships between words\",\n",
        "        \"Can be learned from scratch or pre-trained\",\n",
        "        \"Skip-gram: Predict context from center word\"\n",
        "    ],\n",
        "    \n",
        "    \"Tensor Operations\": [\n",
        "        \"Masking: Handle variable-length sequences\",\n",
        "        \"Embedding lookup: Convert indices to vectors\",\n",
        "        \"Aggregation: Mean, sum, attention-weighted\",\n",
        "        \"Similarity: Cosine similarity for semantic comparison\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for category, points in concepts.items():\n",
        "    print(f\"{category}:\")\n",
        "    for point in points:\n",
        "        print(f\"  • {point}\")\n",
        "    print()\n",
        "\n",
        "print(\"Key Takeaways:\")\n",
        "print(\"• Text must be converted to numerical form for neural networks\")\n",
        "print(\"• Word embeddings capture semantic meaning in dense vectors\")\n",
        "print(\"• Masking is crucial for handling variable-length sequences\")\n",
        "print(\"• Tensor operations enable efficient batch processing\")\n",
        "print(\"• Similarity measures help understand semantic relationships\")\n",
        "\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"• Data Preprocessing and Tokenization (Notebook 03)\")\n",
        "print(\"• Neural Networks Basics (Notebook 04)\")\n",
        "print(\"• Sequence Models for Chatbots (Notebook 05+)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Ready to build more sophisticated NLP models!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}