{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apple MPS GPU Acceleration Demo\n",
    "\n",
    "This notebook demonstrates how to use Apple's Metal Performance Shaders (MPS) for GPU acceleration on Apple Silicon Macs (M1/M2/M3).\n",
    "\n",
    "MPS provides GPU acceleration for PyTorch operations on Apple Silicon, offering significant speedups for machine learning workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n",
    "print(f\"MPS built: {torch.backends.mps.is_built()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Selection\n",
    "\n",
    "The recommended way to select a device for Apple Silicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device selection for Apple Silicon\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS (Metal Performance Shaders) for GPU acceleration\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using NVIDIA CUDA for GPU acceleration\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (no GPU acceleration available)\")\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors on the selected device\n",
    "x = torch.randn(1000, 1000, device=device)\n",
    "y = torch.randn(1000, 1000, device=device)\n",
    "\n",
    "print(f\"Tensor x device: {x.device}\")\n",
    "print(f\"Tensor y device: {y.device}\")\n",
    "\n",
    "# Perform matrix multiplication\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"Result tensor device: {z.device}\")\n",
    "print(f\"Result tensor shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: CPU vs MPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(device, size=2000, iterations=10):\n",
    "    \"\"\"Benchmark matrix multiplication on specified device.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # Create random tensors\n",
    "        x = torch.randn(size, size, device=device)\n",
    "        y = torch.randn(size, size, device=device)\n",
    "        \n",
    "        # Time the operation\n",
    "        start_time = time.time()\n",
    "        z = torch.matmul(x, y)\n",
    "        \n",
    "        # Synchronize if using MPS\n",
    "        if device.type == 'mps':\n",
    "            torch.mps.synchronize()\n",
    "        elif device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    \n",
    "    return times\n",
    "\n",
    "# Benchmark CPU\n",
    "print(\"Benchmarking CPU...\")\n",
    "cpu_times = benchmark_matmul(torch.device('cpu'), iterations=5)\n",
    "avg_cpu_time = np.mean(cpu_times)\n",
    "print(f\"Average CPU time: {avg_cpu_time:.4f} seconds\")\n",
    "\n",
    "# Benchmark MPS (if available)\n",
    "if torch.backends.mps.is_available():\n",
    "    print(\"Benchmarking MPS...\")\n",
    "    mps_times = benchmark_matmul(torch.device('mps'), iterations=5)\n",
    "    avg_mps_time = np.mean(mps_times)\n",
    "    print(f\"Average MPS time: {avg_mps_time:.4f} seconds\")\n",
    "    \n",
    "    speedup = avg_cpu_time / avg_mps_time\n",
    "    print(f\"MPS speedup: {speedup:.2f}x\")\n",
    "else:\n",
    "    print(\"MPS not available for benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple neural network for demonstration\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=512, num_classes=10):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and move to device\n",
    "model = SimpleNet().to(device)\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Create dummy data\n",
    "batch_size = 128\n",
    "input_data = torch.randn(batch_size, 784, device=device)\n",
    "target = torch.randint(0, 10, (batch_size,), device=device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Training for a few steps...\")\n",
    "for step in range(10):\n",
    "    # Forward pass\n",
    "    outputs = model(input_data)\n",
    "    loss = criterion(outputs, target)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % 2 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for MPS\n",
    "\n",
    "1. **Always check availability**: Use `torch.backends.mps.is_available()` before using MPS\n",
    "2. **Move tensors and models**: Ensure both your data and model are on the same device\n",
    "3. **Synchronization**: Use `torch.mps.synchronize()` when timing operations\n",
    "4. **Memory management**: MPS shares memory with the system, so be mindful of memory usage\n",
    "5. **Fallback to CPU**: Always have a CPU fallback for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of proper device handling\n",
    "def get_device():\n",
    "    \"\"\"Get the best available device.\"\"\"\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    elif torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "def move_to_device(tensor_or_model, device):\n",
    "    \"\"\"Safely move tensor or model to device.\"\"\"\n",
    "    try:\n",
    "        return tensor_or_model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to move to {device}, falling back to CPU: {e}\")\n",
    "        return tensor_or_model.to(torch.device(\"cpu\"))\n",
    "\n",
    "# Example usage\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create and move tensor\n",
    "tensor = torch.randn(100, 100)\n",
    "tensor = move_to_device(tensor, device)\n",
    "print(f\"Tensor is on: {tensor.device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
