{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Text Preprocessing for Chatbots\n",
    "\n",
    "**Duration:** 2-3 hours | **Difficulty:** Beginner-Intermediate\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Master text cleaning and normalization techniques\n",
    "- Understand tokenization strategies (word, character, subword)\n",
    "- Build vocabulary and implement encoding/decoding\n",
    "- Create a complete text preprocessing pipeline\n",
    "\n",
    "## ðŸ“š Contents\n",
    "1. Text Cleaning and Normalization\n",
    "2. Tokenization Strategies\n",
    "3. Vocabulary Building\n",
    "4. Encoding and Padding\n",
    "5. Complete Pipeline Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import unicodedata\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import our custom utilities\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from text_utils import SimpleTokenizer, clean_text, pad_sequences\n",
    "\n",
    "print(\"Text preprocessing setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning and Normalization\n",
    "\n",
    "Before we can train chatbots, we need to clean and normalize text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample conversation data\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    conversation_data = json.load(f)\n",
    "\n",
    "# Extract texts for preprocessing\n",
    "texts = []\n",
    "for item in conversation_data:\n",
    "    texts.append(item['query'])\n",
    "    texts.append(item['response'])\n",
    "\n",
    "print(f\"Loaded {len(conversation_data)} conversation pairs\")\n",
    "print(f\"Total texts: {len(texts)}\")\n",
    "print(\"\\nFirst few examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"Q: {conversation_data[i]['query']}\")\n",
    "    print(f\"A: {conversation_data[i]['response']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning examples\n",
    "sample_texts = [\n",
    "    \"Hello!!!   How are you? ðŸ˜Š\",\n",
    "    \"What's YOUR name???\",\n",
    "    \"I'm   learning   PyTorch...   \",\n",
    "    \"Can you help with ML/AI topics?\"\n",
    "]\n",
    "\n",
    "print(\"Text Cleaning Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in sample_texts:\n",
    "    print(f\"Original: {repr(text)}\")\n",
    "    \n",
    "    # Basic cleaning\n",
    "    basic_clean = clean_text(text, lowercase=True, remove_extra_whitespace=True)\n",
    "    print(f\"Basic:    {repr(basic_clean)}\")\n",
    "    \n",
    "    # Remove punctuation\n",
    "    no_punct = clean_text(text, lowercase=True, remove_punctuation=True)\n",
    "    print(f\"No punct: {repr(no_punct)}\")\n",
    "    \n",
    "    # Remove special characters\n",
    "    no_special = clean_text(text, lowercase=True, remove_special_chars=True)\n",
    "    print(f\"No spec:  {repr(no_special)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced text normalization\n",
    "def advanced_text_cleaning(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Advanced text cleaning for chatbot preprocessing.\n",
    "    \"\"\"\n",
    "    # Handle contractions\n",
    "    contractions = {\n",
    "        \"can't\": \"cannot\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'m\": \" am\"\n",
    "    }\n",
    "    \n",
    "    # Apply contractions\n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = clean_text(text, lowercase=True, remove_extra_whitespace=True)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Handle repeated punctuation\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    text = re.sub(r'[.]{2,}', '.', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test advanced cleaning\n",
    "test_cases = [\n",
    "    \"I can't believe it's working!!!\",\n",
    "    \"What's your name??? I'm curious...\",\n",
    "    \"Check out this link: https://pytorch.org\",\n",
    "    \"Won't you help me??? Please!!!\"\n",
    "]\n",
    "\n",
    "print(\"Advanced Text Cleaning:\")\n",
    "for text in test_cases:\n",
    "    cleaned = advanced_text_cleaning(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Cleaned:  {cleaned}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization Strategies\n",
    "\n",
    "Tokenization breaks text into smaller units (tokens) that models can process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level tokenization\n",
    "def word_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple word tokenization.\"\"\"\n",
    "    # Clean text first\n",
    "    text = advanced_text_cleaning(text)\n",
    "    # Split on whitespace and punctuation\n",
    "    tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "    return tokens\n",
    "\n",
    "# Character-level tokenization\n",
    "def char_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Character-level tokenization.\"\"\"\n",
    "    text = advanced_text_cleaning(text)\n",
    "    return list(text)\n",
    "\n",
    "# Subword tokenization (simplified)\n",
    "def simple_subword_tokenize(text: str, max_word_length: int = 6) -> List[str]:\n",
    "    \"\"\"Simplified subword tokenization.\"\"\"\n",
    "    words = word_tokenize(text)\n",
    "    tokens = []\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) <= max_word_length:\n",
    "            tokens.append(word)\n",
    "        else:\n",
    "            # Split long words into chunks\n",
    "            for i in range(0, len(word), max_word_length):\n",
    "                chunk = word[i:i + max_word_length]\n",
    "                if i == 0:\n",
    "                    tokens.append(chunk)\n",
    "                else:\n",
    "                    tokens.append('##' + chunk)  # Continuation marker\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Compare tokenization strategies\n",
    "sample_text = \"Hello! What's machine learning and how does tokenization work?\"\n",
    "\n",
    "word_tokens = word_tokenize(sample_text)\n",
    "char_tokens = char_tokenize(sample_text)\n",
    "subword_tokens = simple_subword_tokenize(sample_text)\n",
    "\n",
    "print(f\"Original text: {sample_text}\")\n",
    "print(f\"\\nWord tokens ({len(word_tokens)}): {word_tokens}\")\n",
    "print(f\"\\nCharacter tokens ({len(char_tokens)}): {char_tokens[:20]}...\")\n",
    "print(f\"\\nSubword tokens ({len(subword_tokens)}): {subword_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Building\n",
    "\n",
    "Create a vocabulary mapping between tokens and numerical IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from our conversation data\n",
    "def build_vocab_from_texts(texts: List[str], vocab_size: int = 1000, min_freq: int = 2) -> SimpleTokenizer:\n",
    "    \"\"\"\n",
    "    Build vocabulary from a list of texts.\n",
    "    \"\"\"\n",
    "    # Clean texts\n",
    "    cleaned_texts = [advanced_text_cleaning(text) for text in texts]\n",
    "    \n",
    "    # Initialize tokenizer\n",
    "    tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "    \n",
    "    # Build vocabulary\n",
    "    tokenizer.build_vocabulary(cleaned_texts, min_freq=min_freq)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "# Build vocabulary from our conversation data\n",
    "tokenizer = build_vocab_from_texts(texts, vocab_size=500, min_freq=1)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Special tokens: {tokenizer.special_tokens}\")\n",
    "\n",
    "# Show some vocabulary examples\n",
    "print(\"\\nSample vocabulary (first 20 tokens):\")\n",
    "for i in range(min(20, len(tokenizer.id_to_token))):\n",
    "    token = tokenizer.id_to_token[i]\n",
    "    print(f\"ID {i:2d}: '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tokenization and encoding\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you help me with programming?\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization and Encoding Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    \n",
    "    # Encode to IDs\n",
    "    token_ids = tokenizer.encode(sentence, add_special_tokens=True, max_length=15)\n",
    "    \n",
    "    # Decode back to text\n",
    "    decoded = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Tokens:   {tokens}\")\n",
    "    print(f\"IDs:      {token_ids}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding and Padding\n",
    "\n",
    "Convert text to tensors and handle variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing example\n",
    "batch_sentences = [\n",
    "    \"Hi there!\",\n",
    "    \"How are you doing today?\",\n",
    "    \"What can you help me with?\",\n",
    "    \"Thanks!\",\n",
    "    \"I need assistance with machine learning and deep learning concepts.\"\n",
    "]\n",
    "\n",
    "print(\"Batch Processing Example:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Encode each sentence\n",
    "encoded_sentences = []\n",
    "for sentence in batch_sentences:\n",
    "    encoded = tokenizer.encode(sentence, add_special_tokens=True)\n",
    "    encoded_sentences.append(encoded)\n",
    "    print(f\"'{sentence}' -> {encoded} (length: {len(encoded)})\")\n",
    "\n",
    "# Pad sequences to same length\n",
    "max_length = 15\n",
    "pad_id = tokenizer.token_to_id[tokenizer.special_tokens[\"pad_token\"]]\n",
    "\n",
    "padded_tensor = pad_sequences(encoded_sentences, max_length=max_length, pad_value=pad_id)\n",
    "\n",
    "print(f\"\\nPadded tensor shape: {padded_tensor.shape}\")\n",
    "print(f\"Padded tensor:\")\n",
    "print(padded_tensor)\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = (padded_tensor != pad_id).long()\n",
    "print(f\"\\nAttention mask:\")\n",
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text statistics\n",
    "def analyze_text_lengths(texts: List[str], tokenizer: SimpleTokenizer) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze token length statistics for a collection of texts.\n",
    "    \"\"\"\n",
    "    lengths = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        lengths.append(len(tokens))\n",
    "    \n",
    "    return {\n",
    "        'min_length': min(lengths),\n",
    "        'max_length': max(lengths),\n",
    "        'avg_length': sum(lengths) / len(lengths),\n",
    "        'lengths': lengths\n",
    "    }\n",
    "\n",
    "# Analyze our conversation data\n",
    "stats = analyze_text_lengths(texts, tokenizer)\n",
    "\n",
    "print(f\"Text Length Statistics:\")\n",
    "print(f\"Min length: {stats['min_length']} tokens\")\n",
    "print(f\"Max length: {stats['max_length']} tokens\")\n",
    "print(f\"Avg length: {stats['avg_length']:.2f} tokens\")\n",
    "\n",
    "# Plot length distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(stats['lengths'], bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_lengths = sorted(stats['lengths'])\n",
    "percentiles = [i / len(sorted_lengths) * 100 for i in range(len(sorted_lengths))]\n",
    "plt.plot(sorted_lengths, percentiles)\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Cumulative %')\n",
    "plt.title('Cumulative Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentile lines\n",
    "p95_idx = int(0.95 * len(sorted_lengths))\n",
    "p95_length = sorted_lengths[p95_idx]\n",
    "plt.axvline(p95_length, color='red', linestyle='--', label=f'95th percentile: {p95_length}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRecommended max_length for 95% coverage: {p95_length} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Preprocessing Pipeline\n",
    "\n",
    "Put everything together into a reusable preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotPreprocessor:\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for chatbot data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 1000, max_length: int = 128):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.pad_id = None\n",
    "    \n",
    "    def fit(self, texts: List[str], min_freq: int = 2):\n",
    "        \"\"\"\n",
    "        Fit the preprocessor on training texts.\n",
    "        \"\"\"\n",
    "        print(f\"Fitting preprocessor on {len(texts)} texts...\")\n",
    "        \n",
    "        # Build tokenizer\n",
    "        self.tokenizer = build_vocab_from_texts(texts, self.vocab_size, min_freq)\n",
    "        self.pad_id = self.tokenizer.token_to_id[self.tokenizer.special_tokens[\"pad_token\"]]\n",
    "        \n",
    "        print(f\"Vocabulary size: {self.tokenizer.get_vocab_size()}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Transform texts to tensors.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: (input_ids, attention_mask)\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Preprocessor not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Encode texts\n",
    "        encoded_texts = []\n",
    "        for text in texts:\n",
    "            encoded = self.tokenizer.encode(text, add_special_tokens=True, max_length=self.max_length)\n",
    "            encoded_texts.append(encoded)\n",
    "        \n",
    "        # Pad sequences\n",
    "        input_ids = pad_sequences(encoded_texts, max_length=self.max_length, pad_value=self.pad_id)\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = (input_ids != self.pad_id).long()\n",
    "        \n",
    "        return input_ids, attention_mask\n",
    "    \n",
    "    def fit_transform(self, texts: List[str], **fit_params) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fit and transform in one step.\n",
    "        \"\"\"\n",
    "        return self.fit(texts, **fit_params).transform(texts)\n",
    "    \n",
    "    def decode(self, token_ids: torch.Tensor, skip_special_tokens: bool = True) -> List[str]:\n",
    "        \"\"\"\n",
    "        Decode tensor back to texts.\n",
    "        \"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Preprocessor not fitted.\")\n",
    "        \n",
    "        texts = []\n",
    "        for row in token_ids:\n",
    "            text = self.tokenizer.decode(row.tolist(), skip_special_tokens=skip_special_tokens)\n",
    "            texts.append(text)\n",
    "        \n",
    "        return texts\n",
    "\n",
    "# Test the complete pipeline\n",
    "preprocessor = ChatbotPreprocessor(vocab_size=500, max_length=32)\n",
    "\n",
    "# Fit on training data\n",
    "train_texts = texts[:40]  # Use first 40 texts for training\n",
    "test_texts = texts[40:45]  # Use next 5 for testing\n",
    "\n",
    "preprocessor.fit(train_texts)\n",
    "\n",
    "# Transform test data\n",
    "input_ids, attention_mask = preprocessor.transform(test_texts)\n",
    "\n",
    "print(f\"\\nTransformed data:\")\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "\n",
    "# Decode back to verify\n",
    "decoded_texts = preprocessor.decode(input_ids)\n",
    "\n",
    "print(f\"\\nOriginal vs Decoded:\")\n",
    "for orig, decoded in zip(test_texts[:3], decoded_texts[:3]):\n",
    "    print(f\"Original: {orig}\")\n",
    "    print(f\"Decoded:  {decoded}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've mastered text preprocessing for chatbot development:\n",
    "\n",
    "âœ… **Text Cleaning**: Normalization and cleaning techniques  \n",
    "âœ… **Tokenization**: Word, character, and subword strategies  \n",
    "âœ… **Vocabulary**: Building and managing token vocabularies  \n",
    "âœ… **Encoding**: Converting text to numerical representations  \n",
    "âœ… **Batch Processing**: Handling variable-length sequences  \n",
    "âœ… **Complete Pipeline**: End-to-end preprocessing system  \n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "In the next notebook, we'll use these preprocessing techniques to build neural networks for text classification:\n",
    "- Multi-layer perceptrons for text\n",
    "- Training and evaluation\n",
    "- Model architectures\n",
    "\n",
    "**Ready to build models?** Continue to [`04_neural_networks_basics.ipynb`](04_neural_networks_basics.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}