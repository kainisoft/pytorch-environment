{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Neural Networks Basics for Text Classification\n",
    "\n",
    "**Duration:** 2-3 hours | **Difficulty:** Intermediate\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "- Understand `nn.Module` and parameter management\n",
    "- Build text classification models\n",
    "- Implement complete training and evaluation pipelines\n",
    "- Analyze model performance and predictions\n",
    "\n",
    "## ðŸ“š Contents\n",
    "1. Understanding nn.Module\n",
    "2. Text Classification Architecture\n",
    "3. Training Loop Implementation\n",
    "4. Model Evaluation and Analysis\n",
    "5. Practical Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from text_utils import SimpleTokenizer, clean_text, pad_sequences\n",
    "from model_helpers import get_device, count_parameters\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding nn.Module\n",
    "\n",
    "The foundation of all PyTorch neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple text classifier using embeddings and linear layers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize model weights.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # Embed tokens\n",
    "        embeddings = self.embedding(input_ids)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Apply attention mask if provided\n",
    "        if attention_mask is not None:\n",
    "            embeddings = embeddings * attention_mask.unsqueeze(-1).float()\n",
    "            seq_lengths = attention_mask.sum(dim=1, keepdim=True).float()\n",
    "            pooled = embeddings.sum(dim=1) / seq_lengths\n",
    "        else:\n",
    "            pooled = embeddings.mean(dim=1)\n",
    "        \n",
    "        # Forward through layers\n",
    "        pooled = self.dropout(pooled)\n",
    "        hidden = F.relu(self.fc1(pooled))\n",
    "        hidden = self.dropout(hidden)\n",
    "        logits = self.fc2(hidden)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Create and examine model\n",
    "model = SimpleTextClassifier(\n",
    "    vocab_size=1000, embedding_dim=64, hidden_dim=128, num_classes=5\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nParameters: {count_parameters(model)}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, 1000, (2, 10))\n",
    "test_mask = torch.ones(2, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_input, test_mask)\n",
    "    \n",
    "print(f\"\\nInput: {test_input.shape} -> Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Load and prepare text classification data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversation data\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract texts and create intent mapping\n",
    "texts = []\n",
    "labels = []\n",
    "intent_to_id = {}\n",
    "\n",
    "for item in data:\n",
    "    query = clean_text(item['query'], lowercase=True, remove_extra_whitespace=True)\n",
    "    intent = item['intent']\n",
    "    \n",
    "    texts.append(query)\n",
    "    \n",
    "    if intent not in intent_to_id:\n",
    "        intent_to_id[intent] = len(intent_to_id)\n",
    "    labels.append(intent_to_id[intent])\n",
    "\n",
    "print(f\"Dataset: {len(texts)} texts, {len(intent_to_id)} intents\")\n",
    "print(f\"Intents: {list(intent_to_id.keys())}\")\n",
    "print(f\"Label distribution: {torch.bincount(torch.tensor(labels))}\")\n",
    "\n",
    "# Prepare tokenized data\n",
    "def prepare_data(texts, labels, vocab_size=500, max_length=32):\n",
    "    # Build tokenizer\n",
    "    tokenizer = SimpleTokenizer(vocab_size=vocab_size)\n",
    "    tokenizer.build_vocabulary(texts, min_freq=1)\n",
    "    \n",
    "    # Encode texts\n",
    "    encoded_texts = [tokenizer.encode(text, add_special_tokens=True, max_length=max_length) \n",
    "                    for text in texts]\n",
    "    \n",
    "    # Create tensors\n",
    "    pad_id = tokenizer.token_to_id[tokenizer.special_tokens[\"pad_token\"]]\n",
    "    input_ids = pad_sequences(encoded_texts, max_length=max_length, pad_value=pad_id)\n",
    "    attention_mask = (input_ids != pad_id).long()\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return input_ids, attention_mask, label_tensor, tokenizer\n",
    "\n",
    "input_ids, attention_mask, label_tensor, tokenizer = prepare_data(texts, labels)\n",
    "print(f\"\\nPrepared data: {input_ids.shape}, vocab size: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Implementation\n",
    "\n",
    "Complete training loop with data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train_size = int(0.8 * len(input_ids))\n",
    "indices = torch.randperm(len(input_ids))\n",
    "\n",
    "train_input_ids = input_ids[indices[:train_size]]\n",
    "train_attention_mask = attention_mask[indices[:train_size]]\n",
    "train_labels = label_tensor[indices[:train_size]]\n",
    "\n",
    "test_input_ids = input_ids[indices[train_size:]]\n",
    "test_attention_mask = attention_mask[indices[train_size:]]\n",
    "test_labels = label_tensor[indices[train_size:]]\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
    "\n",
    "# Create model\n",
    "model = SimpleTextClassifier(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_classes=len(intent_to_id),\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 15\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_input_ids, batch_attention_mask, batch_labels in train_loader:\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_attention_mask = batch_attention_mask.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_input_ids, batch_attention_mask)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "        total += batch_labels.size(0)\n",
    "    \n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = correct / total\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_input_ids, batch_attention_mask, batch_labels in test_loader:\n",
    "            batch_input_ids = batch_input_ids.to(device)\n",
    "            batch_attention_mask = batch_attention_mask.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            \n",
    "            logits = model(batch_input_ids, batch_attention_mask)\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            test_correct += (predicted == batch_labels).sum().item()\n",
    "            test_total += batch_labels.size(0)\n",
    "    \n",
    "    test_acc = test_correct / test_total\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:2d}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nFinal Test Accuracy: {test_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation and Analysis\n",
    "\n",
    "Detailed analysis of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get detailed predictions\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_input_ids, batch_attention_mask, batch_labels in test_loader:\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_attention_mask = batch_attention_mask.to(device)\n",
    "        \n",
    "        logits = model(batch_input_ids, batch_attention_mask)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "id_to_intent = {v: k for k, v in intent_to_id.items()}\n",
    "target_names = [id_to_intent[i] for i in range(len(intent_to_id))]\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=target_names, yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise: Test New Examples\n",
    "\n",
    "Test the trained model on new text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent(text):\n",
    "    \"\"\"\n",
    "    Predict intent for new text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    cleaned_text = clean_text(text, lowercase=True, remove_extra_whitespace=True)\n",
    "    encoded = tokenizer.encode(cleaned_text, add_special_tokens=True, max_length=32)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    input_ids = torch.tensor([encoded]).to(device)\n",
    "    pad_id = tokenizer.token_to_id[tokenizer.special_tokens[\"pad_token\"]]\n",
    "    attention_mask = (input_ids != pad_id).long()\n",
    "    \n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        predicted_id = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    predicted_intent = id_to_intent[predicted_id]\n",
    "    confidence = probabilities[0][predicted_id].item()\n",
    "    \n",
    "    return predicted_intent, confidence, probabilities[0].cpu().numpy()\n",
    "\n",
    "# Test examples\n",
    "test_texts = [\n",
    "    \"Hi there, how are you?\",\n",
    "    \"What's your name?\",\n",
    "    \"Can you explain machine learning?\",\n",
    "    \"Thank you so much!\",\n",
    "    \"Goodbye, see you later\"\n",
    "]\n",
    "\n",
    "print(\"Testing model on new examples:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for text in test_texts:\n",
    "    intent, confidence, probs = predict_intent(text)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Predicted: {intent} (confidence: {confidence:.3f})\")\n",
    "    \n",
    "    # Show top 2 predictions\n",
    "    top_indices = np.argsort(probs)[::-1][:2]\n",
    "    print(\"Top predictions:\")\n",
    "    for i, idx in enumerate(top_indices):\n",
    "        intent_name = target_names[idx]\n",
    "        print(f\"  {i+1}. {intent_name}: {probs[idx]:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully built and trained a neural network for text classification:\n",
    "\n",
    "âœ… **nn.Module**: Understanding PyTorch's neural network foundation  \n",
    "âœ… **Text Classification**: Complete pipeline from data to predictions  \n",
    "âœ… **Training Loop**: Implementing training with validation  \n",
    "âœ… **Model Evaluation**: Classification reports and confusion matrices  \n",
    "âœ… **Real-world Testing**: Predicting on new examples  \n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "In the next notebook, we'll explore language modeling:\n",
    "- Character and word-level models\n",
    "- Text generation techniques\n",
    "- Perplexity evaluation\n",
    "\n",
    "**Ready for language modeling?** Continue to [`05_language_modeling.ipynb`](05_language_modeling.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}