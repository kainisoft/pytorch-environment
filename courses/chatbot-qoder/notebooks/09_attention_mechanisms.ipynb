{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09: Attention Mechanisms\n",
    "\n",
    "**Duration:** 3-4 hours | **Difficulty:** Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "- Attention mechanism fundamentals\n",
    "- Multi-head attention implementation\n",
    "- Attention visualization techniques\n",
    "- Attention-enhanced seq2seq models\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Attention](#1-introduction)\n",
    "2. [Basic Attention](#2-basic-attention)\n",
    "3. [Multi-Head Attention](#3-multihead)\n",
    "4. [Attention Visualization](#4-visualization)\n",
    "5. [Practical Exercise](#5-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.model_helpers import get_device, count_parameters\n",
    "\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Attention {#1-introduction}\n",
    "\n",
    "**Attention mechanisms** solve the information bottleneck in seq2seq models:\n",
    "\n",
    "- **Problem**: Fixed context vector loses information in long sequences\n",
    "- **Solution**: Dynamically attend to all encoder states\n",
    "- **Benefit**: Better alignment and long-range dependencies\n",
    "\n",
    "### Core Idea:\n",
    "Instead of compressing entire input into one vector, create weighted combinations of all input states based on current decoder state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention concept\n",
    "def visualize_attention_concept():\n",
    "    \"\"\"Visualize basic attention alignment.\"\"\"\n",
    "    # Simulated attention weights for \"How are you?\" -> \"I am fine\"\n",
    "    input_words = ['How', 'are', 'you', '?']\n",
    "    output_words = ['I', 'am', 'fine']\n",
    "    \n",
    "    # Simulated attention matrix (output x input)\n",
    "    attention_matrix = np.array([\n",
    "        [0.1, 0.2, 0.6, 0.1],  # \"I\" attends mostly to \"you\"\n",
    "        [0.2, 0.7, 0.1, 0.0],  # \"am\" attends mostly to \"are\"\n",
    "        [0.1, 0.1, 0.2, 0.6]   # \"fine\" attends mostly to \"?\"\n",
    "    ])\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=input_words, \n",
    "                yticklabels=output_words,\n",
    "                annot=True, cmap='Blues', cbar=True)\n",
    "    plt.title('Attention Alignment Example')\n",
    "    plt.xlabel('Input Words')\n",
    "    plt.ylabel('Output Words')\n",
    "    plt.show()\n",
    "    \n",
    "    return attention_matrix\n",
    "\n",
    "attention_example = visualize_attention_concept()\n",
    "print(\"Each row shows where the output word 'attends' to in the input.\")\n",
    "print(\"Higher values (darker blue) indicate stronger attention.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Attention Implementation {#2-basic-attention}\n",
    "\n",
    "Implementing additive (Bahdanau) attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    \"\"\"Additive (Bahdanau) attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, attention_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.encoder_proj = nn.Linear(hidden_dim, attention_dim, bias=False)\n",
    "        self.decoder_proj = nn.Linear(hidden_dim, attention_dim, bias=False)\n",
    "        self.attention_v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.output_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            decoder_hidden: (batch_size, hidden_dim)\n",
    "            encoder_outputs: (batch_size, seq_len, hidden_dim)\n",
    "            mask: (batch_size, seq_len)\n",
    "        \n",
    "        Returns:\n",
    "            context: (batch_size, hidden_dim)\n",
    "            attention_weights: (batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_dim = encoder_outputs.shape\n",
    "        \n",
    "        # Project encoder and decoder states\n",
    "        encoder_proj = self.encoder_proj(encoder_outputs)  # (batch, seq_len, att_dim)\n",
    "        decoder_proj = self.decoder_proj(decoder_hidden).unsqueeze(1)  # (batch, 1, att_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = self.attention_v(torch.tanh(encoder_proj + decoder_proj)).squeeze(-1)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -float('inf'))\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = F.softmax(scores, dim=1)\n",
    "        \n",
    "        # Context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        \n",
    "        # Combine with decoder hidden\n",
    "        combined = torch.cat([context, decoder_hidden], dim=1)\n",
    "        output = torch.tanh(self.output_proj(combined))\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention (foundation of transformers).\"\"\"\n",
    "    \n",
    "    def __init__(self, d_k: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query: (batch_size, seq_len_q, d_k)\n",
    "            key: (batch_size, seq_len_k, d_k)\n",
    "            value: (batch_size, seq_len_v, d_k)\n",
    "            mask: (batch_size, seq_len_q, seq_len_k)\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch_size, seq_len_q, d_k)\n",
    "            attention_weights: (batch_size, seq_len_q, seq_len_k)\n",
    "        \"\"\"\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply to values\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test basic attention\n",
    "hidden_dim = 256\n",
    "seq_len = 8\n",
    "batch_size = 2\n",
    "\n",
    "decoder_hidden = torch.randn(batch_size, hidden_dim)\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "attention = AdditiveAttention(hidden_dim)\n",
    "context, weights = attention(decoder_hidden, encoder_outputs)\n",
    "\n",
    "print(f\"Additive Attention Test:\")\n",
    "print(f\"Context shape: {context.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"Weights sum (should be 1.0): {weights.sum(dim=1)}\")\n",
    "print(f\"Sample attention weights: {weights[0].detach().numpy():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention {#3-multihead}\n",
    "\n",
    "Multi-head attention allows attending to different representation subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(self.d_k, dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size, seq_len_q, _ = query.shape\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q = self.w_q(query).view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
    "        \n",
    "        output, attention_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        output = output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len_q, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.w_o(output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 256\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "output, attention_weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Multi-Head Attention Test:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "print(f\"Parameters: {count_parameters(mha)['total']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attention Visualization {#4-visualization}\n",
    "\n",
    "Visualizing attention patterns to understand model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_heads(attention_weights, input_tokens=None, max_heads=4):\n",
    "    \"\"\"Visualize attention patterns from multiple heads.\"\"\"\n",
    "    batch_idx = 0  # Show first example in batch\n",
    "    attention = attention_weights[batch_idx].detach().numpy()\n",
    "    \n",
    "    num_heads = min(attention.shape[0], max_heads)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_heads, figsize=(4 * num_heads, 4))\n",
    "    if num_heads == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        im = axes[head].imshow(attention[head], cmap='Blues', aspect='auto')\n",
    "        axes[head].set_title(f'Head {head + 1}')\n",
    "        axes[head].set_xlabel('Key Position')\n",
    "        axes[head].set_ylabel('Query Position')\n",
    "        \n",
    "        # Add token labels if provided\n",
    "        if input_tokens:\n",
    "            axes[head].set_xticks(range(len(input_tokens)))\n",
    "            axes[head].set_xticklabels(input_tokens, rotation=45)\n",
    "            axes[head].set_yticks(range(len(input_tokens)))\n",
    "            axes[head].set_yticklabels(input_tokens)\n",
    "        \n",
    "        plt.colorbar(im, ax=axes[head], fraction=0.046, pad=0.04)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def create_sample_attention_pattern():\n",
    "    \"\"\"Create interpretable attention pattern for visualization.\"\"\"\n",
    "    tokens = ['What', 'is', 'machine', 'learning', '?']\n",
    "    seq_len = len(tokens)\n",
    "    num_heads = 4\n",
    "    \n",
    "    # Create different attention patterns for each head\n",
    "    attention_patterns = torch.zeros(1, num_heads, seq_len, seq_len)\n",
    "    \n",
    "    # Head 1: Local attention (adjacent words)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(max(0, i-1), min(seq_len, i+2)):\n",
    "            attention_patterns[0, 0, i, j] = 0.5 if i != j else 0.3\n",
    "    \n",
    "    # Head 2: Query words attention\n",
    "    question_words = [0, 4]  # \"What\" and \"?\"\n",
    "    for i in range(seq_len):\n",
    "        for j in question_words:\n",
    "            attention_patterns[0, 1, i, j] = 0.4\n",
    "        attention_patterns[0, 1, i, i] = 0.2\n",
    "    \n",
    "    # Head 3: Content words attention\n",
    "    content_words = [2, 3]  # \"machine\", \"learning\"\n",
    "    for i in range(seq_len):\n",
    "        for j in content_words:\n",
    "            attention_patterns[0, 2, i, j] = 0.4\n",
    "        attention_patterns[0, 2, i, i] = 0.2\n",
    "    \n",
    "    # Head 4: Global attention (uniform)\n",
    "    attention_patterns[0, 3, :, :] = 0.2\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    attention_patterns = F.softmax(attention_patterns, dim=-1)\n",
    "    \n",
    "    return attention_patterns, tokens\n",
    "\n",
    "# Visualize sample attention patterns\n",
    "sample_attention, tokens = create_sample_attention_pattern()\n",
    "print(\"Sample Attention Patterns:\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "visualize_attention_heads(sample_attention, tokens)\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"Head 1: Local attention (focuses on adjacent words)\")\n",
    "print(\"Head 2: Question structure (focuses on 'What' and '?')\")\n",
    "print(\"Head 3: Content focus (focuses on 'machine' and 'learning')\")\n",
    "print(\"Head 4: Global context (uniform attention)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise {#5-exercise}\n",
    "\n",
    "**Exercise**: Implement and experiment with attention mechanisms\n",
    "\n",
    "### Tasks:\n",
    "1. Compare additive vs multiplicative attention\n",
    "2. Implement attention in seq2seq decoder\n",
    "3. Visualize attention alignments\n",
    "4. Experiment with different numbers of heads\n",
    "\n",
    "### Questions:\n",
    "1. How does attention help with long sequences?\n",
    "2. What do different attention heads capture?\n",
    "3. When might attention patterns be problematic?\n",
    "\n",
    "### Extensions:\n",
    "- Self-attention for encoder\n",
    "- Cross-attention between sequences\n",
    "- Attention dropout and regularization\n",
    "- Positional encoding effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Attention analysis\n",
    "def analyze_attention_patterns():\n",
    "    \"\"\"Analyze different attention mechanisms.\"\"\"\n",
    "    \n",
    "    print(\"=== Attention Analysis ===\")\n",
    "    \n",
    "    # Create test sequences\n",
    "    seq_lengths = [5, 10, 20]\n",
    "    hidden_dim = 128\n",
    "    \n",
    "    additive_attn = AdditiveAttention(hidden_dim)\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Test data\n",
    "        decoder_hidden = torch.randn(1, hidden_dim)\n",
    "        encoder_outputs = torch.randn(1, seq_len, hidden_dim)\n",
    "        \n",
    "        # Compute attention\n",
    "        context, weights = additive_attn(decoder_hidden, encoder_outputs)\n",
    "        \n",
    "        # Analyze attention distribution\n",
    "        entropy = -torch.sum(weights * torch.log(weights + 1e-8), dim=1)\n",
    "        max_attention = torch.max(weights, dim=1)[0]\n",
    "        \n",
    "        print(f\"\\nSequence length: {seq_len}\")\n",
    "        print(f\"Attention entropy: {entropy.item():.3f} (higher = more uniform)\")\n",
    "        print(f\"Max attention weight: {max_attention.item():.3f}\")\n",
    "        print(f\"Attention distribution: {weights[0].detach().numpy()[:5]}...\")\n",
    "\n",
    "def compare_attention_types():\n",
    "    \"\"\"Compare different attention mechanisms.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Attention Type Comparison ===\")\n",
    "    \n",
    "    hidden_dim = 256\n",
    "    seq_len = 8\n",
    "    \n",
    "    # Test inputs\n",
    "    decoder_hidden = torch.randn(1, hidden_dim)\n",
    "    encoder_outputs = torch.randn(1, seq_len, hidden_dim)\n",
    "    \n",
    "    # Additive attention\n",
    "    additive_attn = AdditiveAttention(hidden_dim)\n",
    "    context_add, weights_add = additive_attn(decoder_hidden, encoder_outputs)\n",
    "    \n",
    "    # Scaled dot-product attention\n",
    "    dot_attn = ScaledDotProductAttention(hidden_dim)\n",
    "    query = decoder_hidden.unsqueeze(1)  # (1, 1, hidden_dim)\n",
    "    context_dot, weights_dot = dot_attn(query, encoder_outputs, encoder_outputs)\n",
    "    \n",
    "    print(f\"Additive attention:\")\n",
    "    print(f\"  Parameters: {count_parameters(additive_attn)['total']:,}\")\n",
    "    print(f\"  Attention weights: {weights_add[0][:4].detach().numpy()}...\")\n",
    "    \n",
    "    print(f\"\\nScaled dot-product attention:\")\n",
    "    print(f\"  Parameters: {count_parameters(dot_attn)['total']:,}\")\n",
    "    print(f\"  Attention weights: {weights_dot[0, 0, :4].detach().numpy()}...\")\n",
    "\n",
    "# Run analysis\n",
    "analyze_attention_patterns()\n",
    "compare_attention_types()\n",
    "\n",
    "print(\"\\n=== Attention Mechanisms Complete ===\")\n",
    "print(\"Key Concepts Learned:\")\n",
    "print(\"• Attention solves information bottleneck in seq2seq\")\n",
    "print(\"• Additive vs multiplicative attention mechanisms\")\n",
    "print(\"• Multi-head attention for diverse representations\")\n",
    "print(\"• Attention visualization and interpretation\")\n",
    "print(\"• Foundation for transformer architectures\")\n",
    "print(\"\\nNext: Full transformer implementation!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}