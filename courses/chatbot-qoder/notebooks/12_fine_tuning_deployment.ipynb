{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12: Fine-Tuning and Deployment\n",
    "\n",
    "**Duration:** 3-4 hours | **Difficulty:** Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "- Model fine-tuning strategies\n",
    "- Deployment optimization techniques\n",
    "- Production monitoring and maintenance\n",
    "- Complete chatbot deployment pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.text_utils import SimpleTokenizer\n",
    "from utils.model_helpers import get_device, count_parameters, save_checkpoint\n",
    "\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Strategies\n",
    "\n",
    "**Key approaches**: Transfer learning, domain adaptation, parameter-efficient tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningManager:\n",
    "    \"\"\"Manage fine-tuning process for chatbot models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.training_history = []\n",
    "    \n",
    "    def freeze_layers(self, freeze_embeddings=True, freeze_layers=2):\n",
    "        \"\"\"Freeze specific layers for transfer learning.\"\"\"\n",
    "        frozen_params = 0\n",
    "        total_params = 0\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            total_params += param.numel()\n",
    "            \n",
    "            if freeze_embeddings and 'embedding' in name.lower():\n",
    "                param.requires_grad = False\n",
    "                frozen_params += param.numel()\n",
    "            elif any(f'layer.{i}' in name for i in range(freeze_layers)):\n",
    "                param.requires_grad = False\n",
    "                frozen_params += param.numel()\n",
    "        \n",
    "        print(f\"Frozen {frozen_params:,} / {total_params:,} parameters\")\n",
    "        return frozen_params, total_params\n",
    "    \n",
    "    def setup_optimizer(self, learning_rate=2e-5):\n",
    "        \"\"\"Setup optimizer with layer-specific learning rates.\"\"\"\n",
    "        # Lower learning rate for embeddings\n",
    "        embedding_params = [p for n, p in self.model.named_parameters() \n",
    "                          if 'embedding' in n.lower() and p.requires_grad]\n",
    "        other_params = [p for n, p in self.model.named_parameters() \n",
    "                       if 'embedding' not in n.lower() and p.requires_grad]\n",
    "        \n",
    "        param_groups = [\n",
    "            {'params': embedding_params, 'lr': learning_rate * 0.1},\n",
    "            {'params': other_params, 'lr': learning_rate}\n",
    "        ]\n",
    "        \n",
    "        self.optimizer = optim.AdamW(param_groups, weight_decay=0.01)\n",
    "        print(f\"Optimizer configured with {len(param_groups)} parameter groups\")\n",
    "    \n",
    "    def fine_tune(self, dataloader, epochs=3):\n",
    "        \"\"\"Execute fine-tuning process.\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "            \n",
    "            for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "                loss.backward()\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 5 == 0:\n",
    "                    print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "            \n",
    "            avg_loss = total_loss / len(dataloader)\n",
    "            self.training_history.append({'epoch': epoch + 1, 'loss': avg_loss})\n",
    "            print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Load data\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    conversations = [(item['question'], item['answer']) for item in json.load(f)]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=1500)\n",
    "all_text = [text for conv in conversations for text in conv]\n",
    "tokenizer.fit(all_text)\n",
    "\n",
    "print(f\"Fine-tuning setup: {len(conversations)} conversations, {len(tokenizer.vocab)} vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Deployment\n",
    "\n",
    "**Optimization techniques**: Quantization, pruning, model export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOptimizer:\n",
    "    \"\"\"Optimize models for production deployment.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_model(model):\n",
    "        \"\"\"Apply dynamic quantization.\"\"\"\n",
    "        quantized = torch.quantization.quantize_dynamic(\n",
    "            model, {nn.Linear}, dtype=torch.qint8\n",
    "        )\n",
    "        return quantized\n",
    "    \n",
    "    @staticmethod\n",
    "    def benchmark_model(model, sample_input, runs=50):\n",
    "        \"\"\"Benchmark inference speed.\"\"\"\n",
    "        model.eval()\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                _ = model(sample_input)\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(runs):\n",
    "                _ = model(sample_input)\n",
    "        end = time.time()\n",
    "        \n",
    "        avg_time = (end - start) / runs\n",
    "        return {'avg_time_ms': avg_time * 1000, 'throughput': 1.0 / avg_time}\n",
    "\n",
    "class DeploymentPipeline:\n",
    "    \"\"\"Complete deployment pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = ModelOptimizer()\n",
    "    \n",
    "    def prepare_for_deployment(self, quantize=True):\n",
    "        \"\"\"Optimize model for deployment.\"\"\"\n",
    "        sample_input = torch.randint(0, len(self.tokenizer.vocab), (1, 20))\n",
    "        \n",
    "        # Original performance\n",
    "        original_params = count_parameters(self.model)\n",
    "        original_perf = self.optimizer.benchmark_model(self.model, sample_input)\n",
    "        \n",
    "        print(f\"Original model:\")\n",
    "        print(f\"  Parameters: {original_params['total']:,}\")\n",
    "        print(f\"  Inference: {original_perf['avg_time_ms']:.2f} ms\")\n",
    "        \n",
    "        # Apply optimizations\n",
    "        optimized_model = self.model\n",
    "        \n",
    "        if quantize:\n",
    "            optimized_model = self.optimizer.quantize_model(optimized_model)\n",
    "            quantized_perf = self.optimizer.benchmark_model(optimized_model, sample_input)\n",
    "            print(f\"\\nQuantized model:\")\n",
    "            print(f\"  Inference: {quantized_perf['avg_time_ms']:.2f} ms\")\n",
    "            print(f\"  Speedup: {original_perf['avg_time_ms'] / quantized_perf['avg_time_ms']:.2f}x\")\n",
    "        \n",
    "        self.optimized_model = optimized_model\n",
    "        return optimized_model\n",
    "    \n",
    "    def export_model(self, export_path):\n",
    "        \"\"\"Export model for deployment.\"\"\"\n",
    "        os.makedirs(os.path.dirname(export_path), exist_ok=True)\n",
    "        \n",
    "        # Export to TorchScript\n",
    "        sample_input = torch.randint(0, len(self.tokenizer.vocab), (1, 20))\n",
    "        scripted_model = torch.jit.trace(self.optimized_model, sample_input)\n",
    "        scripted_model.save(export_path)\n",
    "        \n",
    "        # Save tokenizer\n",
    "        tokenizer_path = export_path.replace('.pt', '_tokenizer.json')\n",
    "        with open(tokenizer_path, 'w') as f:\n",
    "            json.dump({\n",
    "                'vocab': self.tokenizer.vocab,\n",
    "                'special_tokens': self.tokenizer.special_tokens,\n",
    "                'vocab_size': len(self.tokenizer.vocab)\n",
    "            }, f)\n",
    "        \n",
    "        print(f\"Model exported: {export_path}\")\n",
    "        print(f\"Tokenizer saved: {tokenizer_path}\")\n",
    "        return export_path, tokenizer_path\n",
    "\n",
    "# Demo model for optimization\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.linear1 = nn.Linear(d_model, d_model)\n",
    "        self.linear2 = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).mean(dim=1)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        return self.linear2(x)\n",
    "\n",
    "# Create and optimize model\n",
    "demo_model = SimpleModel(len(tokenizer.vocab)).to(device)\n",
    "pipeline = DeploymentPipeline(demo_model, tokenizer)\n",
    "optimized_model = pipeline.prepare_for_deployment(quantize=True)\n",
    "\n",
    "# Export for deployment\n",
    "model_path, tokenizer_path = pipeline.export_model('../models/production_model.pt')\n",
    "print(f\"\\nDeployment ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Monitoring\n",
    "\n",
    "**Key metrics**: Response time, accuracy, safety, user satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMonitor:\n",
    "    \"\"\"Monitor chatbot performance in production.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'successful_responses': 0,\n",
    "            'response_times': [],\n",
    "            'safety_flags': 0,\n",
    "            'user_ratings': []\n",
    "        }\n",
    "        self.alerts = []\n",
    "    \n",
    "    def log_request(self, response_time, success=True):\n",
    "        \"\"\"Log request metrics.\"\"\"\n",
    "        self.metrics['total_requests'] += 1\n",
    "        self.metrics['response_times'].append(response_time)\n",
    "        \n",
    "        if success:\n",
    "            self.metrics['successful_responses'] += 1\n",
    "        \n",
    "        # Check for slow responses\n",
    "        if response_time > 2.0:  # 2 second threshold\n",
    "            self.alerts.append({\n",
    "                'type': 'slow_response',\n",
    "                'time': response_time,\n",
    "                'timestamp': time.time()\n",
    "            })\n",
    "    \n",
    "    def log_safety_flag(self, reason):\n",
    "        \"\"\"Log safety violations.\"\"\"\n",
    "        self.metrics['safety_flags'] += 1\n",
    "        self.alerts.append({\n",
    "            'type': 'safety_flag',\n",
    "            'reason': reason,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "    \n",
    "    def log_user_rating(self, rating):\n",
    "        \"\"\"Log user satisfaction rating (1-5).\"\"\"\n",
    "        self.metrics['user_ratings'].append(rating)\n",
    "    \n",
    "    def get_dashboard(self):\n",
    "        \"\"\"Get monitoring dashboard data.\"\"\"\n",
    "        import numpy as np\n",
    "        \n",
    "        success_rate = (self.metrics['successful_responses'] / \n",
    "                       max(1, self.metrics['total_requests']))\n",
    "        \n",
    "        avg_response_time = (np.mean(self.metrics['response_times']) \n",
    "                           if self.metrics['response_times'] else 0)\n",
    "        \n",
    "        avg_rating = (np.mean(self.metrics['user_ratings']) \n",
    "                     if self.metrics['user_ratings'] else 0)\n",
    "        \n",
    "        return {\n",
    "            'total_requests': self.metrics['total_requests'],\n",
    "            'success_rate': success_rate,\n",
    "            'avg_response_time_ms': avg_response_time * 1000,\n",
    "            'safety_flags': self.metrics['safety_flags'],\n",
    "            'avg_user_rating': avg_rating,\n",
    "            'recent_alerts': self.alerts[-5:]\n",
    "        }\n",
    "\n",
    "class ProductionChatbot:\n",
    "    \"\"\"Production-ready chatbot with monitoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, monitor):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.monitor = monitor\n",
    "        self.unsafe_words = ['hate', 'violence', 'harmful']\n",
    "    \n",
    "    def safety_check(self, text):\n",
    "        \"\"\"Basic safety filter.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        for word in self.unsafe_words:\n",
    "            if word in text_lower:\n",
    "                return False, word\n",
    "        return True, None\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        \"\"\"Generate response with monitoring.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Safety check\n",
    "            is_safe, flag = self.safety_check(user_input)\n",
    "            if not is_safe:\n",
    "                self.monitor.log_safety_flag(f'input_contains_{flag}')\n",
    "                response = \"I can't respond to that type of message.\"\n",
    "            else:\n",
    "                # Generate response (simplified)\n",
    "                tokens = self.tokenizer.encode(user_input, add_special_tokens=True, max_length=50)\n",
    "                input_tensor = torch.tensor([tokens]).to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = self.model(input_tensor)\n",
    "                    predicted = torch.argmax(output, dim=-1)\n",
    "                    response = self.tokenizer.decode(predicted[0].cpu().tolist())\n",
    "                \n",
    "                # Safety check response\n",
    "                response_safe, flag = self.safety_check(response)\n",
    "                if not response_safe:\n",
    "                    self.monitor.log_safety_flag(f'output_contains_{flag}')\n",
    "                    response = \"I apologize, but I need to rephrase my response.\"\n",
    "            \n",
    "            # Log successful request\n",
    "            response_time = time.time() - start_time\n",
    "            self.monitor.log_request(response_time, success=True)\n",
    "            \n",
    "            return {\n",
    "                'response': response,\n",
    "                'response_time': response_time,\n",
    "                'safe': is_safe,\n",
    "                'status': 'success'\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log failed request\n",
    "            response_time = time.time() - start_time\n",
    "            self.monitor.log_request(response_time, success=False)\n",
    "            \n",
    "            return {\n",
    "                'response': \"I'm having technical difficulties. Please try again.\",\n",
    "                'response_time': response_time,\n",
    "                'safe': True,\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Create production system\n",
    "monitor = ProductionMonitor()\n",
    "prod_chatbot = ProductionChatbot(optimized_model, tokenizer, monitor)\n",
    "\n",
    "# Demo production usage\n",
    "print(\"=== Production Chatbot Demo ===\")\n",
    "test_inputs = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Tell me about AI\",\n",
    "    \"How can you help me?\"\n",
    "]\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    result = prod_chatbot.respond(user_input)\n",
    "    print(f\"\\nUser: {user_input}\")\n",
    "    print(f\"Bot: {result['response']}\")\n",
    "    print(f\"Time: {result['response_time']*1000:.1f}ms, Status: {result['status']}\")\n",
    "    \n",
    "    # Simulate user feedback\n",
    "    rating = 4  # Simulated rating\n",
    "    monitor.log_user_rating(rating)\n",
    "\n",
    "# Show monitoring dashboard\n",
    "dashboard = monitor.get_dashboard()\n",
    "print(f\"\\n=== Monitoring Dashboard ===\")\n",
    "print(f\"Total Requests: {dashboard['total_requests']}\")\n",
    "print(f\"Success Rate: {dashboard['success_rate']:.2%}\")\n",
    "print(f\"Avg Response Time: {dashboard['avg_response_time_ms']:.1f}ms\")\n",
    "print(f\"Safety Flags: {dashboard['safety_flags']}\")\n",
    "print(f\"Avg User Rating: {dashboard['avg_user_rating']:.1f}/5\")\n",
    "print(f\"Recent Alerts: {len(dashboard['recent_alerts'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Checklist\n",
    "\n",
    "**Essential considerations** for deploying chatbots in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production deployment checklist\n",
    "def print_production_checklist():\n",
    "    \"\"\"Complete checklist for production deployment.\"\"\"\n",
    "    \n",
    "    checklist = {\n",
    "        \"Model Preparation\": [\n",
    "            \"‚úì Model trained and validated on diverse data\",\n",
    "            \"‚úì Fine-tuned for specific domain/use case\",\n",
    "            \"‚úì Quantized and optimized for inference\",\n",
    "            \"‚úì Exported to deployment format (TorchScript/ONNX)\",\n",
    "            \"‚úì Model versioning and artifact management\"\n",
    "        ],\n",
    "        \"Safety and Ethics\": [\n",
    "            \"‚úì Content filtering and safety checks implemented\",\n",
    "            \"‚úì Bias testing and mitigation strategies\",\n",
    "            \"‚úì Privacy protection measures\",\n",
    "            \"‚úì Clear user disclaimers about AI limitations\",\n",
    "            \"‚úì Human escalation mechanisms\"\n",
    "        ],\n",
    "        \"Infrastructure\": [\n",
    "            \"‚úì Scalable deployment architecture\",\n",
    "            \"‚úì Load balancing and auto-scaling\",\n",
    "            \"‚úì Fallback mechanisms for failures\",\n",
    "            \"‚úì Rate limiting and abuse prevention\",\n",
    "            \"‚úì Secure API endpoints\"\n",
    "        ],\n",
    "        \"Monitoring and Maintenance\": [\n",
    "            \"‚úì Real-time performance monitoring\",\n",
    "            \"‚úì Error tracking and alerting\",\n",
    "            \"‚úì User feedback collection\",\n",
    "            \"‚úì Regular model retraining pipeline\",\n",
    "            \"‚úì A/B testing framework\"\n",
    "        ],\n",
    "        \"Compliance and Documentation\": [\n",
    "            \"‚úì Data privacy compliance (GDPR, CCPA)\",\n",
    "            \"‚úì Security audits and penetration testing\",\n",
    "            \"‚úì API documentation and user guides\",\n",
    "            \"‚úì Incident response procedures\",\n",
    "            \"‚úì Regular compliance reviews\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nüöÄ PRODUCTION DEPLOYMENT CHECKLIST\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for category, items in checklist.items():\n",
    "        print(f\"\\nüìã {category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    print(\"\\nüéØ Best Practices:\")\n",
    "    print(\"‚Ä¢ Start with safety-first design principles\")\n",
    "    print(\"‚Ä¢ Implement gradual rollouts and canary deployments\")\n",
    "    print(\"‚Ä¢ Maintain human oversight and intervention capabilities\")\n",
    "    print(\"‚Ä¢ Continuously monitor and improve based on real usage\")\n",
    "    print(\"‚Ä¢ Keep models updated with latest safety measures\")\n",
    "    \n",
    "    print(\"\\n‚ö†Ô∏è  Remember:\")\n",
    "    print(\"‚Ä¢ AI chatbots are tools to assist, not replace human judgment\")\n",
    "    print(\"‚Ä¢ Transparency about AI capabilities builds user trust\")\n",
    "    print(\"‚Ä¢ Regular audits ensure continued safety and fairness\")\n",
    "\n",
    "print_production_checklist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéâ CONGRATULATIONS! üéâ\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou have completed the Chatbot-Qoder tutorial series!\")\n",
    "print(\"\\nüìö What you've learned:\")\n",
    "print(\"‚Ä¢ PyTorch fundamentals and neural networks\")\n",
    "print(\"‚Ä¢ Text preprocessing and tokenization\")\n",
    "print(\"‚Ä¢ Rule-based and retrieval-based chatbots\")\n",
    "print(\"‚Ä¢ Sequence models (RNN, LSTM, Seq2Seq)\")\n",
    "print(\"‚Ä¢ Attention mechanisms and transformers\")\n",
    "print(\"‚Ä¢ Generative models and advanced sampling\")\n",
    "print(\"‚Ä¢ Fine-tuning and production deployment\")\n",
    "print(\"\\nüöÄ Next steps:\")\n",
    "print(\"‚Ä¢ Experiment with larger pre-trained models\")\n",
    "print(\"‚Ä¢ Explore domain-specific fine-tuning\")\n",
    "print(\"‚Ä¢ Build and deploy your own chatbot\")\n",
    "print(\"‚Ä¢ Contribute to open-source chatbot projects\")\n",
    "print(\"‚Ä¢ Stay updated with latest research in conversational AI\")\n",
    "print(\"\\nThank you for completing this comprehensive journey!\")\n",
    "print(\"Ready to build amazing conversational AI systems! ü§ñ‚ú®\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}