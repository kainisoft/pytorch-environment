{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08: Sequence Models for Chatbots\n",
    "\n",
    "**Duration:** 3-4 hours | **Difficulty:** Intermediate-Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "- RNN, LSTM, and GRU architectures\n",
    "- Sequence-to-sequence modeling\n",
    "- Teacher forcing strategies\n",
    "- Conversational response generation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Sequence Models Overview](#1-overview)\n",
    "2. [LSTM Implementation](#2-lstm)\n",
    "3. [Seq2Seq Architecture](#3-seq2seq)\n",
    "4. [Training and Generation](#4-training)\n",
    "5. [Practical Exercise](#5-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Import utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.text_utils import SimpleTokenizer\n",
    "from utils.model_helpers import get_device, count_parameters\n",
    "\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequence Models Overview {#1-overview}\n",
    "\n",
    "**Sequence models** process sequential data where order matters:\n",
    "- **RNN**: Basic recurrent connections, short memory\n",
    "- **LSTM**: Long Short-Term Memory, handles long dependencies\n",
    "- **Seq2Seq**: Encoder-decoder for input→output transformation\n",
    "\n",
    "For chatbots: User message → Bot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load conversation data\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    conversation_data = json.load(f)\n",
    "\n",
    "conversations = [(item['question'], item['answer']) for item in conversation_data]\n",
    "print(f\"Loaded {len(conversations)} conversation pairs\")\n",
    "\n",
    "# Sample conversations\n",
    "for i in range(3):\n",
    "    print(f\"Q: {conversations[i][0]}\")\n",
    "    print(f\"A: {conversations[i][1]}\\n\")\n",
    "\n",
    "# Prepare tokenizer\n",
    "tokenizer = SimpleTokenizer(vocab_size=3000)\n",
    "all_text = []\n",
    "for q, a in conversations:\n",
    "    all_text.extend([q, a])\n",
    "\n",
    "tokenizer.fit(all_text)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Implementation {#2-lstm}\n",
    "\n",
    "LSTM networks can learn long-term dependencies through gating mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    \"\"\"LSTM Encoder for sequence encoding.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, (hidden, cell)\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    \"\"\"LSTM Decoder for sequence generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, \n",
    "                           batch_first=True, dropout=0.2)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedding(x)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        prediction = self.output(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "# Test LSTM components\n",
    "encoder = LSTMEncoder(vocab_size).to(device)\n",
    "decoder = LSTMDecoder(vocab_size).to(device)\n",
    "\n",
    "print(f\"Encoder params: {count_parameters(encoder)['total']:,}\")\n",
    "print(f\"Decoder params: {count_parameters(decoder)['total']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Seq2Seq Architecture {#3-seq2seq}\n",
    "\n",
    "Complete encoder-decoder model for conversation generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    \"\"\"Sequence-to-sequence model for conversation.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.encoder = LSTMEncoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "        self.decoder = LSTMDecoder(vocab_size, embed_dim, hidden_dim, num_layers)\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"Training forward pass with teacher forcing.\"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        \n",
    "        # Encode source\n",
    "        _, hidden = self.encoder(src)\n",
    "        \n",
    "        # Decode with teacher forcing\n",
    "        outputs = torch.zeros(batch_size, tgt_len, self.vocab_size).to(device)\n",
    "        decoder_input = tgt[:, 0:1]  # Start with SOS token\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decoder(decoder_input, hidden)\n",
    "            outputs[:, t:t+1, :] = output\n",
    "            \n",
    "            # Teacher forcing decision\n",
    "            if random.random() < teacher_forcing_ratio:\n",
    "                decoder_input = tgt[:, t:t+1]\n",
    "            else:\n",
    "                decoder_input = output.argmax(dim=-1)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def generate(self, src, max_length=30, sos_token=1, eos_token=2):\n",
    "        \"\"\"Generate response without teacher forcing.\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Encode\n",
    "            _, hidden = self.encoder(src)\n",
    "            \n",
    "            # Generate tokens\n",
    "            generated = []\n",
    "            decoder_input = torch.tensor([[sos_token]]).to(device)\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                output, hidden = self.decoder(decoder_input, hidden)\n",
    "                predicted = output.argmax(dim=-1)\n",
    "                generated.append(predicted.item())\n",
    "                \n",
    "                if predicted.item() == eos_token:\n",
    "                    break\n",
    "                \n",
    "                decoder_input = predicted\n",
    "        \n",
    "        return generated\n",
    "\n",
    "# Create model\n",
    "model = Seq2SeqModel(vocab_size, embed_dim=128, hidden_dim=256).to(device)\n",
    "print(f\"Total parameters: {count_parameters(model)['total']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Generation {#4-training}\n",
    "\n",
    "Train the model and implement text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    \"\"\"Dataset for conversation pairs.\"\"\"\n",
    "    \n",
    "    def __init__(self, conversations, tokenizer, max_length=40):\n",
    "        self.data = []\n",
    "        \n",
    "        for q, a in conversations:\n",
    "            src = tokenizer.encode(q, add_special_tokens=True, max_length=max_length)\n",
    "            tgt = tokenizer.encode(a, add_special_tokens=True, max_length=max_length)\n",
    "            \n",
    "            if len(src) > 0 and len(tgt) > 0:\n",
    "                self.data.append((torch.tensor(src), torch.tensor(tgt)))\n",
    "        \n",
    "        print(f\"Dataset size: {len(self.data)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Pad sequences in batch.\"\"\"\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=0)\n",
    "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=0)\n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "def train_model(model, dataloader, epochs=3, lr=0.001):\n",
    "    \"\"\"Train the sequence model.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.7)\n",
    "            \n",
    "            # Calculate loss (skip first token)\n",
    "            output_flat = output[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "            target_flat = tgt[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output_flat, target_flat)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 5 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Test generation\n",
    "        test_generation(model, tokenizer, \"What is machine learning?\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def test_generation(model, tokenizer, input_text):\n",
    "    \"\"\"Test response generation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokens = tokenizer.encode(input_text, add_special_tokens=True)\n",
    "    src = torch.tensor([tokens]).to(device)\n",
    "    \n",
    "    generated = model.generate(src, max_length=20)\n",
    "    response = tokenizer.decode(generated)\n",
    "    \n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Response: {response}\\n\")\n",
    "\n",
    "# Create dataset and train\n",
    "dataset = ConversationDataset(conversations, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(\"Training Seq2Seq model...\")\n",
    "losses = train_model(model, dataloader, epochs=3)\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test final model\n",
    "print(\"Final model testing:\")\n",
    "test_queries = [\n",
    "    \"What is deep learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"What is artificial intelligence?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_generation(model, tokenizer, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise {#5-exercise}\n",
    "\n",
    "**Exercise**: Enhance the sequence-to-sequence model\n",
    "\n",
    "### Tasks:\n",
    "1. Add bidirectional encoder\n",
    "2. Implement beam search decoding\n",
    "3. Add attention mechanism (preview for next notebook)\n",
    "4. Create conversational context handling\n",
    "\n",
    "### Questions:\n",
    "1. How does teacher forcing affect training?\n",
    "2. What are the limitations of basic seq2seq?\n",
    "3. How can we handle longer conversations?\n",
    "\n",
    "### Extensions:\n",
    "- Multi-turn conversation modeling\n",
    "- Copy mechanism for handling rare words\n",
    "- Scheduled sampling for training\n",
    "- Evaluation metrics (BLEU, ROUGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Simple conversational bot\n",
    "class SimpleSeq2SeqBot:\n",
    "    \"\"\"Simple conversational bot using seq2seq model.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.history = []\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        \"\"\"Generate response to user input.\"\"\"\n",
    "        tokens = self.tokenizer.encode(user_input, add_special_tokens=True)\n",
    "        src = torch.tensor([tokens]).to(device)\n",
    "        \n",
    "        generated = self.model.generate(src, max_length=25)\n",
    "        response = self.tokenizer.decode(generated)\n",
    "        \n",
    "        # Clean response\n",
    "        response = response.replace('<SOS>', '').replace('<EOS>', '').strip()\n",
    "        if not response:\n",
    "            response = \"I don't understand.\"\n",
    "        \n",
    "        self.history.append({'user': user_input, 'bot': response})\n",
    "        return response\n",
    "\n",
    "# Create and test bot\n",
    "bot = SimpleSeq2SeqBot(model, tokenizer)\n",
    "\n",
    "print(\"=== Seq2Seq Chatbot Demo ===\")\n",
    "test_inputs = [\n",
    "    \"Hello\",\n",
    "    \"What is AI?\",\n",
    "    \"How do I learn programming?\",\n",
    "    \"Tell me about neural networks\"\n",
    "]\n",
    "\n",
    "for user_input in test_inputs:\n",
    "    response = bot.respond(user_input)\n",
    "    print(f\"User: {user_input}\")\n",
    "    print(f\"Bot: {response}\\n\")\n",
    "\n",
    "print(\"=== Sequence Models Complete ===\")\n",
    "print(\"Key Concepts Learned:\")\n",
    "print(\"• LSTM architecture and memory mechanisms\")\n",
    "print(\"• Encoder-decoder sequence-to-sequence models\")\n",
    "print(\"• Teacher forcing training strategy\")\n",
    "print(\"• Text generation and decoding\")\n",
    "print(\"• Conversational response generation\")\n",
    "print(\"\\nNext: Attention mechanisms for better context understanding!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}