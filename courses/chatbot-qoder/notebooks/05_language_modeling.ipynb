{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05: Language Modeling Fundamentals\n",
    "\n",
    "**Duration:** 2-3 hours | **Difficulty:** Intermediate\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- Language modeling fundamentals and applications\n",
    "- N-gram vs neural language models\n",
    "- Character-level modeling with LSTM\n",
    "- Perplexity evaluation and text generation\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction to Language Modeling](#1-introduction)\n",
    "2. [N-gram Language Models](#2-ngram-models)\n",
    "3. [Neural Character-Level Model](#3-neural-model)\n",
    "4. [Text Generation and Evaluation](#4-generation)\n",
    "5. [Practical Exercise](#5-exercise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Import our custom utilities\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.model_helpers import get_device, count_parameters\n",
    "from configs.training_configs import get_training_config\n",
    "\n",
    "# Set device and random seeds\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Language Modeling {#1-introduction}\n",
    "\n",
    "**Language modeling** predicts the next word/character in a sequence. It's fundamental to:\n",
    "- Text generation and chatbots\n",
    "- Speech recognition and machine translation\n",
    "- Auto-completion systems\n",
    "\n",
    "### Types:\n",
    "1. **N-gram Models**: Statistical models based on word frequency\n",
    "2. **Neural Models**: Deep learning models capturing complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare text corpus\n",
    "with open('../data/corpora/ml_text_corpus.txt', 'r', encoding='utf-8') as f:\n",
    "    corpus_text = f.read()\n",
    "\n",
    "print(f\"Corpus length: {len(corpus_text)} characters\")\n",
    "print(f\"Sample text: {corpus_text[:200]}...\")\n",
    "\n",
    "# Character statistics\n",
    "chars = sorted(list(set(corpus_text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"\\nUnique characters: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars[:30])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. N-gram Language Models {#2-ngram-models}\n",
    "\n",
    "N-gram models predict next words based on frequency of previous N-1 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNGramModel:\n",
    "    \"\"\"Simple N-gram language model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, n: int = 3):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(int)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "    \n",
    "    def train(self, text: str):\n",
    "        \"\"\"Train model on text.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        self.vocab.update(words)\n",
    "        \n",
    "        # Add sentence markers\n",
    "        words = ['<START>'] * (self.n - 1) + words + ['<END>']\n",
    "        \n",
    "        # Count N-grams\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            ngram = tuple(words[i:i + self.n])\n",
    "            context = ngram[:-1]\n",
    "            \n",
    "            self.ngram_counts[ngram] += 1\n",
    "            self.context_counts[context] += 1\n",
    "    \n",
    "    def get_probability(self, word: str, context: Tuple) -> float:\n",
    "        \"\"\"Get probability of word given context.\"\"\"\n",
    "        ngram = context + (word,)\n",
    "        \n",
    "        # Add-one smoothing\n",
    "        ngram_count = self.ngram_counts.get(ngram, 0) + 1\n",
    "        context_count = self.context_counts.get(context, 0) + len(self.vocab)\n",
    "        \n",
    "        return ngram_count / context_count\n",
    "    \n",
    "    def generate(self, prompt: str = \"\", max_length: int = 20) -> str:\n",
    "        \"\"\"Generate text using the model.\"\"\"\n",
    "        if not prompt:\n",
    "            context = ['<START>'] * (self.n - 1)\n",
    "        else:\n",
    "            words = prompt.lower().split()\n",
    "            context = (['<START>'] * max(0, self.n - 1 - len(words)) + words)[-(self.n - 1):]\n",
    "        \n",
    "        generated = list(context) if prompt else []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            candidates = list(self.vocab)\n",
    "            probs = [self.get_probability(word, tuple(context)) for word in candidates]\n",
    "            \n",
    "            if not probs:\n",
    "                break\n",
    "            \n",
    "            # Sample next word\n",
    "            probs = np.array(probs)\n",
    "            probs = probs / probs.sum()\n",
    "            next_word = np.random.choice(candidates, p=probs)\n",
    "            \n",
    "            if next_word == '<END>':\n",
    "                break\n",
    "            \n",
    "            generated.append(next_word)\n",
    "            context = context[1:] + [next_word]\n",
    "        \n",
    "        return ' '.join([w for w in generated if w != '<START>'])\n",
    "\n",
    "# Train and test N-gram models\n",
    "print(\"Training N-gram models...\")\n",
    "models = {}\n",
    "for n in [2, 3]:\n",
    "    model = SimpleNGramModel(n=n)\n",
    "    model.train(corpus_text)\n",
    "    models[n] = model\n",
    "    print(f\"{n}-gram: {len(model.vocab)} vocab, {len(model.ngram_counts)} n-grams\")\n",
    "\n",
    "# Generate examples\n",
    "print(\"\\nGenerated text examples:\")\n",
    "for n, model in models.items():\n",
    "    text = model.generate(\"machine learning\", 10)\n",
    "    print(f\"{n}-gram: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Character-Level Model {#3-neural-model}\n",
    "\n",
    "Neural models can capture longer dependencies and learn character representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"Character-level dataset for language modeling.\"\"\"\n",
    "    \n",
    "    def __init__(self, text: str, seq_length: int = 50):\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        # Convert to indices\n",
    "        self.data = [self.char_to_idx[ch] for ch in text]\n",
    "        print(f\"Dataset: {len(self.data)} chars, vocab_size: {self.vocab_size}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx + self.seq_length], dtype=torch.long)\n",
    "        y = torch.tensor(self.data[idx + 1:idx + self.seq_length + 1], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "\n",
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"LSTM-based character language model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 64, \n",
    "                 hidden_dim: int = 128, num_layers: int = 2, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers,\n",
    "                           dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        output = self.output(self.dropout(lstm_out))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "\n",
    "# Create dataset and model\n",
    "dataset = CharDataset(corpus_text, seq_length=50)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = CharLSTM(\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embed_dim=64,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {count_parameters(model)['total']:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "sample_x, sample_y = next(iter(dataloader))\n",
    "with torch.no_grad():\n",
    "    output, _ = model(sample_x.to(device))\n",
    "    print(f\"Input shape: {sample_x.shape}, Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Generation and Evaluation {#4-generation}\n",
    "\n",
    "Let's train the model and implement text generation with different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_char_model(model, dataloader, epochs=3, lr=0.002):\n",
    "    \"\"\"Train character-level language model.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(dataloader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(data)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            loss = criterion(output.reshape(-1, dataset.vocab_size), target.reshape(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def generate_text(model, dataset, prompt=\"\", length=100, temperature=1.0):\n",
    "    \"\"\"Generate text using trained model.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if prompt:\n",
    "        input_seq = [dataset.char_to_idx.get(ch, 0) for ch in prompt]\n",
    "    else:\n",
    "        input_seq = [random.randint(0, dataset.vocab_size - 1)]\n",
    "    \n",
    "    generated = input_seq.copy()\n",
    "    hidden = None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(length):\n",
    "            x = torch.tensor([generated[-50:]], dtype=torch.long).to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            # Apply temperature and sample\n",
    "            logits = output[0, -1, :] / temperature\n",
    "            probs = F.softmax(logits, dim=0)\n",
    "            next_char = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            generated.append(next_char)\n",
    "    \n",
    "    return dataset.decode(generated)\n",
    "\n",
    "def calculate_perplexity(model, dataloader):\n",
    "    \"\"\"Calculate perplexity on test data.\"\"\"\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            \n",
    "            loss = criterion(output.reshape(-1, dataset.vocab_size), target.reshape(-1))\n",
    "            total_loss += loss.item() * target.numel()\n",
    "            total_tokens += target.numel()\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "# Train the model\n",
    "print(\"Training character-level language model...\")\n",
    "training_losses = train_char_model(model, dataloader, epochs=3)\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(training_losses, 'b-', linewidth=2)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = calculate_perplexity(model, dataloader)\n",
    "print(f\"\\nModel Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "# Generate text samples\n",
    "print(\"\\nGenerated text samples:\")\n",
    "prompts = [\"machine learning\", \"neural network\", \"deep learning\"]\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "for prompt in prompts:\n",
    "    for temp in temperatures:\n",
    "        text = generate_text(model, dataset, prompt, 80, temp)\n",
    "        print(f\"Prompt: '{prompt}', Temp: {temp}\")\n",
    "        print(f\"Generated: {text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Practical Exercise {#5-exercise}\n",
    "\n",
    "**Exercise**: Experiment with different model configurations and generation strategies.\n",
    "\n",
    "### Tasks:\n",
    "1. Try different LSTM hidden sizes (64, 256)\n",
    "2. Experiment with generation temperatures (0.1, 2.0)\n",
    "3. Compare character vs word-level modeling\n",
    "4. Implement beam search for generation\n",
    "\n",
    "### Questions:\n",
    "1. How does temperature affect generation quality?\n",
    "2. What are trade-offs between character and word-level models?\n",
    "3. How does model size impact perplexity and generation?\n",
    "\n",
    "### Extension Ideas:\n",
    "- Add attention mechanism to the LSTM\n",
    "- Try GRU instead of LSTM\n",
    "- Implement top-k and nucleus sampling\n",
    "- Fine-tune on domain-specific text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Try different configurations\n",
    "print(\"Exercise: Experimenting with model configurations\")\n",
    "\n",
    "# 1. Different hidden sizes\n",
    "hidden_sizes = [64, 256]\n",
    "for hidden_size in hidden_sizes:\n",
    "    print(f\"\\nTesting hidden_size = {hidden_size}\")\n",
    "    \n",
    "    test_model = CharLSTM(\n",
    "        vocab_size=dataset.vocab_size,\n",
    "        hidden_dim=hidden_size,\n",
    "        num_layers=1\n",
    "    ).to(device)\n",
    "    \n",
    "    params = count_parameters(test_model)\n",
    "    print(f\"Parameters: {params['total']:,}\")\n",
    "    \n",
    "    # Quick training\n",
    "    train_char_model(test_model, dataloader, epochs=1)\n",
    "    \n",
    "    # Generate sample\n",
    "    sample = generate_text(test_model, dataset, \"neural\", 50, 1.0)\n",
    "    print(f\"Sample: {sample[:80]}...\")\n",
    "\n",
    "# 2. Temperature experiments\n",
    "print(\"\\n\\nTemperature effects on generation:\")\n",
    "temperatures = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    text = generate_text(model, dataset, \"machine\", 60, temp)\n",
    "    print(f\"Temp {temp}: {text[:70]}...\")\n",
    "\n",
    "print(\"\\n=== Language Modeling Complete ===\")\n",
    "print(\"Key Concepts Learned:\")\n",
    "print(\"• N-gram vs neural language models\")\n",
    "print(\"• Character-level LSTM implementation\")\n",
    "print(\"• Text generation strategies\")\n",
    "print(\"• Perplexity evaluation\")\n",
    "print(\"• Temperature effects on sampling\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}