{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10: Transformer Basics\n",
    "\n",
    "**Duration:** 4-5 hours | **Difficulty:** Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "- Transformer architecture fundamentals\n",
    "- Self-attention and positional encoding\n",
    "- Complete transformer implementation\n",
    "- Text generation with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.text_utils import SimpleTokenizer\n",
    "from utils.model_helpers import get_device, count_parameters\n",
    "\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Components\n",
    "\n",
    "**Key innovations**: Self-attention, parallel processing, positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.w_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.w_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.w_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return self.w_o(output)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.norm1(x + self.attention(x, mask))\n",
    "        x = self.norm2(x + self.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=8, n_layers=4, d_ff=1024):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1000, d_model))\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:seq_len]\n",
    "        \n",
    "        # Causal mask\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        return self.head(x)\n",
    "    \n",
    "    def generate(self, start_tokens, max_length=30):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            tokens = start_tokens\n",
    "            for _ in range(max_length):\n",
    "                logits = self(tokens)\n",
    "                next_token = torch.multinomial(F.softmax(logits[:, -1], dim=-1), 1)\n",
    "                tokens = torch.cat([tokens, next_token], dim=1)\n",
    "                if next_token.item() == 2:  # EOS\n",
    "                    break\n",
    "            return tokens\n",
    "\n",
    "# Load data and create model\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    conversations = [(item['question'], item['answer']) for item in json.load(f)]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=2000)\n",
    "all_text = [text for conv in conversations for text in conv]\n",
    "tokenizer.fit(all_text)\n",
    "\n",
    "model = TransformerLM(len(tokenizer.vocab)).to(device)\n",
    "print(f\"Model parameters: {count_parameters(model)['total']:,}\")\n",
    "\n",
    "# Test generation\n",
    "test_input = torch.tensor([[1, 10, 20, 30]]).to(device)  # Sample tokens\n",
    "output = model.generate(test_input)\n",
    "print(f\"Generated: {output.shape}\")\n",
    "\n",
    "print(\"\\n=== Transformer Basics Complete ===\")\n",
    "print(\"Key Concepts Learned:\")\n",
    "print(\"• Self-attention mechanism\")\n",
    "print(\"• Positional encoding\")\n",
    "print(\"• Transformer blocks with residual connections\")\n",
    "print(\"• Autoregressive text generation\")\n",
    "print(\"• Foundation for modern language models\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}