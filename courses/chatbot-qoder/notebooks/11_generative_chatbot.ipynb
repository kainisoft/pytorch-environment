{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11: Generative Chatbot\n",
    "\n",
    "**Duration:** 4-5 hours | **Difficulty:** Advanced\n",
    "\n",
    "## Learning Objectives\n",
    "- Generative model training strategies\n",
    "- Advanced sampling and decoding methods\n",
    "- Safety and bias considerations\n",
    "- End-to-end generative chatbot implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.text_utils import SimpleTokenizer\n",
    "from utils.model_helpers import get_device, count_parameters\n",
    "\n",
    "device = get_device(\"auto\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Generation Strategies\n",
    "\n",
    "**Key methods**: Temperature, top-k, nucleus sampling, beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedGenerator:\n",
    "    \"\"\"Advanced text generation methods.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def temperature_sampling(logits, temperature=1.0):\n",
    "        \"\"\"Temperature-based sampling for controlling randomness.\"\"\"\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        return torch.multinomial(probs, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def top_k_sampling(logits, k=50, temperature=1.0):\n",
    "        \"\"\"Sample from top-k most likely tokens.\"\"\"\n",
    "        scaled_logits = logits / temperature\n",
    "        top_k_logits, top_k_indices = torch.topk(scaled_logits, k)\n",
    "        \n",
    "        filtered_logits = torch.full_like(scaled_logits, -float('inf'))\n",
    "        filtered_logits.scatter_(-1, top_k_indices, top_k_logits)\n",
    "        \n",
    "        probs = F.softmax(filtered_logits, dim=-1)\n",
    "        return torch.multinomial(probs, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def nucleus_sampling(logits, p=0.9, temperature=1.0):\n",
    "        \"\"\"Nucleus (top-p) sampling for dynamic vocabulary.\"\"\"\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = F.softmax(scaled_logits, dim=-1)\n",
    "        \n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # Find nucleus\n",
    "        nucleus_mask = cumulative_probs <= p\n",
    "        nucleus_mask[..., 1:] = nucleus_mask[..., :-1].clone()\n",
    "        nucleus_mask[..., 0] = True\n",
    "        \n",
    "        filtered_probs = torch.where(nucleus_mask, sorted_probs, torch.zeros_like(sorted_probs))\n",
    "        filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        sampled_idx = torch.multinomial(filtered_probs, 1)\n",
    "        return torch.gather(sorted_indices, -1, sampled_idx)\n",
    "\n",
    "# Load data\n",
    "with open('../data/conversations/simple_qa_pairs.json', 'r') as f:\n",
    "    conversations = [(item['question'], item['answer']) for item in json.load(f)]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=2000)\n",
    "all_text = [text for conv in conversations for text in conv]\n",
    "tokenizer.fit(all_text)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Test generation strategies\n",
    "sample_logits = torch.randn(1, vocab_size)\n",
    "generator = AdvancedGenerator()\n",
    "\n",
    "print(\"\\nGeneration Strategy Comparison:\")\n",
    "print(f\"Temperature (0.5): {generator.temperature_sampling(sample_logits, 0.5).item()}\")\n",
    "print(f\"Temperature (1.5): {generator.temperature_sampling(sample_logits, 1.5).item()}\")\n",
    "print(f\"Top-k (k=10): {generator.top_k_sampling(sample_logits, k=10).item()}\")\n",
    "print(f\"Nucleus (p=0.9): {generator.nucleus_sampling(sample_logits, p=0.9).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Chatbot Implementation\n",
    "\n",
    "Complete chatbot with conversation management and safety features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGenerativeModel(nn.Module):\n",
    "    \"\"\"Simple generative model for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model=256, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(512, d_model))\n",
    "        self.lstm = nn.LSTM(d_model, d_model, n_layers, batch_first=True)\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x) + self.pos_encoding[:seq_len]\n",
    "        x, _ = self.lstm(x)\n",
    "        return self.output(x)\n",
    "\n",
    "class GenerativeChatbot:\n",
    "    \"\"\"Generative chatbot with safety features.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, max_turns=5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_turns = max_turns\n",
    "        self.history = []\n",
    "        self.generator = AdvancedGenerator()\n",
    "        \n",
    "        # Basic safety filter\n",
    "        self.unsafe_words = ['hate', 'violence', 'harmful']\n",
    "    \n",
    "    def generate_response(self, user_input, strategy='nucleus', max_length=30, **kwargs):\n",
    "        \"\"\"Generate response using specified strategy.\"\"\"\n",
    "        \n",
    "        # Add context\n",
    "        if self.history:\n",
    "            context = \" \".join([f\"User: {h['user']} Bot: {h['bot']}\" for h in self.history[-2:]])\n",
    "            full_input = f\"{context} User: {user_input} Bot:\"\n",
    "        else:\n",
    "            full_input = f\"User: {user_input} Bot:\"\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = self.tokenizer.encode(full_input, add_special_tokens=True, max_length=100)\n",
    "        input_tensor = torch.tensor([tokens]).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        self.model.eval()\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            current = input_tensor\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                logits = self.model(current)\n",
    "                next_logits = logits[:, -1, :]\n",
    "                \n",
    "                # Apply generation strategy\n",
    "                if strategy == 'temperature':\n",
    "                    next_token = self.generator.temperature_sampling(\n",
    "                        next_logits, kwargs.get('temperature', 0.8)\n",
    "                    )\n",
    "                elif strategy == 'top_k':\n",
    "                    next_token = self.generator.top_k_sampling(\n",
    "                        next_logits, kwargs.get('k', 50)\n",
    "                    )\n",
    "                elif strategy == 'nucleus':\n",
    "                    next_token = self.generator.nucleus_sampling(\n",
    "                        next_logits, kwargs.get('p', 0.9)\n",
    "                    )\n",
    "                else:\n",
    "                    next_token = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                if next_token.item() == 2:  # EOS token\n",
    "                    break\n",
    "                \n",
    "                generated.append(next_token.item())\n",
    "                current = torch.cat([current, next_token], dim=1)\n",
    "        \n",
    "        # Decode response\n",
    "        if generated:\n",
    "            response = self.tokenizer.decode(generated)\n",
    "        else:\n",
    "            response = \"I'm not sure how to respond.\"\n",
    "        \n",
    "        return response.strip()\n",
    "    \n",
    "    def safety_check(self, response):\n",
    "        \"\"\"Basic safety filtering.\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        # Check unsafe words\n",
    "        for word in self.unsafe_words:\n",
    "            if word in response_lower:\n",
    "                return False\n",
    "        \n",
    "        # Check length\n",
    "        if len(response.split()) > 50 or len(response.strip()) < 2:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def respond(self, user_input, strategy='nucleus', **kwargs):\n",
    "        \"\"\"Generate safe response to user input.\"\"\"\n",
    "        \n",
    "        response = self.generate_response(user_input, strategy, **kwargs)\n",
    "        \n",
    "        if not self.safety_check(response):\n",
    "            response = \"I'd prefer to discuss something else. What can I help you with?\"\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append({'user': user_input, 'bot': response})\n",
    "        if len(self.history) > self.max_turns:\n",
    "            self.history = self.history[-self.max_turns:]\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset conversation history.\"\"\"\n",
    "        self.history = []\n",
    "\n",
    "# Create model and chatbot\n",
    "model = SimpleGenerativeModel(vocab_size).to(device)\n",
    "chatbot = GenerativeChatbot(model, tokenizer)\n",
    "\n",
    "print(f\"\\nGenerative Chatbot Created:\")\n",
    "print(f\"Model parameters: {count_parameters(model)['total']:,}\")\n",
    "\n",
    "# Demo conversation\n",
    "print(\"\\n=== Chatbot Demo ===\")\n",
    "test_inputs = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"What is machine learning?\",\n",
    "    \"Can you help me learn programming?\"\n",
    "]\n",
    "\n",
    "for strategy in ['nucleus', 'temperature', 'top_k']:\n",
    "    print(f\"\\nUsing {strategy} strategy:\")\n",
    "    chatbot.reset()\n",
    "    \n",
    "    for user_input in test_inputs[:2]:\n",
    "        response = chatbot.respond(user_input, strategy=strategy, temperature=0.8, p=0.9, k=50)\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Bot: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safety and Ethics Considerations\n",
    "\n",
    "**Critical aspects**: Content filtering, bias mitigation, privacy, human oversight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety and ethics discussion\n",
    "print(\"=== Safety and Ethics in Generative AI ===\")\n",
    "print(\"\\nKey Considerations:\")\n",
    "print(\"• Content Filtering: Prevent harmful, biased, or inappropriate outputs\")\n",
    "print(\"• Bias Mitigation: Ensure fair and inclusive responses\")\n",
    "print(\"• Privacy Protection: Avoid memorizing personal information\")\n",
    "print(\"• Factual Accuracy: Implement fact-checking and uncertainty measures\")\n",
    "print(\"• Human Oversight: Maintain human-in-the-loop validation\")\n",
    "\n",
    "print(\"\\nImplementation Strategies:\")\n",
    "print(\"• Multi-layer safety filters (keyword, ML-based, rule-based)\")\n",
    "print(\"• Diverse training data and bias auditing\")\n",
    "print(\"• Clear disclaimers about model limitations\")\n",
    "print(\"• Regular monitoring and model updates\")\n",
    "print(\"• User feedback and reporting mechanisms\")\n",
    "\n",
    "print(\"\\nBest Practices:\")\n",
    "print(\"• Safety-first design from the beginning\")\n",
    "print(\"• Transparent communication about capabilities\")\n",
    "print(\"• Continuous improvement based on real-world usage\")\n",
    "print(\"• Collaboration with ethicists and domain experts\")\n",
    "\n",
    "# Compare generation methods\n",
    "def compare_generation_quality():\n",
    "    \"\"\"Compare different generation strategies.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Generation Strategy Analysis ===\")\n",
    "    \n",
    "    strategies_info = {\n",
    "        'Greedy': {\n",
    "            'description': 'Always picks most likely token',\n",
    "            'pros': ['Deterministic', 'Fast'],\n",
    "            'cons': ['Repetitive', 'Less creative']\n",
    "        },\n",
    "        'Temperature': {\n",
    "            'description': 'Controls randomness in sampling',\n",
    "            'pros': ['Tunable creativity', 'Simple'],\n",
    "            'cons': ['Can be too random', 'Hard to tune']\n",
    "        },\n",
    "        'Top-k': {\n",
    "            'description': 'Sample from k most likely tokens',\n",
    "            'pros': ['Balanced quality', 'Prevents unlikely words'],\n",
    "            'cons': ['Fixed vocabulary size', 'Context-independent']\n",
    "        },\n",
    "        'Nucleus (Top-p)': {\n",
    "            'description': 'Dynamic vocabulary based on probability mass',\n",
    "            'pros': ['Adaptive', 'High quality', 'Context-aware'],\n",
    "            'cons': ['More complex', 'Computational overhead']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for strategy, info in strategies_info.items():\n",
    "        print(f\"\\n{strategy}:\")\n",
    "        print(f\"  Description: {info['description']}\")\n",
    "        print(f\"  Pros: {', '.join(info['pros'])}\")\n",
    "        print(f\"  Cons: {', '.join(info['cons'])}\")\n",
    "\n",
    "compare_generation_quality()\n",
    "\n",
    "print(\"\\n=== Generative Chatbot Complete ===\")\n",
    "print(\"Key Concepts Learned:\")\n",
    "print(\"• Advanced sampling strategies (temperature, top-k, nucleus)\")\n",
    "print(\"• Safety filtering and content moderation\")\n",
    "print(\"• Conversation context management\")\n",
    "print(\"• Ethical considerations in AI deployment\")\n",
    "print(\"• Quality vs creativity trade-offs in generation\")\n",
    "print(\"\\nNext: Model fine-tuning and deployment strategies!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}